{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML: Automatic Machine Learning\n",
    "\n",
    "AutoML: Automatic Machine Learning  \n",
    "\n",
    "H2O’s AutoML is used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "\n",
    "* Intro to AutoML + Hands-on Lab - Erin LeDell, Machine Learning Scientist... [https://youtu.be/42Oo8TOl85I](https://youtu.be/42Oo8TOl85I)  \n",
    "* Scalable Automatic Machine Learning in H2O [https://youtu.be/j6rqrEYQNdo](https://youtu.be/j6rqrEYQNdo)      \n",
    "\n",
    "* Scalable Automatic Machine Learning in H2O [https://youtu.be/j6rqrEYQNdo](https://youtu.be/j6rqrEYQNdo)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing H2O and h2o python\n",
    "\n",
    "See [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html)  \n",
    "\n",
    "Click the Download H2O button on the [http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html](http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html) page. This downloads a zip file that contains everything you need to get started.\n",
    "\n",
    "\n",
    "```bash\n",
    "cd ~/Downloads\n",
    "unzip h2o-3.20.0.1.zip\n",
    "cd h2o-3.20.0.1\n",
    "java -jar h2o.jar\n",
    "```\n",
    "\n",
    "Point your browser to http://localhost:54321.\n",
    "\n",
    "**Install in Python**  \n",
    "\n",
    "Install dependencies (prepending with sudo if needed):\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "pip install tabulate\n",
    "pip install scikit-learn\n",
    "pip install colorama\n",
    "pip install future\n",
    "```\n",
    "\n",
    "Remove any existing H2O module for Python.\n",
    "\n",
    "```bash\n",
    "pip uninstall h2o\n",
    "```\n",
    "\n",
    "Use pip to install this version of the H2O Python module.  \n",
    "\n",
    "```bash\n",
    "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\n",
    "```\n",
    "\n",
    "Note: When installing H2O from pip in OS X El Capitan, users must include the --user flag. For example:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o --user\n",
    "```\n",
    "\n",
    "Initialize H2O in Python and run a demo to see H2O at work.\n",
    "\n",
    "```python\n",
    "python\n",
    "import h2o\n",
    "h2o.init()\n",
    "h2o.demo(\"glm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "H2O model file file will be saved in one of two formats.\n",
    "\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use.\n",
    "\n",
    "See [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html)    \n",
    "\n",
    "```python\n",
    "\n",
    "# save the model\n",
    "model_path = h2o.save_model(model=model, path=\"/tmp/mymodel\", force=True)\n",
    "\n",
    "# or\n",
    "\n",
    "h2o.save_model(aml.leader, path = \"./models\")\n",
    "\n",
    "# or\n",
    "\n",
    "aml.leader.download_mojo(path = \"./models\")\n",
    "\n",
    "# load the model\n",
    "saved_model = h2o.load_model(model_path)\n",
    "\n",
    "```\n",
    "\n",
    "**Saving data from runs**   \n",
    "\n",
    "\n",
    "Stats about the models can be saved as text or csv or put directly in a database.\n",
    "\n",
    "Much of the data is gathered by converting H2O objects to pandas data frame.  So anything that a pandas data frame can be saved as is supported.\n",
    "[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)    \n",
    "\n",
    "\n",
    "```python\n",
    "data_pd = object.as_data_frame(use_pandas=True)\n",
    "```  \n",
    "\n",
    "Otherwise data is returned as python dictionaries or lists.  \n",
    "\n",
    "```python\n",
    "[('addr_state', 258199.28125, 1.0, 0.19965953057652525), ('int_rate', 203347.0625, 0.7875585924002257, 0.15724357886013807), ('dti', 116477.5703125, 0.45111500600856147, 0.09006941033569575), ('revol_util', 110586.1484375, 0.42829766179877776, 0.08551371010176734), ('annual_inc', 96993.90625, 0.3756552139898724, 0.07500314368384206), ('loan_amnt', 95294.5, 0.36907345186500207, 0.0736890321476241), ('total_acc', 90064.8046875, 0.3488189597255124, 0.06964502975498767), ('longest_credit_length', 84291.921875, 0.3264607146345416, 0.06518099303560954), ('purpose', 77462.203125, 0.30000936776426446, 0.05989972953637317), ('emp_length', 63839.28125, 0.24724809821677224, 0.04936543922589935), ('term', 34895.7265625, 0.1351503629040408, 0.02698405801466782), ('home_ownership', 26499.876953125, 0.10263342649457897, 0.02049174175536795), ('delinq_2yrs', 20556.2578125, 0.0796139234508423, 0.015895678583550586), ('verification_status', 14689.3369140625, 0.056891470971368166, 0.01135892438795138)]\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting H2O server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import h2o package and specific estimator \n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o.init with python seems very sensitive the the H2O version.  If the H2O cluster version is 3.20.0.1 and the python h2o library is 3.19.0 it will fail so we set strict_version_check=False\n",
    "\n",
    "If the H2O cluster isn't found h2o.init will start one.\n",
    "\n",
    "Note that the current script starts each H2O instance on a different port.  It's not clear why but should we do this we should choose from only the higher ports.\n",
    "\n",
    "A port number is a 16-bit unsigned integer, thus ranging from 0 to 65535.  There is no reason to choose a port less than 10000.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>10 mins 06 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>5 months and 13 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_bear_07058y</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>2.129 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         10 mins 06 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.1\n",
       "H2O cluster version age:    5 months and 13 days !!!\n",
       "H2O cluster name:           H2O_from_python_bear_07058y\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    2.129 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(strict_version_check=False) # start h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h2o.automl Parameters\n",
    "\n",
    "[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)  \n",
    "\n",
    "NB:  Eventually one wants to expose all the parameters to the expert user.   \n",
    "\n",
    "**Required Data Parameters**\n",
    "\n",
    "y: This argument is the name (or index) of the response column.  \n",
    "\n",
    "training_frame: Specifies the training set.  \n",
    "\n",
    "The user gives the name of the depenent variable and training file name.   \n",
    "\n",
    "\n",
    "**Required Stopping Parameters**  \n",
    "\n",
    "One of the following stopping strategies (time or number-of-model based) must be specified. When both options are set, then the AutoML run will stop as soon as it hits one of either of these limits.\n",
    "\n",
    "max_runtime_secs: This argument controls how long the AutoML run will execute for. This defaults to 3600 seconds (1 hour).  \n",
    "\n",
    "\n",
    "max_models: Specify the maximum number of models to build in an AutoML run, excluding the Stacked Ensemble models. Defaults to NULL/None.\n",
    "\n",
    "\n",
    "### Optional Parameters\n",
    "\n",
    "**Optional Data Parameters**  \n",
    "\n",
    "x: A list/vector of predictor column names or indexes. This argument only needs to be specified if the user wants to exclude columns from the set of predictors. If all columns (other than the response) should be used in prediction, then this does not need to be set.  \n",
    "\n",
    "validation_frame: This argument is used to specify the validation frame used for early stopping of individual models and early stopping of the grid searches (unless max_models or max_runtime_secs overrides metric-based early stopping).  \n",
    "\n",
    "leaderboard_frame: This argument allows the user to specify a particular data frame use to score & rank models on the leaderboard. This frame will not be used for anything besides leaderboard scoring. If a leaderboard frame is not specified by the user, then the leaderboard will use cross-validation metrics instead (or if cross-validation is turned off by setting nfolds = 0, then a leaderboard frame will be generated automatically from the validation frame (if provided) or the training frame).  \n",
    "\n",
    "fold_column: Specifies a column with cross-validation fold index assignment per observation. This is used to override the default, randomized, 5-fold cross-validation scheme for individual models in the AutoML run.  \n",
    "\n",
    "weights_column: Specifies a column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative weights are not allowed.  \n",
    "\n",
    "ignored_columns: (Optional, Python only) Specify the column or columns (as a list/vector) to be excluded from the model. This is the converse of the x argument.  \n",
    "\n",
    "**Optional Miscellaneous Parameters**  \n",
    "\n",
    "nfolds: Number of folds for k-fold cross-validation of the models in the AutoML run. Defaults to 5. Use 0 to disable cross-validation; this will also disable Stacked Ensembles (thus decreasing the overall best model performance).\n",
    "\n",
    "balance_classes: Specify whether to oversample the minority classes to balance the class distribution. This option is not enabled by default and can increase the data frame size. This option is only applicable for classification. Majority classes can be undersampled to satisfy the max_after_balance_size parameter.\n",
    "\n",
    "class_sampling_factors: Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance.\n",
    "\n",
    "max_after_balance_size: Specify the maximum relative size of the training data after balancing class counts (balance_classes must be enabled). Defaults to 5.0. (The value can be less than 1.0).\n",
    "\n",
    "stopping_metric: Specifies the metric to use for early stopping of the grid searches and individual models. Defaults to \"AUTO\". The available options are:\n",
    "\n",
    "AUTO: This defaults to logloss for classification, deviance for regression\n",
    "deviance (mean residual deviance)\n",
    "logloss\n",
    "MSE\n",
    "RMSE\n",
    "MAE\n",
    "RMSLE\n",
    "AUC\n",
    "lift_top_group\n",
    "misclassification\n",
    "mean_per_class_error  \n",
    "\n",
    "stopping_tolerance: This option specifies the relative tolerance for the metric-based stopping criterion to stop a grid search and the training of individual models within the AutoML run. This value defaults to 0.001 if the dataset is at least 1 million rows; otherwise it defaults to a bigger value determined by the size of the dataset and the non-NA-rate. In that case, the value is computed as 1/sqrt(nrows * non-NA-rate).\n",
    "\n",
    "stopping_rounds: This argument is used to stop model training when the stopping metric (e.g. AUC) doesn’t improve for this specified number of training rounds, based on a simple moving average. In the context of AutoML, this controls early stopping both within the random grid searches as well as the individual models. Defaults to 3 and must be an non-negative integer. To disable early stopping altogether, set this to 0.\n",
    "\n",
    "sort_metric: Specifies the metric used to sort the Leaderboard by at the end of an AutoML run. Available options include:\n",
    "\n",
    "AUTO: This defaults to AUC for binary classification, mean_per_class_error for multinomial classification, and deviance for regression.\n",
    "deviance (mean residual deviance)\n",
    "logloss\n",
    "MSE\n",
    "RMSE\n",
    "MAE\n",
    "RMSLE\n",
    "AUC\n",
    "mean_per_class_error  \n",
    "\n",
    "seed: Integer. Set a seed for reproducibility. AutoML can only guarantee reproducibility if max_models is used because max_runtime_secs is resource limited, meaning that if the available compute resources are not the same between runs, AutoML may be able to train more models on one run vs another. Defaults to NULL/None.\n",
    "\n",
    "project_name: Character string to identify an AutoML project. Defaults to NULL/None, which means a project name will be auto-generated based on the training frame ID. More models can be trained and added to an existing AutoML project by specifying the same project name in muliple calls to the AutoML function (as long as the same training frame is used in subsequent runs).\n",
    "\n",
    "exclude_algos: List/vector of character strings naming the algorithms to skip during the model-building phase. An example use is exclude_algos = [\"GLM\", \"DeepLearning\", \"DRF\"] in Python or exclude_algos = c(\"GLM\", \"DeepLearning\", \"DRF\") in R. Defaults to None/NULL, which means that all appropriate H2O algorithms will be used, if the search stopping criteria allow. The algorithm names are:\n",
    "\n",
    "GLM\n",
    "DeepLearning\n",
    "GBM\n",
    "DRF (This includes both the Random Forest and Extremely Randomized Trees (XRT) models. Refer to the Extremely Randomized Trees section in the DRF chapter and the histogram_type parameter description for more information.)\n",
    "StackedEnsemble\n",
    "keep_cross_validation_predictions: Specify whether to keep the predictions of the cross-validation predictions. If set to FALSE, then running the same AutoML object for repeated runs will cause an exception because CV predictions are are required to build additional Stacked Ensemble models in AutoML. This option defaults to TRUE.\n",
    "\n",
    "\n",
    "keep_cross_validation_models: Specify whether to keep the cross-validated models. Deleting cross-validation models will save memory in the H2O cluster. This option defaults to TRUE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume the following are passed by the user from the web interface\n",
    "\n",
    "'''\n",
    "Need a user id and project id?\n",
    "\n",
    "'''\n",
    "target='bad_loan' \n",
    "data_file='loan.csv'\n",
    "run_time=333\n",
    "run_id='SOME_ID_20180617_221529' # Just some arbitrary ID\n",
    "server_path='/Users/bear/Documents/INFO_7390/H2O'\n",
    "classification=True\n",
    "scale=False\n",
    "max_models=None\n",
    "balance_y=False # balance_classes=balance_y\n",
    "balance_threshold=0.2\n",
    "project =\"automl_test\"  # project_name = project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/loan.csv'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "import os\n",
    "\n",
    "data_path=os.path.join(server_path,data_file)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "if not os.path.isfile(data_path):\n",
    "  data_path = 'https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv'\n",
    "\n",
    "# Load data into H2O\n",
    "df = h2o.import_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:163987\n",
      "Cols:15\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>loan_amnt         </th><th>term     </th><th>int_rate          </th><th>emp_length       </th><th>home_ownership  </th><th>annual_inc       </th><th>purpose           </th><th>addr_state  </th><th>dti               </th><th>delinq_2yrs        </th><th>revol_util        </th><th>total_acc         </th><th>bad_loan          </th><th>longest_credit_length  </th><th>verification_status  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>enum     </td><td>real              </td><td>int              </td><td>enum            </td><td>real             </td><td>enum              </td><td>enum        </td><td>real              </td><td>int                </td><td>real              </td><td>int               </td><td>int               </td><td>int                    </td><td>enum                 </td></tr>\n",
       "<tr><td>mins   </td><td>500.0             </td><td>         </td><td>5.42              </td><td>0.0              </td><td>                </td><td>1896.0           </td><td>                  </td><td>            </td><td>0.0               </td><td>0.0                </td><td>0.0               </td><td>1.0               </td><td>0.0               </td><td>0.0                    </td><td>                     </td></tr>\n",
       "<tr><td>mean   </td><td>13074.169141456332</td><td>         </td><td>13.715904065566189</td><td>5.684352932995338</td><td>                </td><td>71915.67051974905</td><td>                  </td><td>            </td><td>15.881530121290167</td><td>0.22735700606252723</td><td>54.07917280242262 </td><td>24.579733834274574</td><td>0.1830388994249544</td><td>14.854273655448333     </td><td>                     </td></tr>\n",
       "<tr><td>maxs   </td><td>35000.0           </td><td>         </td><td>26.06             </td><td>10.0             </td><td>                </td><td>7141778.0        </td><td>                  </td><td>            </td><td>39.99             </td><td>29.0               </td><td>150.70000000000002</td><td>118.0             </td><td>1.0               </td><td>65.0                   </td><td>                     </td></tr>\n",
       "<tr><td>sigma  </td><td>7993.556188734672 </td><td>         </td><td>4.391939870545808 </td><td>3.610663731100238</td><td>                </td><td>59070.91565491818</td><td>                  </td><td>            </td><td>7.5876682241925355</td><td>0.6941679229284191 </td><td>25.285366766770498</td><td>11.685190365910666</td><td>0.3866995896078875</td><td>6.947732922546689      </td><td>                     </td></tr>\n",
       "<tr><td>zeros  </td><td>0                 </td><td>         </td><td>0                 </td><td>14248            </td><td>                </td><td>0                </td><td>                  </td><td>            </td><td>270               </td><td>139459             </td><td>1562              </td><td>0                 </td><td>133971            </td><td>11                     </td><td>                     </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0        </td><td>0                 </td><td>5804             </td><td>0               </td><td>4                </td><td>0                 </td><td>0           </td><td>0                 </td><td>29                 </td><td>193               </td><td>29                </td><td>0                 </td><td>29                     </td><td>0                    </td></tr>\n",
       "<tr><td>0      </td><td>5000.0            </td><td>36 months</td><td>10.65             </td><td>10.0             </td><td>RENT            </td><td>24000.0          </td><td>credit_card       </td><td>AZ          </td><td>27.65             </td><td>0.0                </td><td>83.7              </td><td>9.0               </td><td>0.0               </td><td>26.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>1      </td><td>2500.0            </td><td>60 months</td><td>15.27             </td><td>0.0              </td><td>RENT            </td><td>30000.0          </td><td>car               </td><td>GA          </td><td>1.0               </td><td>0.0                </td><td>9.4               </td><td>4.0               </td><td>1.0               </td><td>12.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>2      </td><td>2400.0            </td><td>36 months</td><td>15.96             </td><td>10.0             </td><td>RENT            </td><td>12252.0          </td><td>small_business    </td><td>IL          </td><td>8.72              </td><td>0.0                </td><td>98.5              </td><td>10.0              </td><td>0.0               </td><td>10.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>3      </td><td>10000.0           </td><td>36 months</td><td>13.49             </td><td>10.0             </td><td>RENT            </td><td>49200.0          </td><td>other             </td><td>CA          </td><td>20.0              </td><td>0.0                </td><td>21.0              </td><td>37.0              </td><td>0.0               </td><td>15.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>4      </td><td>5000.0            </td><td>36 months</td><td>7.9               </td><td>3.0              </td><td>RENT            </td><td>36000.0          </td><td>wedding           </td><td>AZ          </td><td>11.2              </td><td>0.0                </td><td>28.3              </td><td>12.0              </td><td>0.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>5      </td><td>3000.0            </td><td>36 months</td><td>18.64             </td><td>9.0              </td><td>RENT            </td><td>48000.0          </td><td>car               </td><td>CA          </td><td>5.3500000000000005</td><td>0.0                </td><td>87.5              </td><td>4.0               </td><td>0.0               </td><td>4.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>6      </td><td>5600.0            </td><td>60 months</td><td>21.28             </td><td>4.0              </td><td>OWN             </td><td>40000.0          </td><td>small_business    </td><td>CA          </td><td>5.55              </td><td>0.0                </td><td>32.6              </td><td>13.0              </td><td>1.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>7      </td><td>5375.0            </td><td>60 months</td><td>12.69             </td><td>0.0              </td><td>RENT            </td><td>15000.0          </td><td>other             </td><td>TX          </td><td>18.08             </td><td>0.0                </td><td>36.5              </td><td>3.0               </td><td>1.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>8      </td><td>6500.0            </td><td>60 months</td><td>14.65             </td><td>5.0              </td><td>OWN             </td><td>72000.0          </td><td>debt_consolidation</td><td>AZ          </td><td>16.12             </td><td>0.0                </td><td>20.6              </td><td>23.0              </td><td>0.0               </td><td>13.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>9      </td><td>12000.0           </td><td>36 months</td><td>12.69             </td><td>10.0             </td><td>OWN             </td><td>75000.0          </td><td>debt_consolidation</td><td>CA          </td><td>10.78             </td><td>0.0                </td><td>67.10000000000001 </td><td>34.0              </td><td>0.0               </td><td>22.0                   </td><td>verified             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_loan\n",
      "['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'revol_util', 'total_acc', 'longest_credit_length', 'verification_status']\n"
     ]
    }
   ],
   "source": [
    "# assign target and inputs for logistic regression\n",
    "y = target\n",
    "X = [name for name in df.columns if name != y]\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_amnt', 'emp_length', 'delinq_2yrs', 'total_acc', 'longest_credit_length']\n",
      "['term', 'home_ownership', 'purpose', 'addr_state', 'verification_status']\n",
      "['int_rate', 'annual_inc', 'dti', 'revol_util']\n"
     ]
    }
   ],
   "source": [
    "# determine column types\n",
    "ints, reals, enums = [], [], []\n",
    "for key, val in df.types.items():\n",
    "    if key in X:\n",
    "        if val == 'enum':\n",
    "            enums.append(key)\n",
    "        elif val == 'int':\n",
    "            ints.append(key)            \n",
    "        else: \n",
    "            reals.append(key)\n",
    "\n",
    "print(ints)\n",
    "print(enums)\n",
    "print(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "_ = df[reals].impute(method='mean')\n",
    "_ = df[ints].impute(method='median')\n",
    "\n",
    "if scale:\n",
    "    df[reals] = df[reals].scale()\n",
    "    df[ints] = df[ints].scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set target to factor for classification by default or if user specifies classification\n",
    "if classification:\n",
    "    df[y] = df[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[y].levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balance_classes check \n",
    "\n",
    "If one class in two class classification is less than 20% of the total then one should set balance_classes=True\n",
    "\n",
    "That is,\n",
    "\n",
    "balance_classes=balance_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if classification:\n",
    "    class_percentage = y_balance=df[y].mean()[0]/(df[y].max()-df[y].min())\n",
    "    if class_percentage < balance_threshold:\n",
    "        balance_y=True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(run_time)\n",
    "type(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate rather than take a test training split\n",
    "\n",
    "Cross-validation rather than taking a test training split reduces the variance of the estimates of goodness of fit statistics.  In rare cases one should take a test training split but this should be left to the expert users.\n",
    "\n",
    "This also means the pro user can just upload the data and not worry about taking a test training split.  \n",
    "\n",
    "We can pass the original, full dataset, `df` (without passing a `leaderboard_frame`).  This is a more efficient use of our data since we can use 100% of the data for training, rather than 80% or so.  This time our leaderboard will use cross-validated metrics. It also gives better estimates of goodness of fit statistics.\n",
    "\n",
    "*Note: Using an explicit `leaderboard_frame` for scoring may be useful in some cases, which is why the option is available.*  \n",
    "\n",
    "But it's not preferable in most cases.  Leave it as an expert option.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# automl\n",
    "# runs for run_time seconds then builds a stacked ensemble\n",
    "aml = H2OAutoML(max_runtime_secs=run_time,project_name = project,balance_classes=balance_y) # init automl, run for 300 seconds\n",
    "aml.train(x=X,  \n",
    "           y=y,\n",
    "           training_frame=df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard\n",
    "\n",
    "Next, we will view the AutoML Leaderboard.  Since we did not specify a `leaderboard_frame` in the `H2OAutoML.train()` method for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.  \n",
    "\n",
    "A default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric.  In the case of binary classification, the default ranking metric is Area Under the ROC Curve (AUC).  In the future, the user will be able to specify any of the H2O metrics so that different metrics can be used to generate rankings on the leaderboard.\n",
    "\n",
    "The leader model is stored at `aml.leader` and the leaderboard is stored at `aml.leaderboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20181120_154205   </td><td style=\"text-align: right;\">0.70712 </td><td style=\"text-align: right;\"> 0.435531</td><td style=\"text-align: right;\">              0.349509</td><td style=\"text-align: right;\">0.370103</td><td style=\"text-align: right;\">0.136977</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20181120_153506   </td><td style=\"text-align: right;\">0.706222</td><td style=\"text-align: right;\"> 0.435883</td><td style=\"text-align: right;\">              0.351685</td><td style=\"text-align: right;\">0.370262</td><td style=\"text-align: right;\">0.137094</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20181120_153506</td><td style=\"text-align: right;\">0.705212</td><td style=\"text-align: right;\"> 0.43622 </td><td style=\"text-align: right;\">              0.350829</td><td style=\"text-align: right;\">0.370407</td><td style=\"text-align: right;\">0.137202</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181120_154205_model_0            </td><td style=\"text-align: right;\">0.703839</td><td style=\"text-align: right;\"> 0.435055</td><td style=\"text-align: right;\">              0.353507</td><td style=\"text-align: right;\">0.370225</td><td style=\"text-align: right;\">0.137067</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181120_153506_model_0            </td><td style=\"text-align: right;\">0.703216</td><td style=\"text-align: right;\"> 0.435227</td><td style=\"text-align: right;\">              0.352786</td><td style=\"text-align: right;\">0.370273</td><td style=\"text-align: right;\">0.137102</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181120_153506_model_1            </td><td style=\"text-align: right;\">0.702313</td><td style=\"text-align: right;\"> 0.435862</td><td style=\"text-align: right;\">              0.353887</td><td style=\"text-align: right;\">0.370509</td><td style=\"text-align: right;\">0.137277</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181120_154205_model_1            </td><td style=\"text-align: right;\">0.702177</td><td style=\"text-align: right;\"> 0.435943</td><td style=\"text-align: right;\">              0.354178</td><td style=\"text-align: right;\">0.370554</td><td style=\"text-align: right;\">0.13731 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181120_153506_model_2            </td><td style=\"text-align: right;\">0.70028 </td><td style=\"text-align: right;\"> 0.437029</td><td style=\"text-align: right;\">              0.354784</td><td style=\"text-align: right;\">0.370872</td><td style=\"text-align: right;\">0.137546</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181120_154205_model_2            </td><td style=\"text-align: right;\">0.699971</td><td style=\"text-align: right;\"> 0.43737 </td><td style=\"text-align: right;\">              0.354552</td><td style=\"text-align: right;\">0.371075</td><td style=\"text-align: right;\">0.137696</td></tr>\n",
       "<tr><td>DeepLearning_0_AutoML_20181120_154205                </td><td style=\"text-align: right;\">0.699148</td><td style=\"text-align: right;\"> 0.43985 </td><td style=\"text-align: right;\">              0.355085</td><td style=\"text-align: right;\">0.372547</td><td style=\"text-align: right;\">0.138791</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will view a snapshot of the top models.  Here we should see the two Stacked Ensembles at or near the top of the leaderboard.  Stacked Ensembles can almost always outperform a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20181120_154205\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12685990330985406\n",
      "RMSE: 0.3561739789904002\n",
      "LogLoss: 0.40703958453227496\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 106607.73758484815\n",
      "AIC: 106631.73758484815\n",
      "AUC: 0.7734040182726795\n",
      "Gini: 0.5468080365453589\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21708051286464972: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>83212.0</td>\n",
       "<td>23869.0</td>\n",
       "<td>0.2229</td>\n",
       "<td> (23869.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9376.0</td>\n",
       "<td>14498.0</td>\n",
       "<td>0.3927</td>\n",
       "<td> (9376.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>92588.0</td>\n",
       "<td>38367.0</td>\n",
       "<td>0.2539</td>\n",
       "<td> (33245.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      83212  23869  0.2229   (23869.0/107081.0)\n",
       "1      9376   14498  0.3927   (9376.0/23874.0)\n",
       "Total  92588  38367  0.2539   (33245.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2170805</td>\n",
       "<td>0.4658666</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1410190</td>\n",
       "<td>0.6067433</td>\n",
       "<td>309.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3338429</td>\n",
       "<td>0.4568693</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4390853</td>\n",
       "<td>0.8290100</td>\n",
       "<td>98.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0676438</td>\n",
       "<td>1.0</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2335835</td>\n",
       "<td>0.3269004</td>\n",
       "<td>224.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1855661</td>\n",
       "<td>0.6993958</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1761337</td>\n",
       "<td>0.7006208</td>\n",
       "<td>274.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.217081     0.465867  238\n",
       "max f2                       0.141019     0.606743  309\n",
       "max f0point5                 0.333843     0.456869  155\n",
       "max accuracy                 0.439085     0.82901   98\n",
       "max precision                0.800677     1         0\n",
       "max recall                   0.0676438    1         394\n",
       "max specificity              0.800677     1         0\n",
       "max absolute_mcc             0.233584     0.3269    224\n",
       "max min_per_class_accuracy   0.185566     0.699396  265\n",
       "max mean_per_class_accuracy  0.176134     0.700621  274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5809896</td>\n",
       "<td>4.2290904</td>\n",
       "<td>4.2290904</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.0423054</td>\n",
       "<td>0.0423054</td>\n",
       "<td>322.9090447</td>\n",
       "<td>322.9090447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5350310</td>\n",
       "<td>3.4879528</td>\n",
       "<td>3.8585216</td>\n",
       "<td>0.6358779</td>\n",
       "<td>0.7034351</td>\n",
       "<td>0.0348915</td>\n",
       "<td>0.0771970</td>\n",
       "<td>248.7952815</td>\n",
       "<td>285.8521631</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5007744</td>\n",
       "<td>3.1972882</td>\n",
       "<td>3.6382227</td>\n",
       "<td>0.5828877</td>\n",
       "<td>0.6632731</td>\n",
       "<td>0.0319595</td>\n",
       "<td>0.1091564</td>\n",
       "<td>219.7288214</td>\n",
       "<td>263.8222689</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4718430</td>\n",
       "<td>2.9436144</td>\n",
       "<td>3.4645375</td>\n",
       "<td>0.5366412</td>\n",
       "<td>0.6316091</td>\n",
       "<td>0.0294463</td>\n",
       "<td>0.1386027</td>\n",
       "<td>194.3614440</td>\n",
       "<td>246.4537481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4483865</td>\n",
       "<td>2.8704357</td>\n",
       "<td>3.3457716</td>\n",
       "<td>0.5233002</td>\n",
       "<td>0.6099572</td>\n",
       "<td>0.0286923</td>\n",
       "<td>0.1672950</td>\n",
       "<td>187.0435684</td>\n",
       "<td>234.5771560</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3623038</td>\n",
       "<td>2.4444070</td>\n",
       "<td>2.8950893</td>\n",
       "<td>0.4456323</td>\n",
       "<td>0.5277947</td>\n",
       "<td>0.1222250</td>\n",
       "<td>0.2895200</td>\n",
       "<td>144.4406963</td>\n",
       "<td>189.5089261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3079302</td>\n",
       "<td>1.9878608</td>\n",
       "<td>2.5926798</td>\n",
       "<td>0.3624007</td>\n",
       "<td>0.4726634</td>\n",
       "<td>0.0993968</td>\n",
       "<td>0.3889168</td>\n",
       "<td>98.7860769</td>\n",
       "<td>159.2679764</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2684456</td>\n",
       "<td>1.7116814</td>\n",
       "<td>2.3724554</td>\n",
       "<td>0.3120513</td>\n",
       "<td>0.4325150</td>\n",
       "<td>0.0855743</td>\n",
       "<td>0.4744911</td>\n",
       "<td>71.1681359</td>\n",
       "<td>137.2455391</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2132009</td>\n",
       "<td>1.4102670</td>\n",
       "<td>2.0517178</td>\n",
       "<td>0.2571014</td>\n",
       "<td>0.3740423</td>\n",
       "<td>0.1410321</td>\n",
       "<td>0.6155232</td>\n",
       "<td>41.0267006</td>\n",
       "<td>105.1717765</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1765625</td>\n",
       "<td>1.1217652</td>\n",
       "<td>1.8192385</td>\n",
       "<td>0.2045055</td>\n",
       "<td>0.3316597</td>\n",
       "<td>0.1121722</td>\n",
       "<td>0.7276954</td>\n",
       "<td>12.1765206</td>\n",
       "<td>81.9238502</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1503145</td>\n",
       "<td>0.8745582</td>\n",
       "<td>1.6302967</td>\n",
       "<td>0.1594380</td>\n",
       "<td>0.2972143</td>\n",
       "<td>0.0874592</td>\n",
       "<td>0.8151546</td>\n",
       "<td>-12.5441786</td>\n",
       "<td>63.0296674</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1300345</td>\n",
       "<td>0.6769128</td>\n",
       "<td>1.4714054</td>\n",
       "<td>0.1234059</td>\n",
       "<td>0.2682474</td>\n",
       "<td>0.0676887</td>\n",
       "<td>0.8828433</td>\n",
       "<td>-32.3087165</td>\n",
       "<td>47.1405434</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1129563</td>\n",
       "<td>0.4959559</td>\n",
       "<td>1.3320601</td>\n",
       "<td>0.0904162</td>\n",
       "<td>0.2428437</td>\n",
       "<td>0.0495937</td>\n",
       "<td>0.9324370</td>\n",
       "<td>-50.4044061</td>\n",
       "<td>33.2060067</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0978849</td>\n",
       "<td>0.3920433</td>\n",
       "<td>1.2145535</td>\n",
       "<td>0.0714722</td>\n",
       "<td>0.2214215</td>\n",
       "<td>0.0392058</td>\n",
       "<td>0.9716428</td>\n",
       "<td>-60.7956663</td>\n",
       "<td>21.4553489</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0824531</td>\n",
       "<td>0.2194940</td>\n",
       "<td>1.1039951</td>\n",
       "<td>0.0400153</td>\n",
       "<td>0.2012659</td>\n",
       "<td>0.0219486</td>\n",
       "<td>0.9935914</td>\n",
       "<td>-78.0505987</td>\n",
       "<td>10.3995078</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597755</td>\n",
       "<td>0.0640840</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0116830</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0064086</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.5915993</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.58099            4.22909   4.22909            0.770992         0.770992                    0.0423054       0.0423054                  322.909   322.909\n",
       "    2        0.0200069                   0.535031           3.48795   3.85852            0.635878         0.703435                    0.0348915       0.077197                   248.795   285.852\n",
       "    3        0.0300027                   0.500774           3.19729   3.63822            0.582888         0.663273                    0.0319595       0.109156                   219.729   263.822\n",
       "    4        0.0400061                   0.471843           2.94361   3.46454            0.536641         0.631609                    0.0294463       0.138603                   194.361   246.454\n",
       "    5        0.0500019                   0.448387           2.87044   3.34577            0.5233           0.609957                    0.0286923       0.167295                   187.044   234.577\n",
       "    6        0.100004                    0.362304           2.44441   2.89509            0.445632         0.527795                    0.122225        0.28952                    144.441   189.509\n",
       "    7        0.150006                    0.30793            1.98786   2.59268            0.362401         0.472663                    0.0993968       0.388917                   98.7861   159.268\n",
       "    8        0.2                         0.268446           1.71168   2.37246            0.312051         0.432515                    0.0855743       0.474491                   71.1681   137.246\n",
       "    9        0.300004                    0.213201           1.41027   2.05172            0.257101         0.374042                    0.141032        0.615523                   41.0267   105.172\n",
       "    10       0.4                         0.176563           1.12177   1.81924            0.204506         0.33166                     0.112172        0.727695                   12.1765   81.9239\n",
       "    11       0.500004                    0.150314           0.874558  1.6303             0.159438         0.297214                    0.0874592       0.815155                   -12.5442  63.0297\n",
       "    12       0.6                         0.130034           0.676913  1.47141            0.123406         0.268247                    0.0676887       0.882843                   -32.3087  47.1405\n",
       "    13       0.699996                    0.112956           0.495956  1.33206            0.0904162        0.242844                    0.0495937       0.932437                   -50.4044  33.206\n",
       "    14       0.8                         0.0978849          0.392043  1.21455            0.0714722        0.221421                    0.0392058       0.971643                   -60.7957  21.4553\n",
       "    15       0.899996                    0.0824531          0.219494  1.104              0.0400153        0.201266                    0.0219486       0.993591                   -78.0506  10.3995\n",
       "    16       1                           0.0597755          0.064084  1                  0.011683         0.182307                    0.00640865      1                          -93.5916  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.13813682407268132\n",
      "RMSE: 0.37166762580655494\n",
      "LogLoss: 0.4379927029517987\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33020\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 28935.54992780763\n",
      "AIC: 28959.54992780763\n",
      "AUC: 0.7122628231156057\n",
      "Gini: 0.4245256462312115\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18933454821029921: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>18652.0</td>\n",
       "<td>8238.0</td>\n",
       "<td>0.3064</td>\n",
       "<td> (8238.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2386.0</td>\n",
       "<td>3756.0</td>\n",
       "<td>0.3885</td>\n",
       "<td> (2386.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21038.0</td>\n",
       "<td>11994.0</td>\n",
       "<td>0.3216</td>\n",
       "<td> (10624.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      18652  8238   0.3064   (8238.0/26890.0)\n",
       "1      2386   3756   0.3885   (2386.0/6142.0)\n",
       "Total  21038  11994  0.3216   (10624.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1893345</td>\n",
       "<td>0.4142038</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1197407</td>\n",
       "<td>0.5738535</td>\n",
       "<td>330.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3129965</td>\n",
       "<td>0.3817812</td>\n",
       "<td>160.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6010242</td>\n",
       "<td>0.8160572</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0640080</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2092723</td>\n",
       "<td>0.2475688</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1762849</td>\n",
       "<td>0.6525562</td>\n",
       "<td>268.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577331</td>\n",
       "<td>0.6547886</td>\n",
       "<td>286.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.189335     0.414204  256\n",
       "max f2                       0.119741     0.573853  330\n",
       "max f0point5                 0.312996     0.381781  160\n",
       "max accuracy                 0.601024     0.816057  29\n",
       "max precision                0.766658     1         0\n",
       "max recall                   0.064008     1         398\n",
       "max specificity              0.766658     1         0\n",
       "max absolute_mcc             0.209272     0.247569  238\n",
       "max min_per_class_accuracy   0.176285     0.652556  268\n",
       "max mean_per_class_accuracy  0.157733     0.654789  286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5783690</td>\n",
       "<td>3.1358434</td>\n",
       "<td>3.1358434</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.0314230</td>\n",
       "<td>0.0314230</td>\n",
       "<td>213.5843447</td>\n",
       "<td>213.5843447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5293169</td>\n",
       "<td>2.5749465</td>\n",
       "<td>2.8558192</td>\n",
       "<td>0.4787879</td>\n",
       "<td>0.5310136</td>\n",
       "<td>0.0257245</td>\n",
       "<td>0.0571475</td>\n",
       "<td>157.4946469</td>\n",
       "<td>185.5819237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4955180</td>\n",
       "<td>2.4119752</td>\n",
       "<td>2.7080205</td>\n",
       "<td>0.4484848</td>\n",
       "<td>0.5035318</td>\n",
       "<td>0.0240964</td>\n",
       "<td>0.0812439</td>\n",
       "<td>141.1975173</td>\n",
       "<td>170.8020508</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4688250</td>\n",
       "<td>2.4859277</td>\n",
       "<td>2.6524133</td>\n",
       "<td>0.4622356</td>\n",
       "<td>0.4931921</td>\n",
       "<td>0.0249105</td>\n",
       "<td>0.1061543</td>\n",
       "<td>148.5927707</td>\n",
       "<td>165.2413309</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4455599</td>\n",
       "<td>2.2164096</td>\n",
       "<td>2.5653181</td>\n",
       "<td>0.4121212</td>\n",
       "<td>0.4769976</td>\n",
       "<td>0.0221426</td>\n",
       "<td>0.1282970</td>\n",
       "<td>121.6409619</td>\n",
       "<td>156.5318141</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3612868</td>\n",
       "<td>2.0607188</td>\n",
       "<td>2.3130185</td>\n",
       "<td>0.3831719</td>\n",
       "<td>0.4300847</td>\n",
       "<td>0.1030609</td>\n",
       "<td>0.2313579</td>\n",
       "<td>106.0718760</td>\n",
       "<td>131.3018450</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.3068785</td>\n",
       "<td>1.7850835</td>\n",
       "<td>2.1371112</td>\n",
       "<td>0.3319200</td>\n",
       "<td>0.3973764</td>\n",
       "<td>0.0892218</td>\n",
       "<td>0.3205796</td>\n",
       "<td>78.5083530</td>\n",
       "<td>113.7111174</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2675737</td>\n",
       "<td>1.5756523</td>\n",
       "<td>1.9967252</td>\n",
       "<td>0.2929782</td>\n",
       "<td>0.3712729</td>\n",
       "<td>0.0788017</td>\n",
       "<td>0.3993813</td>\n",
       "<td>57.5652259</td>\n",
       "<td>99.6725200</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2132448</td>\n",
       "<td>1.3758567</td>\n",
       "<td>1.7897899</td>\n",
       "<td>0.2558280</td>\n",
       "<td>0.3327952</td>\n",
       "<td>0.1375773</td>\n",
       "<td>0.5369586</td>\n",
       "<td>37.5856668</td>\n",
       "<td>78.9789907</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1764937</td>\n",
       "<td>1.1299935</td>\n",
       "<td>1.6248533</td>\n",
       "<td>0.2101120</td>\n",
       "<td>0.3021267</td>\n",
       "<td>0.1129925</td>\n",
       "<td>0.6499512</td>\n",
       "<td>12.9993524</td>\n",
       "<td>62.4853295</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1497263</td>\n",
       "<td>0.9964784</td>\n",
       "<td>1.4991859</td>\n",
       "<td>0.1852861</td>\n",
       "<td>0.2787600</td>\n",
       "<td>0.0996418</td>\n",
       "<td>0.7495930</td>\n",
       "<td>-0.3521561</td>\n",
       "<td>49.9185933</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1296393</td>\n",
       "<td>0.7587565</td>\n",
       "<td>1.3757872</td>\n",
       "<td>0.1410839</td>\n",
       "<td>0.2558151</td>\n",
       "<td>0.0758711</td>\n",
       "<td>0.8254640</td>\n",
       "<td>-24.1243542</td>\n",
       "<td>37.5787247</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1130667</td>\n",
       "<td>0.6284978</td>\n",
       "<td>1.2690362</td>\n",
       "<td>0.1168635</td>\n",
       "<td>0.2359657</td>\n",
       "<td>0.0628460</td>\n",
       "<td>0.8883100</td>\n",
       "<td>-37.1502161</td>\n",
       "<td>26.9036234</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0982229</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1761501</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2186944</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9408987</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6150117</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0826149</td>\n",
       "<td>0.3989170</td>\n",
       "<td>1.0897938</td>\n",
       "<td>0.0741750</td>\n",
       "<td>0.2026372</td>\n",
       "<td>0.0398893</td>\n",
       "<td>0.9807880</td>\n",
       "<td>-60.1082978</td>\n",
       "<td>8.9793790</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0605773</td>\n",
       "<td>0.1920733</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0357143</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0192120</td>\n",
       "<td>1.0</td>\n",
       "<td>-80.7926687</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain       cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  ---------  -----------------\n",
       "    1        0.0100206                   0.578369           3.13584   3.13584            0.583082         0.583082                    0.031423        0.031423                   213.584    213.584\n",
       "    2        0.0200109                   0.529317           2.57495   2.85582            0.478788         0.531014                    0.0257245       0.0571475                  157.495    185.582\n",
       "    3        0.0300012                   0.495518           2.41198   2.70802            0.448485         0.503532                    0.0240964       0.0812439                  141.198    170.802\n",
       "    4        0.0400218                   0.468825           2.48593   2.65241            0.462236         0.493192                    0.0249105       0.106154                   148.593    165.241\n",
       "    5        0.0500121                   0.44556            2.21641   2.56532            0.412121         0.476998                    0.0221426       0.128297                   121.641    156.532\n",
       "    6        0.100024                    0.361287           2.06072   2.31302            0.383172         0.430085                    0.103061        0.231358                   106.072    131.302\n",
       "    7        0.150006                    0.306878           1.78508   2.13711            0.33192          0.397376                    0.0892218       0.32058                    78.5084    113.711\n",
       "    8        0.200018                    0.267574           1.57565   1.99673            0.292978         0.371273                    0.0788017       0.399381                   57.5652    99.6725\n",
       "    9        0.300012                    0.213245           1.37586   1.78979            0.255828         0.332795                    0.137577        0.536959                   37.5857    78.979\n",
       "    10       0.400006                    0.176494           1.12999   1.62485            0.210112         0.302127                    0.112993        0.649951                   12.9994    62.4853\n",
       "    11       0.5                         0.149726           0.996478  1.49919            0.185286         0.27876                     0.0996418       0.749593                   -0.352156  49.9186\n",
       "    12       0.599994                    0.129639           0.758756  1.37579            0.141084         0.255815                    0.0758711       0.825464                   -24.1244   37.5787\n",
       "    13       0.699988                    0.113067           0.628498  1.26904            0.116863         0.235966                    0.062846        0.88831                    -37.1502   26.9036\n",
       "    14       0.799982                    0.0982229          0.525919  1.17615            0.0977899        0.218694                    0.0525887       0.940899                   -47.4081   17.615\n",
       "    15       0.899976                    0.0826149          0.398917  1.08979            0.074175         0.202637                    0.0398893       0.980788                   -60.1083   8.97938\n",
       "    16       1                           0.0605773          0.192073  1                  0.0357143        0.185941                    0.019212        1                          -80.7927   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.1369765435568584\n",
      "RMSE: 0.3701034227845757\n",
      "LogLoss: 0.4355309009590542\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114069.89827018589\n",
      "AIC: 114093.89827018589\n",
      "AUC: 0.7071203432205223\n",
      "Gini: 0.4142406864410446\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17843972441369682: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>73561.0</td>\n",
       "<td>33520.0</td>\n",
       "<td>0.313</td>\n",
       "<td> (33520.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9215.0</td>\n",
       "<td>14659.0</td>\n",
       "<td>0.386</td>\n",
       "<td> (9215.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>82776.0</td>\n",
       "<td>48179.0</td>\n",
       "<td>0.3263</td>\n",
       "<td> (42735.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      73561  33520  0.313    (33520.0/107081.0)\n",
       "1      9215   14659  0.386    (9215.0/23874.0)\n",
       "Total  82776  48179  0.3263   (42735.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1784397</td>\n",
       "<td>0.4068949</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1136746</td>\n",
       "<td>0.5665672</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2879939</td>\n",
       "<td>0.3708050</td>\n",
       "<td>176.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5663591</td>\n",
       "<td>0.8184567</td>\n",
       "<td>36.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0620870</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1815977</td>\n",
       "<td>0.2412252</td>\n",
       "<td>258.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1671726</td>\n",
       "<td>0.6498819</td>\n",
       "<td>272.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1616542</td>\n",
       "<td>0.6512829</td>\n",
       "<td>278.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.17844      0.406895  261\n",
       "max f2                       0.113675     0.566567  334\n",
       "max f0point5                 0.287994     0.370805  176\n",
       "max accuracy                 0.566359     0.818457  36\n",
       "max precision                0.785924     1         0\n",
       "max recall                   0.062087     1         399\n",
       "max specificity              0.785924     1         0\n",
       "max absolute_mcc             0.181598     0.241225  258\n",
       "max min_per_class_accuracy   0.167173     0.649882  272\n",
       "max mean_per_class_accuracy  0.161654     0.651283  278"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5609815</td>\n",
       "<td>2.9394272</td>\n",
       "<td>2.9394272</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.0294044</td>\n",
       "<td>0.0294044</td>\n",
       "<td>193.9427222</td>\n",
       "<td>193.9427222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5136764</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7719385</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5053435</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0554578</td>\n",
       "<td>160.4449761</td>\n",
       "<td>177.1938491</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4802174</td>\n",
       "<td>2.4346323</td>\n",
       "<td>2.6595603</td>\n",
       "<td>0.4438503</td>\n",
       "<td>0.4848562</td>\n",
       "<td>0.0243361</td>\n",
       "<td>0.0797939</td>\n",
       "<td>143.4632310</td>\n",
       "<td>165.9560331</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4541967</td>\n",
       "<td>2.2987828</td>\n",
       "<td>2.5693487</td>\n",
       "<td>0.4190840</td>\n",
       "<td>0.4684100</td>\n",
       "<td>0.0229957</td>\n",
       "<td>0.1027896</td>\n",
       "<td>129.8782827</td>\n",
       "<td>156.9348739</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4304407</td>\n",
       "<td>2.3592048</td>\n",
       "<td>2.5273392</td>\n",
       "<td>0.4300993</td>\n",
       "<td>0.4607514</td>\n",
       "<td>0.0235821</td>\n",
       "<td>0.1263718</td>\n",
       "<td>135.9204803</td>\n",
       "<td>152.7339208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3489922</td>\n",
       "<td>2.0674422</td>\n",
       "<td>2.2973907</td>\n",
       "<td>0.3769090</td>\n",
       "<td>0.4188302</td>\n",
       "<td>0.1033761</td>\n",
       "<td>0.2297478</td>\n",
       "<td>106.7442216</td>\n",
       "<td>129.7390712</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2957391</td>\n",
       "<td>1.7583311</td>\n",
       "<td>2.1177042</td>\n",
       "<td>0.3205559</td>\n",
       "<td>0.3860721</td>\n",
       "<td>0.0879199</td>\n",
       "<td>0.3176678</td>\n",
       "<td>75.8331123</td>\n",
       "<td>111.7704182</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2569097</td>\n",
       "<td>1.5826559</td>\n",
       "<td>1.9839574</td>\n",
       "<td>0.2885291</td>\n",
       "<td>0.3616891</td>\n",
       "<td>0.0791237</td>\n",
       "<td>0.3967915</td>\n",
       "<td>58.2655941</td>\n",
       "<td>98.3957443</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2032167</td>\n",
       "<td>1.3679632</td>\n",
       "<td>1.7786208</td>\n",
       "<td>0.2493891</td>\n",
       "<td>0.3242548</td>\n",
       "<td>0.1368015</td>\n",
       "<td>0.5335930</td>\n",
       "<td>36.7963184</td>\n",
       "<td>77.8620797</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1679941</td>\n",
       "<td>1.1284673</td>\n",
       "<td>1.6160886</td>\n",
       "<td>0.2057274</td>\n",
       "<td>0.2946241</td>\n",
       "<td>0.1128424</td>\n",
       "<td>0.6464355</td>\n",
       "<td>12.8467313</td>\n",
       "<td>61.6088632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1428590</td>\n",
       "<td>0.9302652</td>\n",
       "<td>1.4789198</td>\n",
       "<td>0.1695938</td>\n",
       "<td>0.2696173</td>\n",
       "<td>0.0930301</td>\n",
       "<td>0.7394655</td>\n",
       "<td>-6.9734773</td>\n",
       "<td>47.8919761</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1238863</td>\n",
       "<td>0.8143060</td>\n",
       "<td>1.3681550</td>\n",
       "<td>0.1484536</td>\n",
       "<td>0.2494241</td>\n",
       "<td>0.0814275</td>\n",
       "<td>0.8208930</td>\n",
       "<td>-18.5693965</td>\n",
       "<td>36.8155036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1081113</td>\n",
       "<td>0.6589009</td>\n",
       "<td>1.2668363</td>\n",
       "<td>0.1201222</td>\n",
       "<td>0.2309530</td>\n",
       "<td>0.0658876</td>\n",
       "<td>0.8867806</td>\n",
       "<td>-34.1099078</td>\n",
       "<td>26.6836336</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0941428</td>\n",
       "<td>0.5185360</td>\n",
       "<td>1.1732952</td>\n",
       "<td>0.0945327</td>\n",
       "<td>0.2138998</td>\n",
       "<td>0.0518556</td>\n",
       "<td>0.9386362</td>\n",
       "<td>-48.1464047</td>\n",
       "<td>17.3295217</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0801775</td>\n",
       "<td>0.3945866</td>\n",
       "<td>1.0867750</td>\n",
       "<td>0.0719359</td>\n",
       "<td>0.1981266</td>\n",
       "<td>0.0394572</td>\n",
       "<td>0.9780933</td>\n",
       "<td>-60.5413434</td>\n",
       "<td>8.6774970</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597666</td>\n",
       "<td>0.2190584</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0399359</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0219067</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.0941597</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.560981           2.93943   2.93943            0.535878         0.535878                    0.0294044       0.0294044                  193.943   193.943\n",
       "    2        0.0200069                   0.513676           2.60445   2.77194            0.474809         0.505344                    0.0260534       0.0554578                  160.445   177.194\n",
       "    3        0.0300027                   0.480217           2.43463   2.65956            0.44385          0.484856                    0.0243361       0.0797939                  143.463   165.956\n",
       "    4        0.0400061                   0.454197           2.29878   2.56935            0.419084         0.46841                     0.0229957       0.10279                    129.878   156.935\n",
       "    5        0.0500019                   0.430441           2.3592    2.52734            0.430099         0.460751                    0.0235821       0.126372                   135.92    152.734\n",
       "    6        0.100004                    0.348992           2.06744   2.29739            0.376909         0.41883                     0.103376        0.229748                   106.744   129.739\n",
       "    7        0.150006                    0.295739           1.75833   2.1177             0.320556         0.386072                    0.0879199       0.317668                   75.8331   111.77\n",
       "    8        0.2                         0.25691            1.58266   1.98396            0.288529         0.361689                    0.0791237       0.396791                   58.2656   98.3957\n",
       "    9        0.300004                    0.203217           1.36796   1.77862            0.249389         0.324255                    0.136802        0.533593                   36.7963   77.8621\n",
       "    10       0.4                         0.167994           1.12847   1.61609            0.205727         0.294624                    0.112842        0.646435                   12.8467   61.6089\n",
       "    11       0.500004                    0.142859           0.930265  1.47892            0.169594         0.269617                    0.0930301       0.739466                   -6.97348  47.892\n",
       "    12       0.6                         0.123886           0.814306  1.36816            0.148454         0.249424                    0.0814275       0.820893                   -18.5694  36.8155\n",
       "    13       0.699996                    0.108111           0.658901  1.26684            0.120122         0.230953                    0.0658876       0.886781                   -34.1099  26.6836\n",
       "    14       0.8                         0.0941428          0.518536  1.1733             0.0945327        0.2139                      0.0518556       0.938636                   -48.1464  17.3295\n",
       "    15       0.899996                    0.0801775          0.394587  1.08677            0.0719359        0.198127                    0.0394572       0.978093                   -60.5413  8.6775\n",
       "    16       1                           0.0597666          0.219058  1                  0.0399359        0.182307                    0.0219067       1                          -78.0942  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackedensemble'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'base_models',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'confusion_matrix',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_levelone_frame',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metalearner_algorithm',\n",
       " 'metalearner_fold_assignment',\n",
       " 'metalearner_fold_column',\n",
       " 'metalearner_nfolds',\n",
       " 'metalearner_params',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aml.leader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Exploration\n",
    "\n",
    "To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model.  The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run.  This is often the top performing model on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>mean_per_class_error</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StackedEnsemble_AllModels_0_AutoML_20181120_15...</td>\n",
       "      <td>0.707120</td>\n",
       "      <td>0.435531</td>\n",
       "      <td>0.349509</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.136977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StackedEnsemble_AllModels_0_AutoML_20181120_15...</td>\n",
       "      <td>0.706222</td>\n",
       "      <td>0.435883</td>\n",
       "      <td>0.351685</td>\n",
       "      <td>0.370262</td>\n",
       "      <td>0.137094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StackedEnsemble_BestOfFamily_0_AutoML_20181120...</td>\n",
       "      <td>0.705212</td>\n",
       "      <td>0.436220</td>\n",
       "      <td>0.350829</td>\n",
       "      <td>0.370407</td>\n",
       "      <td>0.137202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_154205_model_0</td>\n",
       "      <td>0.703839</td>\n",
       "      <td>0.435055</td>\n",
       "      <td>0.353507</td>\n",
       "      <td>0.370225</td>\n",
       "      <td>0.137067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_153506_model_0</td>\n",
       "      <td>0.703216</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>0.352786</td>\n",
       "      <td>0.370273</td>\n",
       "      <td>0.137102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_153506_model_1</td>\n",
       "      <td>0.702313</td>\n",
       "      <td>0.435862</td>\n",
       "      <td>0.353887</td>\n",
       "      <td>0.370509</td>\n",
       "      <td>0.137277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_154205_model_1</td>\n",
       "      <td>0.702177</td>\n",
       "      <td>0.435943</td>\n",
       "      <td>0.354178</td>\n",
       "      <td>0.370554</td>\n",
       "      <td>0.137310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_153506_model_2</td>\n",
       "      <td>0.700280</td>\n",
       "      <td>0.437029</td>\n",
       "      <td>0.354784</td>\n",
       "      <td>0.370872</td>\n",
       "      <td>0.137546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_154205_model_2</td>\n",
       "      <td>0.699971</td>\n",
       "      <td>0.437370</td>\n",
       "      <td>0.354552</td>\n",
       "      <td>0.371075</td>\n",
       "      <td>0.137696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DeepLearning_0_AutoML_20181120_154205</td>\n",
       "      <td>0.699148</td>\n",
       "      <td>0.439850</td>\n",
       "      <td>0.355085</td>\n",
       "      <td>0.372547</td>\n",
       "      <td>0.138791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GLM_grid_0_AutoML_20181120_154205_model_0</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>0.438455</td>\n",
       "      <td>0.354668</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>0.137968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GLM_grid_0_AutoML_20181120_153506_model_0</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>0.438455</td>\n",
       "      <td>0.354668</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>0.137968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DeepLearning_0_AutoML_20181120_153506</td>\n",
       "      <td>0.696297</td>\n",
       "      <td>0.439618</td>\n",
       "      <td>0.357903</td>\n",
       "      <td>0.371964</td>\n",
       "      <td>0.138357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_154205_model_4</td>\n",
       "      <td>0.694225</td>\n",
       "      <td>0.443191</td>\n",
       "      <td>0.361761</td>\n",
       "      <td>0.372926</td>\n",
       "      <td>0.139073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_153506_model_4</td>\n",
       "      <td>0.694205</td>\n",
       "      <td>0.442884</td>\n",
       "      <td>0.358243</td>\n",
       "      <td>0.372862</td>\n",
       "      <td>0.139026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_153506_model_3</td>\n",
       "      <td>0.690842</td>\n",
       "      <td>0.443542</td>\n",
       "      <td>0.362873</td>\n",
       "      <td>0.373139</td>\n",
       "      <td>0.139233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_154205_model_3</td>\n",
       "      <td>0.689891</td>\n",
       "      <td>0.443962</td>\n",
       "      <td>0.363898</td>\n",
       "      <td>0.373311</td>\n",
       "      <td>0.139361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XRT_0_AutoML_20181120_154205</td>\n",
       "      <td>0.688110</td>\n",
       "      <td>0.471744</td>\n",
       "      <td>0.364619</td>\n",
       "      <td>0.382286</td>\n",
       "      <td>0.146143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>XRT_0_AutoML_20181120_153506</td>\n",
       "      <td>0.687225</td>\n",
       "      <td>0.472021</td>\n",
       "      <td>0.365550</td>\n",
       "      <td>0.382442</td>\n",
       "      <td>0.146262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DRF_0_AutoML_20181120_154205</td>\n",
       "      <td>0.684609</td>\n",
       "      <td>0.479048</td>\n",
       "      <td>0.366846</td>\n",
       "      <td>0.383622</td>\n",
       "      <td>0.147166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DRF_0_AutoML_20181120_153506</td>\n",
       "      <td>0.684478</td>\n",
       "      <td>0.479350</td>\n",
       "      <td>0.367447</td>\n",
       "      <td>0.383614</td>\n",
       "      <td>0.147160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DRF_0_AutoML_20181120_153207</td>\n",
       "      <td>0.683488</td>\n",
       "      <td>0.479453</td>\n",
       "      <td>0.368185</td>\n",
       "      <td>0.383608</td>\n",
       "      <td>0.147155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model_id       auc   logloss  \\\n",
       "0   StackedEnsemble_AllModels_0_AutoML_20181120_15...  0.707120  0.435531   \n",
       "1   StackedEnsemble_AllModels_0_AutoML_20181120_15...  0.706222  0.435883   \n",
       "2   StackedEnsemble_BestOfFamily_0_AutoML_20181120...  0.705212  0.436220   \n",
       "3           GBM_grid_0_AutoML_20181120_154205_model_0  0.703839  0.435055   \n",
       "4           GBM_grid_0_AutoML_20181120_153506_model_0  0.703216  0.435227   \n",
       "5           GBM_grid_0_AutoML_20181120_153506_model_1  0.702313  0.435862   \n",
       "6           GBM_grid_0_AutoML_20181120_154205_model_1  0.702177  0.435943   \n",
       "7           GBM_grid_0_AutoML_20181120_153506_model_2  0.700280  0.437029   \n",
       "8           GBM_grid_0_AutoML_20181120_154205_model_2  0.699971  0.437370   \n",
       "9               DeepLearning_0_AutoML_20181120_154205  0.699148  0.439850   \n",
       "10          GLM_grid_0_AutoML_20181120_154205_model_0  0.697503  0.438455   \n",
       "11          GLM_grid_0_AutoML_20181120_153506_model_0  0.697503  0.438455   \n",
       "12              DeepLearning_0_AutoML_20181120_153506  0.696297  0.439618   \n",
       "13          GBM_grid_0_AutoML_20181120_154205_model_4  0.694225  0.443191   \n",
       "14          GBM_grid_0_AutoML_20181120_153506_model_4  0.694205  0.442884   \n",
       "15          GBM_grid_0_AutoML_20181120_153506_model_3  0.690842  0.443542   \n",
       "16          GBM_grid_0_AutoML_20181120_154205_model_3  0.689891  0.443962   \n",
       "17                       XRT_0_AutoML_20181120_154205  0.688110  0.471744   \n",
       "18                       XRT_0_AutoML_20181120_153506  0.687225  0.472021   \n",
       "19                       DRF_0_AutoML_20181120_154205  0.684609  0.479048   \n",
       "20                       DRF_0_AutoML_20181120_153506  0.684478  0.479350   \n",
       "21                       DRF_0_AutoML_20181120_153207  0.683488  0.479453   \n",
       "\n",
       "    mean_per_class_error      rmse       mse  \n",
       "0               0.349509  0.370103  0.136977  \n",
       "1               0.351685  0.370262  0.137094  \n",
       "2               0.350829  0.370407  0.137202  \n",
       "3               0.353507  0.370225  0.137067  \n",
       "4               0.352786  0.370273  0.137102  \n",
       "5               0.353887  0.370509  0.137277  \n",
       "6               0.354178  0.370554  0.137310  \n",
       "7               0.354784  0.370872  0.137546  \n",
       "8               0.354552  0.371075  0.137696  \n",
       "9               0.355085  0.372547  0.138791  \n",
       "10              0.354668  0.371441  0.137968  \n",
       "11              0.354668  0.371441  0.137968  \n",
       "12              0.357903  0.371964  0.138357  \n",
       "13              0.361761  0.372926  0.139073  \n",
       "14              0.358243  0.372862  0.139026  \n",
       "15              0.362873  0.373139  0.139233  \n",
       "16              0.363898  0.373311  0.139361  \n",
       "17              0.364619  0.382286  0.146143  \n",
       "18              0.365550  0.382442  0.146262  \n",
       "19              0.366846  0.383622  0.147166  \n",
       "20              0.367447  0.383614  0.147160  \n",
       "21              0.368185  0.383608  0.147155  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "aml_leaderboard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting models\n",
    "\n",
    "Individul models can ne found through a search of the leader board or directly by the name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(aml.leader.metalearner()['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DRF_0_AutoML_20181120_153207': 0.0,\n",
       " 'DRF_0_AutoML_20181120_153506': 0.0,\n",
       " 'DRF_0_AutoML_20181120_154205': 0.0,\n",
       " 'DeepLearning_0_AutoML_20181120_153506': 0.1344838464922838,\n",
       " 'DeepLearning_0_AutoML_20181120_154205': 0.16114849497175607,\n",
       " 'GBM_grid_0_AutoML_20181120_153506_model_0': 0.06857343556357838,\n",
       " 'GBM_grid_0_AutoML_20181120_153506_model_1': 0.06556630998879215,\n",
       " 'GBM_grid_0_AutoML_20181120_153506_model_2': 0.06399896524231677,\n",
       " 'GBM_grid_0_AutoML_20181120_153506_model_3': 0.0,\n",
       " 'GBM_grid_0_AutoML_20181120_153506_model_4': 0.029475852258499607,\n",
       " 'GBM_grid_0_AutoML_20181120_154205_model_0': 0.10436142902876248,\n",
       " 'GBM_grid_0_AutoML_20181120_154205_model_1': 0.030089245243887962,\n",
       " 'GBM_grid_0_AutoML_20181120_154205_model_2': 0.024832084306272362,\n",
       " 'GBM_grid_0_AutoML_20181120_154205_model_3': 0.0,\n",
       " 'GBM_grid_0_AutoML_20181120_154205_model_4': 0.03820562737398706,\n",
       " 'GLM_grid_0_AutoML_20181120_153506_model_0': 0.0,\n",
       " 'GLM_grid_0_AutoML_20181120_154205_model_0': 0.0,\n",
       " 'Intercept': -1.648080783111384,\n",
       " 'XRT_0_AutoML_20181120_153506': 0.0,\n",
       " 'XRT_0_AutoML_20181120_154205': 0.006042541276377857}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metalearner.coef_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bear/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAAJTCAYAAABEh264AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGsKigyRJkiRJaoj/D3tJt1UkfPCrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting a model directly by name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>mean_per_class_error</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StackedEnsemble_AllModels_0_AutoML_20181120_15...</td>\n",
       "      <td>0.707120</td>\n",
       "      <td>0.435531</td>\n",
       "      <td>0.349509</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.136977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StackedEnsemble_AllModels_0_AutoML_20181120_15...</td>\n",
       "      <td>0.706222</td>\n",
       "      <td>0.435883</td>\n",
       "      <td>0.351685</td>\n",
       "      <td>0.370262</td>\n",
       "      <td>0.137094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StackedEnsemble_BestOfFamily_0_AutoML_20181120...</td>\n",
       "      <td>0.705212</td>\n",
       "      <td>0.436220</td>\n",
       "      <td>0.350829</td>\n",
       "      <td>0.370407</td>\n",
       "      <td>0.137202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_154205_model_0</td>\n",
       "      <td>0.703839</td>\n",
       "      <td>0.435055</td>\n",
       "      <td>0.353507</td>\n",
       "      <td>0.370225</td>\n",
       "      <td>0.137067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBM_grid_0_AutoML_20181120_153506_model_0</td>\n",
       "      <td>0.703216</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>0.352786</td>\n",
       "      <td>0.370273</td>\n",
       "      <td>0.137102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id       auc   logloss  \\\n",
       "0  StackedEnsemble_AllModels_0_AutoML_20181120_15...  0.707120  0.435531   \n",
       "1  StackedEnsemble_AllModels_0_AutoML_20181120_15...  0.706222  0.435883   \n",
       "2  StackedEnsemble_BestOfFamily_0_AutoML_20181120...  0.705212  0.436220   \n",
       "3          GBM_grid_0_AutoML_20181120_154205_model_0  0.703839  0.435055   \n",
       "4          GBM_grid_0_AutoML_20181120_153506_model_0  0.703216  0.435227   \n",
       "\n",
       "   mean_per_class_error      rmse       mse  \n",
       "0              0.349509  0.370103  0.136977  \n",
       "1              0.351685  0.370262  0.137094  \n",
       "2              0.350829  0.370407  0.137202  \n",
       "3              0.353507  0.370225  0.137067  \n",
       "4              0.352786  0.370273  0.137102  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_leaderboard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM_grid_0_AutoML_20181120_154205_model_0\n",
      "GBM_grid_0_AutoML_20181120_153506_model_0\n",
      "GBM_grid_0_AutoML_20181120_153506_model_1\n",
      "GBM_grid_0_AutoML_20181120_154205_model_1\n",
      "GBM_grid_0_AutoML_20181120_153506_model_2\n",
      "GBM_grid_0_AutoML_20181120_154205_model_2\n",
      "DeepLearning_0_AutoML_20181120_154205\n",
      "GLM_grid_0_AutoML_20181120_154205_model_0\n",
      "GLM_grid_0_AutoML_20181120_153506_model_0\n",
      "DeepLearning_0_AutoML_20181120_153506\n",
      "GBM_grid_0_AutoML_20181120_154205_model_4\n",
      "GBM_grid_0_AutoML_20181120_153506_model_4\n",
      "GBM_grid_0_AutoML_20181120_153506_model_3\n",
      "GBM_grid_0_AutoML_20181120_154205_model_3\n",
      "XRT_0_AutoML_20181120_154205\n",
      "XRT_0_AutoML_20181120_153506\n",
      "DRF_0_AutoML_20181120_154205\n",
      "DRF_0_AutoML_20181120_153506\n",
      "DRF_0_AutoML_20181120_153207\n",
      "model_id  GBM_grid_0_AutoML_20181120_154205_model_0\n"
     ]
    }
   ],
   "source": [
    "m_id=''\n",
    "for model in aml_leaderboard_df['model_id']:\n",
    "    if 'StackedEnsemble' not in model:\n",
    "      print (model)\n",
    "      if m_id=='':\n",
    "            m_id=model\n",
    "print (\"model_id \", m_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm\n"
     ]
    }
   ],
   "source": [
    "non_stacked= h2o.get_model(m_id)\n",
    "print (non_stacked.algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'balance_classes',\n",
       " 'biases',\n",
       " 'build_tree_one_node',\n",
       " 'calibrate_model',\n",
       " 'calibration_frame',\n",
       " 'categorical_encoding',\n",
       " 'catoffsets',\n",
       " 'checkpoint',\n",
       " 'class_sampling_factors',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'col_sample_rate',\n",
       " 'col_sample_rate_change_per_level',\n",
       " 'col_sample_rate_per_tree',\n",
       " 'confusion_matrix',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'custom_metric_func',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'distribution',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fold_assignment',\n",
       " 'fold_column',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'histogram_type',\n",
       " 'huber_alpha',\n",
       " 'ignore_const_cols',\n",
       " 'ignored_columns',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_cross_validation_fold_assignment',\n",
       " 'keep_cross_validation_predictions',\n",
       " 'learn_rate',\n",
       " 'learn_rate_annealing',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_abs_leafnode_pred',\n",
       " 'max_after_balance_size',\n",
       " 'max_confusion_matrix_size',\n",
       " 'max_depth',\n",
       " 'max_hit_ratio_k',\n",
       " 'max_per_class_error',\n",
       " 'max_runtime_secs',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metric',\n",
       " 'min_rows',\n",
       " 'min_split_improvement',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'nbins',\n",
       " 'nbins_cats',\n",
       " 'nbins_top_level',\n",
       " 'nfolds',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'ntrees',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'offset_column',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'pred_noise_bandwidth',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'quantile_alpha',\n",
       " 'r2',\n",
       " 'r2_stopping',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'sample_rate',\n",
       " 'sample_rate_per_class',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_each_iteration',\n",
       " 'score_history',\n",
       " 'score_tree_interval',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'stopping_metric',\n",
       " 'stopping_rounds',\n",
       " 'stopping_tolerance',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'tweedie_power',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'weights_column',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(non_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since this is a pandas dataframe the data can be saved.\n",
    "\n",
    "The type of exploration depends on the learner.  If the learner isn't an ensemble then ensemble exploration doesn't make sense.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the variable importance of the metalearner (combiner) algorithm in the ensemble.  This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Leader Model\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/models/StackedEnsemble_AllModels_0_AutoML_20181120_154205'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.save_model(aml.leader, path = \"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/models/StackedEnsemble_AllModels_0_AutoML_20181120_154205.zip'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.download_mojo(path = \"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "If one wants predictions the user will do this on new data.\n",
    "\n",
    "Here we are taking 10% of original file just to show the syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and test for showing how to predict\n",
    "train, test = df.split_frame([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using Leader Model\n",
    "\n",
    "If you need to generate predictions on a test set, you can make predictions on the `\"H2OAutoML\"` object directly, or on the leader model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.709939</td><td style=\"text-align: right;\">0.290061 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.645258</td><td style=\"text-align: right;\">0.354742 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.701456</td><td style=\"text-align: right;\">0.298544 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.865275</td><td style=\"text-align: right;\">0.134725 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.899026</td><td style=\"text-align: right;\">0.100974 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.865134</td><td style=\"text-align: right;\">0.134866 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.624003</td><td style=\"text-align: right;\">0.375997 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.921187</td><td style=\"text-align: right;\">0.0788131</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.903194</td><td style=\"text-align: right;\">0.0968065</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.925731</td><td style=\"text-align: right;\">0.0742695</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = aml.predict(test)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_performance()\n",
    "\n",
    "The standard `model_performance()` method can be applied to the AutoML leader model and a test set to generate an H2O model performance object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.13055715415749164\n",
      "RMSE: 0.36132693527813786\n",
      "LogLoss: 0.41655140391705325\n",
      "Null degrees of freedom: 32682\n",
      "Residual degrees of freedom: 32671\n",
      "Null deviance: 31504.82333185961\n",
      "Residual deviance: 27228.299068442102\n",
      "AIC: 27252.299068442102\n",
      "AUC: 0.764167884838059\n",
      "Gini: 0.528335769676118\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21295302564978025: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>20323.0</td>\n",
       "<td>6247.0</td>\n",
       "<td>0.2351</td>\n",
       "<td> (6247.0/26570.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2383.0</td>\n",
       "<td>3730.0</td>\n",
       "<td>0.3898</td>\n",
       "<td> (2383.0/6113.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>22706.0</td>\n",
       "<td>9977.0</td>\n",
       "<td>0.2641</td>\n",
       "<td> (8630.0/32683.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      20323  6247  0.2351   (6247.0/26570.0)\n",
       "1      2383   3730  0.3898   (2383.0/6113.0)\n",
       "Total  22706  9977  0.2641   (8630.0/32683.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2129530</td>\n",
       "<td>0.4636420</td>\n",
       "<td>234.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1407219</td>\n",
       "<td>0.6066398</td>\n",
       "<td>306.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3219515</td>\n",
       "<td>0.4539944</td>\n",
       "<td>156.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4260168</td>\n",
       "<td>0.8235168</td>\n",
       "<td>99.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.6726948</td>\n",
       "<td>0.8888889</td>\n",
       "<td>9.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0679730</td>\n",
       "<td>1.0</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7559667</td>\n",
       "<td>0.9999624</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2294254</td>\n",
       "<td>0.3180632</td>\n",
       "<td>221.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1851642</td>\n",
       "<td>0.6908228</td>\n",
       "<td>260.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1802490</td>\n",
       "<td>0.6935079</td>\n",
       "<td>265.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.212953     0.463642  234\n",
       "max f2                       0.140722     0.60664   306\n",
       "max f0point5                 0.321952     0.453994  156\n",
       "max accuracy                 0.426017     0.823517  99\n",
       "max precision                0.672695     0.888889  9\n",
       "max recall                   0.067973     1         393\n",
       "max specificity              0.755967     0.999962  0\n",
       "max absolute_mcc             0.229425     0.318063  221\n",
       "max min_per_class_accuracy   0.185164     0.690823  260\n",
       "max mean_per_class_accuracy  0.180249     0.693508  265"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.70 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100052</td>\n",
       "<td>0.5792591</td>\n",
       "<td>3.9240181</td>\n",
       "<td>3.9240181</td>\n",
       "<td>0.7339450</td>\n",
       "<td>0.7339450</td>\n",
       "<td>0.0392606</td>\n",
       "<td>0.0392606</td>\n",
       "<td>292.4018148</td>\n",
       "<td>292.4018148</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200104</td>\n",
       "<td>0.5330392</td>\n",
       "<td>3.3681156</td>\n",
       "<td>3.6460669</td>\n",
       "<td>0.6299694</td>\n",
       "<td>0.6819572</td>\n",
       "<td>0.0336987</td>\n",
       "<td>0.0729593</td>\n",
       "<td>236.8115577</td>\n",
       "<td>264.6066862</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300156</td>\n",
       "<td>0.4993342</td>\n",
       "<td>3.0738142</td>\n",
       "<td>3.4553160</td>\n",
       "<td>0.5749235</td>\n",
       "<td>0.6462793</td>\n",
       "<td>0.0307541</td>\n",
       "<td>0.1037134</td>\n",
       "<td>207.3814216</td>\n",
       "<td>245.5315980</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400208</td>\n",
       "<td>0.4721518</td>\n",
       "<td>2.6977625</td>\n",
       "<td>3.2659276</td>\n",
       "<td>0.5045872</td>\n",
       "<td>0.6108563</td>\n",
       "<td>0.0269917</td>\n",
       "<td>0.1307051</td>\n",
       "<td>169.7762476</td>\n",
       "<td>226.5927604</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500260</td>\n",
       "<td>0.4484122</td>\n",
       "<td>2.9430136</td>\n",
       "<td>3.2013448</td>\n",
       "<td>0.5504587</td>\n",
       "<td>0.5987768</td>\n",
       "<td>0.0294454</td>\n",
       "<td>0.1601505</td>\n",
       "<td>194.3013611</td>\n",
       "<td>220.1344805</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000214</td>\n",
       "<td>0.3612914</td>\n",
       "<td>2.3820279</td>\n",
       "<td>2.7918117</td>\n",
       "<td>0.4455324</td>\n",
       "<td>0.5221780</td>\n",
       "<td>0.1190905</td>\n",
       "<td>0.2792410</td>\n",
       "<td>138.2027907</td>\n",
       "<td>179.1811672</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500168</td>\n",
       "<td>0.3076471</td>\n",
       "<td>1.9992020</td>\n",
       "<td>2.5276623</td>\n",
       "<td>0.3739290</td>\n",
       "<td>0.4727718</td>\n",
       "<td>0.0999509</td>\n",
       "<td>0.3791919</td>\n",
       "<td>99.9201994</td>\n",
       "<td>152.7662332</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000122</td>\n",
       "<td>0.2681940</td>\n",
       "<td>1.7145366</td>\n",
       "<td>2.3244120</td>\n",
       "<td>0.3206854</td>\n",
       "<td>0.4347560</td>\n",
       "<td>0.0857190</td>\n",
       "<td>0.4649108</td>\n",
       "<td>71.4536571</td>\n",
       "<td>132.4411989</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000031</td>\n",
       "<td>0.2147773</td>\n",
       "<td>1.3873349</td>\n",
       "<td>2.0120848</td>\n",
       "<td>0.2594859</td>\n",
       "<td>0.3763386</td>\n",
       "<td>0.1387208</td>\n",
       "<td>0.6036316</td>\n",
       "<td>38.7334935</td>\n",
       "<td>101.2084828</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999939</td>\n",
       "<td>0.1779800</td>\n",
       "<td>1.0993975</td>\n",
       "<td>1.7839304</td>\n",
       "<td>0.2056304</td>\n",
       "<td>0.3336648</td>\n",
       "<td>0.1099297</td>\n",
       "<td>0.7135613</td>\n",
       "<td>9.9397496</td>\n",
       "<td>78.3930449</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000153</td>\n",
       "<td>0.1507619</td>\n",
       "<td>0.8880807</td>\n",
       "<td>1.6047276</td>\n",
       "<td>0.1661058</td>\n",
       "<td>0.3001469</td>\n",
       "<td>0.0888271</td>\n",
       "<td>0.8023884</td>\n",
       "<td>-11.1919310</td>\n",
       "<td>60.4727606</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000061</td>\n",
       "<td>0.1301321</td>\n",
       "<td>0.6854874</td>\n",
       "<td>1.4515365</td>\n",
       "<td>0.1282130</td>\n",
       "<td>0.2714941</td>\n",
       "<td>0.0685425</td>\n",
       "<td>0.8709308</td>\n",
       "<td>-31.4512573</td>\n",
       "<td>45.1536534</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999969</td>\n",
       "<td>0.1126705</td>\n",
       "<td>0.5137066</td>\n",
       "<td>1.3175725</td>\n",
       "<td>0.0960832</td>\n",
       "<td>0.2464376</td>\n",
       "<td>0.0513659</td>\n",
       "<td>0.9222967</td>\n",
       "<td>-48.6293432</td>\n",
       "<td>31.7572537</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999878</td>\n",
       "<td>0.0979212</td>\n",
       "<td>0.4466302</td>\n",
       "<td>1.2087131</td>\n",
       "<td>0.0835373</td>\n",
       "<td>0.2260766</td>\n",
       "<td>0.0446589</td>\n",
       "<td>0.9669557</td>\n",
       "<td>-55.3369767</td>\n",
       "<td>20.8713077</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999786</td>\n",
       "<td>0.0825174</td>\n",
       "<td>0.2486732</td>\n",
       "<td>1.1020492</td>\n",
       "<td>0.0465116</td>\n",
       "<td>0.2061263</td>\n",
       "<td>0.0248650</td>\n",
       "<td>0.9918207</td>\n",
       "<td>-75.1326757</td>\n",
       "<td>10.2049237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0605814</td>\n",
       "<td>0.0817754</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0152952</td>\n",
       "<td>0.1870391</td>\n",
       "<td>0.0081793</td>\n",
       "<td>1.0</td>\n",
       "<td>-91.8224614</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100052                   0.579259           3.92402    3.92402            0.733945         0.733945                    0.0392606       0.0392606                  292.402   292.402\n",
       "    2        0.0200104                   0.533039           3.36812    3.64607            0.629969         0.681957                    0.0336987       0.0729593                  236.812   264.607\n",
       "    3        0.0300156                   0.499334           3.07381    3.45532            0.574924         0.646279                    0.0307541       0.103713                   207.381   245.532\n",
       "    4        0.0400208                   0.472152           2.69776    3.26593            0.504587         0.610856                    0.0269917       0.130705                   169.776   226.593\n",
       "    5        0.050026                    0.448412           2.94301    3.20134            0.550459         0.598777                    0.0294454       0.16015                    194.301   220.134\n",
       "    6        0.100021                    0.361291           2.38203    2.79181            0.445532         0.522178                    0.11909         0.279241                   138.203   179.181\n",
       "    7        0.150017                    0.307647           1.9992     2.52766            0.373929         0.472772                    0.0999509       0.379192                   99.9202   152.766\n",
       "    8        0.200012                    0.268194           1.71454    2.32441            0.320685         0.434756                    0.085719        0.464911                   71.4537   132.441\n",
       "    9        0.300003                    0.214777           1.38733    2.01208            0.259486         0.376339                    0.138721        0.603632                   38.7335   101.208\n",
       "    10       0.399994                    0.17798            1.0994     1.78393            0.20563          0.333665                    0.10993         0.713561                   9.93975   78.393\n",
       "    11       0.500015                    0.150762           0.888081   1.60473            0.166106         0.300147                    0.0888271       0.802388                   -11.1919  60.4728\n",
       "    12       0.600006                    0.130132           0.685487   1.45154            0.128213         0.271494                    0.0685425       0.870931                   -31.4513  45.1537\n",
       "    13       0.699997                    0.11267            0.513707   1.31757            0.0960832        0.246438                    0.0513659       0.922297                   -48.6293  31.7573\n",
       "    14       0.799988                    0.0979212          0.44663    1.20871            0.0835373        0.226077                    0.0446589       0.966956                   -55.337   20.8713\n",
       "    15       0.899979                    0.0825174          0.248673   1.10205            0.0465116        0.206126                    0.024865        0.991821                   -75.1327  10.2049\n",
       "    16       1                           0.0605814          0.0817754  1                  0.0152952        0.187039                    0.00817929      1                          -91.8225  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = aml.leader.model_performance(test)\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_algo',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_has',\n",
       " '_metric_json',\n",
       " '_on_train',\n",
       " '_on_valid',\n",
       " '_on_xval',\n",
       " 'accuracy',\n",
       " 'aic',\n",
       " 'auc',\n",
       " 'confusion_matrix',\n",
       " 'custom_metric_name',\n",
       " 'custom_metric_value',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'fprs',\n",
       " 'gains_lift',\n",
       " 'gini',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'make',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mse',\n",
       " 'nobs',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'plot',\n",
       " 'precision',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'sensitivity',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'tprs']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21295302564978025: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>20323.0</td>\n",
       "<td>6247.0</td>\n",
       "<td>0.2351</td>\n",
       "<td> (6247.0/26570.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2383.0</td>\n",
       "<td>3730.0</td>\n",
       "<td>0.3898</td>\n",
       "<td> (2383.0/6113.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>22706.0</td>\n",
       "<td>9977.0</td>\n",
       "<td>0.2641</td>\n",
       "<td> (8630.0/32683.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      20323  6247  0.2351   (6247.0/26570.0)\n",
       "1      2383   3730  0.3898   (2383.0/6113.0)\n",
       "Total  22706  9977  0.2641   (8630.0/32683.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=perf.confusion_matrix()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_algo',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_has',\n",
       " '_metric_json',\n",
       " '_on_train',\n",
       " '_on_valid',\n",
       " '_on_xval',\n",
       " 'accuracy',\n",
       " 'aic',\n",
       " 'auc',\n",
       " 'confusion_matrix',\n",
       " 'custom_metric_name',\n",
       " 'custom_metric_value',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'fprs',\n",
       " 'gains_lift',\n",
       " 'gini',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'make',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mse',\n",
       " 'nobs',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'plot',\n",
       " 'precision',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'sensitivity',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'tprs']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In R we get plots like:\n",
    "    \n",
    "    #compute performance\n",
    "perf <- h2o.performance(automl_leader,conv_data.hex)\n",
    "h2o.confusionMatrix(perf)\n",
    "h2o.accuracy(perf)\n",
    "h2o.tpr(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackedensemble'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'base_models',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'confusion_matrix',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_levelone_frame',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metalearner_algorithm',\n",
       " 'metalearner_fold_assignment',\n",
       " 'metalearner_fold_column',\n",
       " 'metalearner_nfolds',\n",
       " 'metalearner_params',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aml.leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764167884838059"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.model_performance(test).auc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12685990330985406\n",
      "RMSE: 0.3561739789904002\n",
      "LogLoss: 0.40703958453227496\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 106607.73758484815\n",
      "AIC: 106631.73758484815\n",
      "AUC: 0.7734040182726795\n",
      "Gini: 0.5468080365453589\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21708051286464972: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>83212.0</td>\n",
       "<td>23869.0</td>\n",
       "<td>0.2229</td>\n",
       "<td> (23869.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9376.0</td>\n",
       "<td>14498.0</td>\n",
       "<td>0.3927</td>\n",
       "<td> (9376.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>92588.0</td>\n",
       "<td>38367.0</td>\n",
       "<td>0.2539</td>\n",
       "<td> (33245.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      83212  23869  0.2229   (23869.0/107081.0)\n",
       "1      9376   14498  0.3927   (9376.0/23874.0)\n",
       "Total  92588  38367  0.2539   (33245.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2170805</td>\n",
       "<td>0.4658666</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1410190</td>\n",
       "<td>0.6067433</td>\n",
       "<td>309.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3338429</td>\n",
       "<td>0.4568693</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4390853</td>\n",
       "<td>0.8290100</td>\n",
       "<td>98.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0676438</td>\n",
       "<td>1.0</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2335835</td>\n",
       "<td>0.3269004</td>\n",
       "<td>224.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1855661</td>\n",
       "<td>0.6993958</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1761337</td>\n",
       "<td>0.7006208</td>\n",
       "<td>274.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.217081     0.465867  238\n",
       "max f2                       0.141019     0.606743  309\n",
       "max f0point5                 0.333843     0.456869  155\n",
       "max accuracy                 0.439085     0.82901   98\n",
       "max precision                0.800677     1         0\n",
       "max recall                   0.0676438    1         394\n",
       "max specificity              0.800677     1         0\n",
       "max absolute_mcc             0.233584     0.3269    224\n",
       "max min_per_class_accuracy   0.185566     0.699396  265\n",
       "max mean_per_class_accuracy  0.176134     0.700621  274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5809896</td>\n",
       "<td>4.2290904</td>\n",
       "<td>4.2290904</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.0423054</td>\n",
       "<td>0.0423054</td>\n",
       "<td>322.9090447</td>\n",
       "<td>322.9090447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5350310</td>\n",
       "<td>3.4879528</td>\n",
       "<td>3.8585216</td>\n",
       "<td>0.6358779</td>\n",
       "<td>0.7034351</td>\n",
       "<td>0.0348915</td>\n",
       "<td>0.0771970</td>\n",
       "<td>248.7952815</td>\n",
       "<td>285.8521631</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5007744</td>\n",
       "<td>3.1972882</td>\n",
       "<td>3.6382227</td>\n",
       "<td>0.5828877</td>\n",
       "<td>0.6632731</td>\n",
       "<td>0.0319595</td>\n",
       "<td>0.1091564</td>\n",
       "<td>219.7288214</td>\n",
       "<td>263.8222689</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4718430</td>\n",
       "<td>2.9436144</td>\n",
       "<td>3.4645375</td>\n",
       "<td>0.5366412</td>\n",
       "<td>0.6316091</td>\n",
       "<td>0.0294463</td>\n",
       "<td>0.1386027</td>\n",
       "<td>194.3614440</td>\n",
       "<td>246.4537481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4483865</td>\n",
       "<td>2.8704357</td>\n",
       "<td>3.3457716</td>\n",
       "<td>0.5233002</td>\n",
       "<td>0.6099572</td>\n",
       "<td>0.0286923</td>\n",
       "<td>0.1672950</td>\n",
       "<td>187.0435684</td>\n",
       "<td>234.5771560</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3623038</td>\n",
       "<td>2.4444070</td>\n",
       "<td>2.8950893</td>\n",
       "<td>0.4456323</td>\n",
       "<td>0.5277947</td>\n",
       "<td>0.1222250</td>\n",
       "<td>0.2895200</td>\n",
       "<td>144.4406963</td>\n",
       "<td>189.5089261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3079302</td>\n",
       "<td>1.9878608</td>\n",
       "<td>2.5926798</td>\n",
       "<td>0.3624007</td>\n",
       "<td>0.4726634</td>\n",
       "<td>0.0993968</td>\n",
       "<td>0.3889168</td>\n",
       "<td>98.7860769</td>\n",
       "<td>159.2679764</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2684456</td>\n",
       "<td>1.7116814</td>\n",
       "<td>2.3724554</td>\n",
       "<td>0.3120513</td>\n",
       "<td>0.4325150</td>\n",
       "<td>0.0855743</td>\n",
       "<td>0.4744911</td>\n",
       "<td>71.1681359</td>\n",
       "<td>137.2455391</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2132009</td>\n",
       "<td>1.4102670</td>\n",
       "<td>2.0517178</td>\n",
       "<td>0.2571014</td>\n",
       "<td>0.3740423</td>\n",
       "<td>0.1410321</td>\n",
       "<td>0.6155232</td>\n",
       "<td>41.0267006</td>\n",
       "<td>105.1717765</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1765625</td>\n",
       "<td>1.1217652</td>\n",
       "<td>1.8192385</td>\n",
       "<td>0.2045055</td>\n",
       "<td>0.3316597</td>\n",
       "<td>0.1121722</td>\n",
       "<td>0.7276954</td>\n",
       "<td>12.1765206</td>\n",
       "<td>81.9238502</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1503145</td>\n",
       "<td>0.8745582</td>\n",
       "<td>1.6302967</td>\n",
       "<td>0.1594380</td>\n",
       "<td>0.2972143</td>\n",
       "<td>0.0874592</td>\n",
       "<td>0.8151546</td>\n",
       "<td>-12.5441786</td>\n",
       "<td>63.0296674</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1300345</td>\n",
       "<td>0.6769128</td>\n",
       "<td>1.4714054</td>\n",
       "<td>0.1234059</td>\n",
       "<td>0.2682474</td>\n",
       "<td>0.0676887</td>\n",
       "<td>0.8828433</td>\n",
       "<td>-32.3087165</td>\n",
       "<td>47.1405434</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1129563</td>\n",
       "<td>0.4959559</td>\n",
       "<td>1.3320601</td>\n",
       "<td>0.0904162</td>\n",
       "<td>0.2428437</td>\n",
       "<td>0.0495937</td>\n",
       "<td>0.9324370</td>\n",
       "<td>-50.4044061</td>\n",
       "<td>33.2060067</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0978849</td>\n",
       "<td>0.3920433</td>\n",
       "<td>1.2145535</td>\n",
       "<td>0.0714722</td>\n",
       "<td>0.2214215</td>\n",
       "<td>0.0392058</td>\n",
       "<td>0.9716428</td>\n",
       "<td>-60.7956663</td>\n",
       "<td>21.4553489</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0824531</td>\n",
       "<td>0.2194940</td>\n",
       "<td>1.1039951</td>\n",
       "<td>0.0400153</td>\n",
       "<td>0.2012659</td>\n",
       "<td>0.0219486</td>\n",
       "<td>0.9935914</td>\n",
       "<td>-78.0505987</td>\n",
       "<td>10.3995078</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597755</td>\n",
       "<td>0.0640840</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0116830</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0064086</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.5915993</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.58099            4.22909   4.22909            0.770992         0.770992                    0.0423054       0.0423054                  322.909   322.909\n",
       "    2        0.0200069                   0.535031           3.48795   3.85852            0.635878         0.703435                    0.0348915       0.077197                   248.795   285.852\n",
       "    3        0.0300027                   0.500774           3.19729   3.63822            0.582888         0.663273                    0.0319595       0.109156                   219.729   263.822\n",
       "    4        0.0400061                   0.471843           2.94361   3.46454            0.536641         0.631609                    0.0294463       0.138603                   194.361   246.454\n",
       "    5        0.0500019                   0.448387           2.87044   3.34577            0.5233           0.609957                    0.0286923       0.167295                   187.044   234.577\n",
       "    6        0.100004                    0.362304           2.44441   2.89509            0.445632         0.527795                    0.122225        0.28952                    144.441   189.509\n",
       "    7        0.150006                    0.30793            1.98786   2.59268            0.362401         0.472663                    0.0993968       0.388917                   98.7861   159.268\n",
       "    8        0.2                         0.268446           1.71168   2.37246            0.312051         0.432515                    0.0855743       0.474491                   71.1681   137.246\n",
       "    9        0.300004                    0.213201           1.41027   2.05172            0.257101         0.374042                    0.141032        0.615523                   41.0267   105.172\n",
       "    10       0.4                         0.176563           1.12177   1.81924            0.204506         0.33166                     0.112172        0.727695                   12.1765   81.9239\n",
       "    11       0.500004                    0.150314           0.874558  1.6303             0.159438         0.297214                    0.0874592       0.815155                   -12.5442  63.0297\n",
       "    12       0.6                         0.130034           0.676913  1.47141            0.123406         0.268247                    0.0676887       0.882843                   -32.3087  47.1405\n",
       "    13       0.699996                    0.112956           0.495956  1.33206            0.0904162        0.242844                    0.0495937       0.932437                   -50.4044  33.206\n",
       "    14       0.8                         0.0978849          0.392043  1.21455            0.0714722        0.221421                    0.0392058       0.971643                   -60.7957  21.4553\n",
       "    15       0.899996                    0.0824531          0.219494  1.104              0.0400153        0.201266                    0.0219486       0.993591                   -78.0506  10.3995\n",
       "    16       1                           0.0597755          0.064084  1                  0.011683         0.182307                    0.00640865      1                          -93.5916  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_perf = aml.leader.model_performance()\n",
    "best_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUiolChERKdX/AxRMMCrOK7FzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_perf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21708051286464972: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>83212.0</td>\n",
       "<td>23869.0</td>\n",
       "<td>0.2229</td>\n",
       "<td> (23869.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9376.0</td>\n",
       "<td>14498.0</td>\n",
       "<td>0.3927</td>\n",
       "<td> (9376.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>92588.0</td>\n",
       "<td>38367.0</td>\n",
       "<td>0.2539</td>\n",
       "<td> (33245.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      83212  23869  0.2229   (23869.0/107081.0)\n",
       "1      9376   14498  0.3927   (9376.0/23874.0)\n",
       "Total  92588  38367  0.2539   (33245.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.confusion_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  2.8016174671510353e-05,\n",
       "  4.669362445251725e-05,\n",
       "  5.6032349343020706e-05,\n",
       "  9.33872489050345e-05,\n",
       "  0.00011206469868604141,\n",
       "  0.0001307421484670483,\n",
       "  0.00016809704802906212,\n",
       "  0.00023346812226258625,\n",
       "  0.0002988391964961104,\n",
       "  0.0003922264454011449,\n",
       "  0.00044825879474416565,\n",
       "  0.0005229685938681932,\n",
       "  0.000579000943211214,\n",
       "  0.0006256945676637312,\n",
       "  0.0007097430916782623,\n",
       "  0.0008031303405832968,\n",
       "  0.0008871788645978278,\n",
       "  0.0009805661135028623,\n",
       "  0.0011019695370794072,\n",
       "  0.0012046955108749452,\n",
       "  0.0012794053099989728,\n",
       "  0.001344776384232497,\n",
       "  0.0014475023580280348,\n",
       "  0.0015782445064950832,\n",
       "  0.001680970480290621,\n",
       "  0.0018584062532101867,\n",
       "  0.0020358420261297524,\n",
       "  0.0021665841745968006,\n",
       "  0.0022879875981733455,\n",
       "  0.0023720361221878767,\n",
       "  0.0025307944453264353,\n",
       "  0.002689552768464994,\n",
       "  0.002857649816494056,\n",
       "  0.003063101764085132,\n",
       "  0.0032405375370046976,\n",
       "  0.003558054183281815,\n",
       "  0.003791522305544401,\n",
       "  0.00400631297802598,\n",
       "  0.004258458550069574,\n",
       "  0.004547959021675181,\n",
       "  0.004893491842623808,\n",
       "  0.005117621239995891,\n",
       "  0.005453815336054015,\n",
       "  0.005799348157002643,\n",
       "  0.006051493729046236,\n",
       "  0.006397026549994864,\n",
       "  0.006686527021600471,\n",
       "  0.007041398567439602,\n",
       "  0.007424286287950243,\n",
       "  0.007536350986636285,\n",
       "  0.007909899982256424,\n",
       "  0.008330142602329078,\n",
       "  0.008703691597949216,\n",
       "  0.009123934218021872,\n",
       "  0.00955351556298503,\n",
       "  0.009917725833714664,\n",
       "  0.010263258654663293,\n",
       "  0.01072085617429796,\n",
       "  0.01085159832276501,\n",
       "  0.01120646986860414,\n",
       "  0.011626712488676796,\n",
       "  0.012168358532325996,\n",
       "  0.012401826654588582,\n",
       "  0.01287810162400426,\n",
       "  0.0132609893445149,\n",
       "  0.013625199615244535,\n",
       "  0.0138026353881641,\n",
       "  0.014456346130499342,\n",
       "  0.01477386277677646,\n",
       "  0.015212782846630121,\n",
       "  0.01572641271560781,\n",
       "  0.01628673620903802,\n",
       "  0.016529543056191108,\n",
       "  0.017127221449183327,\n",
       "  0.017603496418599006,\n",
       "  0.018145142462248205,\n",
       "  0.018313239510277267,\n",
       "  0.01889224045348848,\n",
       "  0.019387192872685165,\n",
       "  0.019816774217648322,\n",
       "  0.020349081536407018,\n",
       "  0.020769324156479675,\n",
       "  0.021254937850785853,\n",
       "  0.02178724516954455,\n",
       "  0.022496988261222813,\n",
       "  0.023225408802682083,\n",
       "  0.02389779699479833,\n",
       "  0.024243329815746957,\n",
       "  0.024803653309177165,\n",
       "  0.025326621903045358,\n",
       "  0.025933639020928084,\n",
       "  0.02640991399034376,\n",
       "  0.027082302182460008,\n",
       "  0.02782006144880978,\n",
       "  0.028408401116911498,\n",
       "  0.029276902531728318,\n",
       "  0.030033339247859097,\n",
       "  0.03077109851420887,\n",
       "  0.031518196505449146,\n",
       "  0.032358681745594454,\n",
       "  0.033068424837272715,\n",
       "  0.03366610323026494,\n",
       "  0.0343758463219432,\n",
       "  0.03509492813851197,\n",
       "  0.035711283981285195,\n",
       "  0.035982107003109796,\n",
       "  0.0368039147934741,\n",
       "  0.037494980435371354,\n",
       "  0.03806464265369206,\n",
       "  0.03876504702047982,\n",
       "  0.039577516085953626,\n",
       "  0.040455356225660946,\n",
       "  0.041286502740915755,\n",
       "  0.04230442375398063,\n",
       "  0.04335969966660752,\n",
       "  0.043929361884928235,\n",
       "  0.04503133142200764,\n",
       "  0.04589983283682446,\n",
       "  0.046693624452517256,\n",
       "  0.04754344841755307,\n",
       "  0.048402611107479385,\n",
       "  0.04940185467076325,\n",
       "  0.050298372260251585,\n",
       "  0.05110150260083488,\n",
       "  0.05202603636499473,\n",
       "  0.05288519905492104,\n",
       "  0.05340816764878924,\n",
       "  0.05437939503740159,\n",
       "  0.05537863860068546,\n",
       "  0.056424575788421846,\n",
       "  0.057218367404114644,\n",
       "  0.05832033694119405,\n",
       "  0.05908611238221533,\n",
       "  0.06037485641710481,\n",
       "  0.061803681325351835,\n",
       "  0.06276556998907369,\n",
       "  0.06405431402396317,\n",
       "  0.06506289631213755,\n",
       "  0.06606213987542141,\n",
       "  0.06743493243432541,\n",
       "  0.06785517505439807,\n",
       "  0.06905053184038251,\n",
       "  0.07018985627702394,\n",
       "  0.07082488956957816,\n",
       "  0.07188016548220506,\n",
       "  0.073047506093518,\n",
       "  0.07407476583147338,\n",
       "  0.07521409026811479,\n",
       "  0.07639076960431823,\n",
       "  0.07763282001475519,\n",
       "  0.07832388565665244,\n",
       "  0.07960329096665142,\n",
       "  0.08105079332467945,\n",
       "  0.08187260111504376,\n",
       "  0.08297457065212316,\n",
       "  0.08421662106256012,\n",
       "  0.08515049355161046,\n",
       "  0.0866166733594195,\n",
       "  0.08827896638992912,\n",
       "  0.08962374277416162,\n",
       "  0.09083777700992707,\n",
       "  0.09220123084394057,\n",
       "  0.09357402340284457,\n",
       "  0.09505888066043462,\n",
       "  0.09646902811890065,\n",
       "  0.0979912402760527,\n",
       "  0.09945742008386176,\n",
       "  0.10049401854670763,\n",
       "  0.1017547464069256,\n",
       "  0.10317423259028212,\n",
       "  0.1044723153500621,\n",
       "  0.10593849515787114,\n",
       "  0.10753541711414723,\n",
       "  0.10908564544597081,\n",
       "  0.11065455122757539,\n",
       "  0.11293320010085822,\n",
       "  0.1147542514545064,\n",
       "  0.11632315723611099,\n",
       "  0.1179667728168396,\n",
       "  0.11959171094778719,\n",
       "  0.12121664907873479,\n",
       "  0.12291629700880642,\n",
       "  0.12363537882537519,\n",
       "  0.12525097823143228,\n",
       "  0.12702533596062793,\n",
       "  0.12906117798675767,\n",
       "  0.13066743866792427,\n",
       "  0.1323857640477769,\n",
       "  0.13414144432719158,\n",
       "  0.13600918930529227,\n",
       "  0.13778354703448792,\n",
       "  0.13955790476368357,\n",
       "  0.14108011692083564,\n",
       "  0.14295720062382683,\n",
       "  0.14510510734864263,\n",
       "  0.14703822340097683,\n",
       "  0.1491207590515591,\n",
       "  0.15137139175017042,\n",
       "  0.15327649162783313,\n",
       "  0.15515357533082433,\n",
       "  0.15689991688534846,\n",
       "  0.15868361333943462,\n",
       "  0.16039259999439676,\n",
       "  0.16310083021264277,\n",
       "  0.16551022123439266,\n",
       "  0.16764878923431795,\n",
       "  0.1699647930071628,\n",
       "  0.17145898898964335,\n",
       "  0.17334541141752505,\n",
       "  0.1753812534436548,\n",
       "  0.17756651506803262,\n",
       "  0.17949029239547631,\n",
       "  0.18143274717270105,\n",
       "  0.18338454067481627,\n",
       "  0.18549509250007004,\n",
       "  0.18736283747817073,\n",
       "  0.18926793735583344,\n",
       "  0.1916212960282403,\n",
       "  0.19307813711115884,\n",
       "  0.19529141491020816,\n",
       "  0.1975420476088195,\n",
       "  0.1997646641327593,\n",
       "  0.20199661938158964,\n",
       "  0.20434997805399652,\n",
       "  0.2059469000102726,\n",
       "  0.2083749684818035,\n",
       "  0.21084973057778691,\n",
       "  0.21316573435063177,\n",
       "  0.2157152062457392,\n",
       "  0.21818062961683213,\n",
       "  0.22035655251631941,\n",
       "  0.22290602441142687,\n",
       "  0.22610920704886955,\n",
       "  0.2292376798871882,\n",
       "  0.23176847433251463,\n",
       "  0.23457009179966568,\n",
       "  0.2373903867165977,\n",
       "  0.2403227463322158,\n",
       "  0.2430216378255713,\n",
       "  0.24567383569447426,\n",
       "  0.24723340275118835,\n",
       "  0.2500256814934489,\n",
       "  0.25297671855884796,\n",
       "  0.255740981126437,\n",
       "  0.258430533894902,\n",
       "  0.2614095871349726,\n",
       "  0.2642112046021236,\n",
       "  0.2670221607941652,\n",
       "  0.26999187530934526,\n",
       "  0.2719997011608035,\n",
       "  0.2749974318506551,\n",
       "  0.2784060664356889,\n",
       "  0.28237502451415286,\n",
       "  0.28546614245290947,\n",
       "  0.2884918893174326,\n",
       "  0.2914242489330507,\n",
       "  0.2946647864700554,\n",
       "  0.29756912991100193,\n",
       "  0.30060421550041555,\n",
       "  0.30380739813785823,\n",
       "  0.30697322587573894,\n",
       "  0.3108954903297504,\n",
       "  0.3138838822947115,\n",
       "  0.3171804521810592,\n",
       "  0.3204770220674069,\n",
       "  0.3237922694035356,\n",
       "  0.3275744529841895,\n",
       "  0.3293021170889327,\n",
       "  0.3325800095254994,\n",
       "  0.3360353377349857,\n",
       "  0.3403498286343983,\n",
       "  0.34413201221505213,\n",
       "  0.3475780016996479,\n",
       "  0.3511267171580392,\n",
       "  0.35575872470372893,\n",
       "  0.35985842493065995,\n",
       "  0.3632577207908032,\n",
       "  0.3668064362491945,\n",
       "  0.37149447614422726,\n",
       "  0.3762105322139315,\n",
       "  0.3801608128426145,\n",
       "  0.38395233514815885,\n",
       "  0.3887431010169871,\n",
       "  0.3926280105714366,\n",
       "  0.3966903558988056,\n",
       "  0.40118228257113775,\n",
       "  0.407289808649527,\n",
       "  0.41139884760134854,\n",
       "  0.4155359027278415,\n",
       "  0.419476844631634,\n",
       "  0.42529487023841767,\n",
       "  0.4292544895919911,\n",
       "  0.43314873787133107,\n",
       "  0.4371457121244665,\n",
       "  0.44103062167891594,\n",
       "  0.44697938943416665,\n",
       "  0.45131255778336027,\n",
       "  0.45561770995788237,\n",
       "  0.4595493131367843,\n",
       "  0.464031901084226,\n",
       "  0.4684211017827626,\n",
       "  0.47288501228042323,\n",
       "  0.47761040707501795,\n",
       "  0.48214902737180265,\n",
       "  0.488555392646688,\n",
       "  0.4941212726814281,\n",
       "  0.49882799002624184,\n",
       "  0.5046646930828065,\n",
       "  0.5091192648555766,\n",
       "  0.5149839840868128,\n",
       "  0.5197373950560791,\n",
       "  0.5243507251519878,\n",
       "  0.529048103771911,\n",
       "  0.5334466431953381,\n",
       "  0.5404973804876683,\n",
       "  0.5451200493084675,\n",
       "  0.5500602347755438,\n",
       "  0.5548790168190435,\n",
       "  0.5595577179891857,\n",
       "  0.5667485361548734,\n",
       "  0.5715206245739207,\n",
       "  0.5767129556130406,\n",
       "  0.5829325463901159,\n",
       "  0.5892735405907678,\n",
       "  0.5944565329049971,\n",
       "  0.5996955575685696,\n",
       "  0.6049439209570325,\n",
       "  0.6111821891838888,\n",
       "  0.6159916325024981,\n",
       "  0.6218096581092818,\n",
       "  0.6269926504235112,\n",
       "  0.6324371270346747,\n",
       "  0.6392263800300707,\n",
       "  0.6472483447110131,\n",
       "  0.6564469887281591,\n",
       "  0.6622743530598332,\n",
       "  0.6674013130247196,\n",
       "  0.6740224689720865,\n",
       "  0.6818296429805475,\n",
       "  0.6880585724825132,\n",
       "  0.6919248045871816,\n",
       "  0.6972105228752066,\n",
       "  0.7026176445868081,\n",
       "  0.7106676254424221,\n",
       "  0.7176716691102997,\n",
       "  0.7231161457214632,\n",
       "  0.7317077726207264,\n",
       "  0.7390853652842241,\n",
       "  0.7450808266639273,\n",
       "  0.7504132385764047,\n",
       "  0.7560911833098308,\n",
       "  0.7630298559034749,\n",
       "  0.7687358168115725,\n",
       "  0.7760573771257272,\n",
       "  0.782267629177912,\n",
       "  0.7881603645838198,\n",
       "  0.7940157450901654,\n",
       "  0.7994228668017669,\n",
       "  0.8062401359718344,\n",
       "  0.8134402928624126,\n",
       "  0.8217424192900701,\n",
       "  0.8287838178575098,\n",
       "  0.8343123429926877,\n",
       "  0.8395513676562603,\n",
       "  0.8448370859442852,\n",
       "  0.8530925187474903,\n",
       "  0.8600498687909153,\n",
       "  0.8668671379609828,\n",
       "  0.8699769333495204,\n",
       "  0.8768035412444785,\n",
       "  0.882229340405861,\n",
       "  0.8880100111130826,\n",
       "  0.8963214762656307,\n",
       "  0.9017939690514657,\n",
       "  0.9071077035141621,\n",
       "  0.9127669707978072,\n",
       "  0.9204807575573631,\n",
       "  0.9262987831641468,\n",
       "  0.9325930837403461,\n",
       "  0.937897479478152,\n",
       "  0.9431551815915055,\n",
       "  0.9474696724909181,\n",
       "  0.9537639730671175,\n",
       "  0.9591617560538284,\n",
       "  0.967557269730391,\n",
       "  0.9740476835292909,\n",
       "  0.9785863038260756,\n",
       "  0.9817894864635183,\n",
       "  0.9877382542187689,\n",
       "  0.9927718269347503,\n",
       "  0.9959002997730689,\n",
       "  0.9989634015371541,\n",
       "  1.0],\n",
       " [8.377314233056883e-05,\n",
       "  0.00016754628466113765,\n",
       "  0.0004607522828181285,\n",
       "  0.0007539582809751194,\n",
       "  0.001130937421462679,\n",
       "  0.0014660299907849543,\n",
       "  0.0019267822736030828,\n",
       "  0.002345647985255927,\n",
       "  0.0029320599815699086,\n",
       "  0.003769791404875597,\n",
       "  0.004691295970511854,\n",
       "  0.005361481109156405,\n",
       "  0.006408645388288514,\n",
       "  0.006995057384602496,\n",
       "  0.007832788807908185,\n",
       "  0.008670520231213872,\n",
       "  0.01026220993549468,\n",
       "  0.011435033928122644,\n",
       "  0.012105219066767195,\n",
       "  0.01357124905755215,\n",
       "  0.0155818044734858,\n",
       "  0.016838401608444332,\n",
       "  0.017885565887576442,\n",
       "  0.018765183882047416,\n",
       "  0.0201474407305018,\n",
       "  0.020943285582642204,\n",
       "  0.022241769288766023,\n",
       "  0.02366591270838569,\n",
       "  0.024419870989360812,\n",
       "  0.026137220407137472,\n",
       "  0.027770796682583563,\n",
       "  0.029613805813856076,\n",
       "  0.031121722375806318,\n",
       "  0.03292284493591355,\n",
       "  0.034346988355533216,\n",
       "  0.03543603920583061,\n",
       "  0.03690206919661557,\n",
       "  0.03866130518555751,\n",
       "  0.0408394068861523,\n",
       "  0.042808075730920665,\n",
       "  0.04532127000083773,\n",
       "  0.047289938845606096,\n",
       "  0.04946804054620089,\n",
       "  0.051604255675630394,\n",
       "  0.05336349166457234,\n",
       "  0.05545782022283656,\n",
       "  0.05771969506576192,\n",
       "  0.06027477590684427,\n",
       "  0.06249476417860434,\n",
       "  0.0647566390215297,\n",
       "  0.06794001843009131,\n",
       "  0.07016000670185138,\n",
       "  0.07288263382759487,\n",
       "  0.07443243696071039,\n",
       "  0.07610789980732177,\n",
       "  0.07803468208092486,\n",
       "  0.07908184636005697,\n",
       "  0.08159504062997402,\n",
       "  0.08406634832872581,\n",
       "  0.08657954259864288,\n",
       "  0.08988858172070034,\n",
       "  0.09185725056546871,\n",
       "  0.0940353522660635,\n",
       "  0.0969674122476334,\n",
       "  0.09943871994638519,\n",
       "  0.10094663650833542,\n",
       "  0.1034179442070872,\n",
       "  0.10563793247884729,\n",
       "  0.1080254670352685,\n",
       "  0.1093658373125576,\n",
       "  0.11238167043645807,\n",
       "  0.11443411242355701,\n",
       "  0.11669598726648236,\n",
       "  0.1179525844014409,\n",
       "  0.12071709809834967,\n",
       "  0.12201558180447349,\n",
       "  0.12486386864371282,\n",
       "  0.1273351763424646,\n",
       "  0.12972271089888582,\n",
       "  0.13148194688782777,\n",
       "  0.133576275446092,\n",
       "  0.1365083354276619,\n",
       "  0.13881209684175252,\n",
       "  0.14015246711904164,\n",
       "  0.14287509424478512,\n",
       "  0.14538828851470217,\n",
       "  0.1485716679232638,\n",
       "  0.15112674876434615,\n",
       "  0.15351428332076736,\n",
       "  0.15657200301583313,\n",
       "  0.1590014241434196,\n",
       "  0.16205914383848538,\n",
       "  0.16482365753539416,\n",
       "  0.16746251151880706,\n",
       "  0.16934740722124486,\n",
       "  0.17144173577950908,\n",
       "  0.17449945547457485,\n",
       "  0.17755717516964062,\n",
       "  0.18053112172237581,\n",
       "  0.18249979056714416,\n",
       "  0.18501298483706125,\n",
       "  0.1867303342548379,\n",
       "  0.1905001256597135,\n",
       "  0.1931389796431264,\n",
       "  0.19590349334003518,\n",
       "  0.1987936667504398,\n",
       "  0.20051101616821648,\n",
       "  0.20356873586328222,\n",
       "  0.2058306107062076,\n",
       "  0.2087207841166122,\n",
       "  0.21161095752701684,\n",
       "  0.2141241517969339,\n",
       "  0.2154226355030577,\n",
       "  0.21848035519812348,\n",
       "  0.2216637346066851,\n",
       "  0.22426070201893272,\n",
       "  0.22681578286001508,\n",
       "  0.22983161598391555,\n",
       "  0.23226103711150206,\n",
       "  0.23494177766608026,\n",
       "  0.23858590935746,\n",
       "  0.24164362905252576,\n",
       "  0.24323531875680657,\n",
       "  0.247130769875178,\n",
       "  0.2504398089972355,\n",
       "  0.2536650749769624,\n",
       "  0.25601072296221833,\n",
       "  0.25860769037446596,\n",
       "  0.26262880120633325,\n",
       "  0.26539331490324203,\n",
       "  0.26773896288849797,\n",
       "  0.27062913629890256,\n",
       "  0.2742732679902823,\n",
       "  0.27599061740805897,\n",
       "  0.2787132445338025,\n",
       "  0.2822317165116863,\n",
       "  0.2849962302085951,\n",
       "  0.2877188573343386,\n",
       "  0.29060903074474326,\n",
       "  0.2926614727318422,\n",
       "  0.2963893775655525,\n",
       "  0.29999162268576696,\n",
       "  0.3032587752366591,\n",
       "  0.3065678143587166,\n",
       "  0.3092066683421295,\n",
       "  0.31247382089302167,\n",
       "  0.316494931724889,\n",
       "  0.31796096171567395,\n",
       "  0.32059981569908685,\n",
       "  0.3239507413923096,\n",
       "  0.3251235653849376,\n",
       "  0.32868392393398677,\n",
       "  0.33216050934070535,\n",
       "  0.33513445589344054,\n",
       "  0.33831783530200216,\n",
       "  0.34137555499706795,\n",
       "  0.3440981821228114,\n",
       "  0.3455223255424311,\n",
       "  0.34891513780681915,\n",
       "  0.3519309709307196,\n",
       "  0.35339700092150456,\n",
       "  0.3570411326128843,\n",
       "  0.3603082851637765,\n",
       "  0.36344977800117284,\n",
       "  0.36633995141157744,\n",
       "  0.37006785624528776,\n",
       "  0.3734606685096758,\n",
       "  0.37647650163357627,\n",
       "  0.3793247884728156,\n",
       "  0.3830526933065259,\n",
       "  0.3859009801457653,\n",
       "  0.3884141744156823,\n",
       "  0.3915137806819134,\n",
       "  0.39457150037697913,\n",
       "  0.3970009215045656,\n",
       "  0.40014241434196196,\n",
       "  0.40345145346401945,\n",
       "  0.40655105973025046,\n",
       "  0.4101533048504649,\n",
       "  0.41333668425902653,\n",
       "  0.4165619502387535,\n",
       "  0.41982910278964564,\n",
       "  0.4239758733350088,\n",
       "  0.4277875513110497,\n",
       "  0.4305939515791237,\n",
       "  0.43352601156069365,\n",
       "  0.43708637010974283,\n",
       "  0.4403116360894697,\n",
       "  0.4433274692133702,\n",
       "  0.4451285917734774,\n",
       "  0.4483538577532043,\n",
       "  0.4515372371617659,\n",
       "  0.4551813688531457,\n",
       "  0.45849040797520313,\n",
       "  0.4621764262377482,\n",
       "  0.4653179190751445,\n",
       "  0.46896205076652425,\n",
       "  0.47273184217139985,\n",
       "  0.4758314484376309,\n",
       "  0.47834464270754795,\n",
       "  0.48165368182960544,\n",
       "  0.4850046075228282,\n",
       "  0.48781100779090225,\n",
       "  0.4919158917651001,\n",
       "  0.49660718773561197,\n",
       "  0.5,\n",
       "  0.5036022451202145,\n",
       "  0.5062829856747927,\n",
       "  0.5097595710815113,\n",
       "  0.512817290776577,\n",
       "  0.5172991538912625,\n",
       "  0.5211108318673033,\n",
       "  0.5241685515623691,\n",
       "  0.5276032503979224,\n",
       "  0.529907011812013,\n",
       "  0.5330485046494094,\n",
       "  0.5363994303426322,\n",
       "  0.5394990366088632,\n",
       "  0.5428080757309207,\n",
       "  0.5459914551394823,\n",
       "  0.5491329479768786,\n",
       "  0.5520231213872833,\n",
       "  0.5555834799363324,\n",
       "  0.5590181787718858,\n",
       "  0.5626623104632654,\n",
       "  0.5645472061657033,\n",
       "  0.567604925860769,\n",
       "  0.5713747172656446,\n",
       "  0.5746837563877021,\n",
       "  0.577322610371115,\n",
       "  0.5805478763508419,\n",
       "  0.5828516377649325,\n",
       "  0.5866633157409734,\n",
       "  0.5903912205746837,\n",
       "  0.5935745999832454,\n",
       "  0.5970511853899639,\n",
       "  0.6006115439390132,\n",
       "  0.6037949233475748,\n",
       "  0.6072715087542934,\n",
       "  0.6114182792996565,\n",
       "  0.6151461841333669,\n",
       "  0.6184971098265896,\n",
       "  0.6222250146602999,\n",
       "  0.6263298986344977,\n",
       "  0.6299740303258775,\n",
       "  0.6334925023037614,\n",
       "  0.6377649325626205,\n",
       "  0.6393985088380665,\n",
       "  0.6417860433944877,\n",
       "  0.6446343302337271,\n",
       "  0.6486554410655944,\n",
       "  0.6520482533299824,\n",
       "  0.6548127670268912,\n",
       "  0.6582474658624445,\n",
       "  0.6618078244114937,\n",
       "  0.6645723381084024,\n",
       "  0.667588171232303,\n",
       "  0.6707296640696992,\n",
       "  0.674373795761079,\n",
       "  0.6785205663064422,\n",
       "  0.6817877188573344,\n",
       "  0.6856412834045406,\n",
       "  0.6889922090977633,\n",
       "  0.6928038870738041,\n",
       "  0.696615565049845,\n",
       "  0.6997989444584066,\n",
       "  0.7028147775823071,\n",
       "  0.7062075898466952,\n",
       "  0.7108569992460417,\n",
       "  0.7145011309374215,\n",
       "  0.7170980983496691,\n",
       "  0.7205327971852225,\n",
       "  0.7239256094496105,\n",
       "  0.7282399262796347,\n",
       "  0.7305436876937254,\n",
       "  0.7336432939599564,\n",
       "  0.7363659210856999,\n",
       "  0.7409734439138812,\n",
       "  0.7440730501801123,\n",
       "  0.7481779341543101,\n",
       "  0.7512356538493759,\n",
       "  0.7556337438217308,\n",
       "  0.759654854653598,\n",
       "  0.762880120633325,\n",
       "  0.7656027477590684,\n",
       "  0.7690374465946218,\n",
       "  0.7733936499958114,\n",
       "  0.777330987685348,\n",
       "  0.7801373879534221,\n",
       "  0.7847449107816035,\n",
       "  0.7881377230459915,\n",
       "  0.7906928038870739,\n",
       "  0.794253162436123,\n",
       "  0.7990701181201307,\n",
       "  0.8015833123900478,\n",
       "  0.8044315992292871,\n",
       "  0.8074474323531876,\n",
       "  0.8113847700427244,\n",
       "  0.8147775823071124,\n",
       "  0.8177934154310128,\n",
       "  0.8205998156990869,\n",
       "  0.8244114936751278,\n",
       "  0.8280556253665075,\n",
       "  0.8309039122057469,\n",
       "  0.8336265393314903,\n",
       "  0.8360559604590768,\n",
       "  0.8392812264388038,\n",
       "  0.8417525341375555,\n",
       "  0.8448940269749519,\n",
       "  0.8482868392393399,\n",
       "  0.8508419200804223,\n",
       "  0.8546535980564631,\n",
       "  0.8582139566055123,\n",
       "  0.8609365837312558,\n",
       "  0.8644131691379744,\n",
       "  0.8674708888330401,\n",
       "  0.8704867219569407,\n",
       "  0.8732512356538493,\n",
       "  0.87555499706794,\n",
       "  0.8784451704783447,\n",
       "  0.88070704532127,\n",
       "  0.8846024964396415,\n",
       "  0.887031917567228,\n",
       "  0.8895869984083103,\n",
       "  0.8918069866800704,\n",
       "  0.8940269749518305,\n",
       "  0.8978805394990366,\n",
       "  0.8999329814861355,\n",
       "  0.9026137220407138,\n",
       "  0.905587668593449,\n",
       "  0.9085197285750188,\n",
       "  0.9111166959872665,\n",
       "  0.9133785708301918,\n",
       "  0.9155147859596213,\n",
       "  0.9182374130853649,\n",
       "  0.9203736282147943,\n",
       "  0.9225936164865544,\n",
       "  0.9243947390466617,\n",
       "  0.9264052944625953,\n",
       "  0.9289184887325124,\n",
       "  0.9317248890005864,\n",
       "  0.9355365669766273,\n",
       "  0.9374214626790651,\n",
       "  0.9396414509508252,\n",
       "  0.9419033257937506,\n",
       "  0.9445421797771635,\n",
       "  0.9474742397587333,\n",
       "  0.9486470637513613,\n",
       "  0.9507413923096255,\n",
       "  0.9526262880120633,\n",
       "  0.9556421211359638,\n",
       "  0.9577783362653933,\n",
       "  0.9601239842506493,\n",
       "  0.9626371785205663,\n",
       "  0.9648990533634917,\n",
       "  0.9664488564966072,\n",
       "  0.9683337521990449,\n",
       "  0.970092988187987,\n",
       "  0.9719778838904247,\n",
       "  0.9729412750272263,\n",
       "  0.9745329647315071,\n",
       "  0.9757476752953004,\n",
       "  0.9769204992879283,\n",
       "  0.978051436709391,\n",
       "  0.97939180698668,\n",
       "  0.9804389712658121,\n",
       "  0.9819887743989277,\n",
       "  0.9837480103878696,\n",
       "  0.9854653598056463,\n",
       "  0.986596297227109,\n",
       "  0.987643461506241,\n",
       "  0.988523079500712,\n",
       "  0.9895702437798441,\n",
       "  0.9903660886319846,\n",
       "  0.9916226857669431,\n",
       "  0.992041551478596,\n",
       "  0.9931724889000586,\n",
       "  0.9937589008963726,\n",
       "  0.9946385188908436,\n",
       "  0.9953505906006535,\n",
       "  0.9959788891681327,\n",
       "  0.9964396414509509,\n",
       "  0.9970260534472648,\n",
       "  0.9976962385859094,\n",
       "  0.9980732177263969,\n",
       "  0.9985339700092151,\n",
       "  0.998827176007372,\n",
       "  0.9991622685766943,\n",
       "  0.9992041551478597,\n",
       "  0.9994135880036861,\n",
       "  0.9996649074306777,\n",
       "  0.9997905671441736,\n",
       "  0.9998743402865041,\n",
       "  0.9999581134288347,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc=aml.leader.roc()\n",
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20181120_154205\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12685990330985406\n",
      "RMSE: 0.3561739789904002\n",
      "LogLoss: 0.40703958453227496\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 106607.73758484815\n",
      "AIC: 106631.73758484815\n",
      "AUC: 0.7734040182726795\n",
      "Gini: 0.5468080365453589\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21708051286464972: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>83212.0</td>\n",
       "<td>23869.0</td>\n",
       "<td>0.2229</td>\n",
       "<td> (23869.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9376.0</td>\n",
       "<td>14498.0</td>\n",
       "<td>0.3927</td>\n",
       "<td> (9376.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>92588.0</td>\n",
       "<td>38367.0</td>\n",
       "<td>0.2539</td>\n",
       "<td> (33245.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      83212  23869  0.2229   (23869.0/107081.0)\n",
       "1      9376   14498  0.3927   (9376.0/23874.0)\n",
       "Total  92588  38367  0.2539   (33245.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2170805</td>\n",
       "<td>0.4658666</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1410190</td>\n",
       "<td>0.6067433</td>\n",
       "<td>309.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3338429</td>\n",
       "<td>0.4568693</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4390853</td>\n",
       "<td>0.8290100</td>\n",
       "<td>98.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0676438</td>\n",
       "<td>1.0</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2335835</td>\n",
       "<td>0.3269004</td>\n",
       "<td>224.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1855661</td>\n",
       "<td>0.6993958</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1761337</td>\n",
       "<td>0.7006208</td>\n",
       "<td>274.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.217081     0.465867  238\n",
       "max f2                       0.141019     0.606743  309\n",
       "max f0point5                 0.333843     0.456869  155\n",
       "max accuracy                 0.439085     0.82901   98\n",
       "max precision                0.800677     1         0\n",
       "max recall                   0.0676438    1         394\n",
       "max specificity              0.800677     1         0\n",
       "max absolute_mcc             0.233584     0.3269    224\n",
       "max min_per_class_accuracy   0.185566     0.699396  265\n",
       "max mean_per_class_accuracy  0.176134     0.700621  274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5809896</td>\n",
       "<td>4.2290904</td>\n",
       "<td>4.2290904</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.0423054</td>\n",
       "<td>0.0423054</td>\n",
       "<td>322.9090447</td>\n",
       "<td>322.9090447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5350310</td>\n",
       "<td>3.4879528</td>\n",
       "<td>3.8585216</td>\n",
       "<td>0.6358779</td>\n",
       "<td>0.7034351</td>\n",
       "<td>0.0348915</td>\n",
       "<td>0.0771970</td>\n",
       "<td>248.7952815</td>\n",
       "<td>285.8521631</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5007744</td>\n",
       "<td>3.1972882</td>\n",
       "<td>3.6382227</td>\n",
       "<td>0.5828877</td>\n",
       "<td>0.6632731</td>\n",
       "<td>0.0319595</td>\n",
       "<td>0.1091564</td>\n",
       "<td>219.7288214</td>\n",
       "<td>263.8222689</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4718430</td>\n",
       "<td>2.9436144</td>\n",
       "<td>3.4645375</td>\n",
       "<td>0.5366412</td>\n",
       "<td>0.6316091</td>\n",
       "<td>0.0294463</td>\n",
       "<td>0.1386027</td>\n",
       "<td>194.3614440</td>\n",
       "<td>246.4537481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4483865</td>\n",
       "<td>2.8704357</td>\n",
       "<td>3.3457716</td>\n",
       "<td>0.5233002</td>\n",
       "<td>0.6099572</td>\n",
       "<td>0.0286923</td>\n",
       "<td>0.1672950</td>\n",
       "<td>187.0435684</td>\n",
       "<td>234.5771560</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3623038</td>\n",
       "<td>2.4444070</td>\n",
       "<td>2.8950893</td>\n",
       "<td>0.4456323</td>\n",
       "<td>0.5277947</td>\n",
       "<td>0.1222250</td>\n",
       "<td>0.2895200</td>\n",
       "<td>144.4406963</td>\n",
       "<td>189.5089261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3079302</td>\n",
       "<td>1.9878608</td>\n",
       "<td>2.5926798</td>\n",
       "<td>0.3624007</td>\n",
       "<td>0.4726634</td>\n",
       "<td>0.0993968</td>\n",
       "<td>0.3889168</td>\n",
       "<td>98.7860769</td>\n",
       "<td>159.2679764</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2684456</td>\n",
       "<td>1.7116814</td>\n",
       "<td>2.3724554</td>\n",
       "<td>0.3120513</td>\n",
       "<td>0.4325150</td>\n",
       "<td>0.0855743</td>\n",
       "<td>0.4744911</td>\n",
       "<td>71.1681359</td>\n",
       "<td>137.2455391</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2132009</td>\n",
       "<td>1.4102670</td>\n",
       "<td>2.0517178</td>\n",
       "<td>0.2571014</td>\n",
       "<td>0.3740423</td>\n",
       "<td>0.1410321</td>\n",
       "<td>0.6155232</td>\n",
       "<td>41.0267006</td>\n",
       "<td>105.1717765</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1765625</td>\n",
       "<td>1.1217652</td>\n",
       "<td>1.8192385</td>\n",
       "<td>0.2045055</td>\n",
       "<td>0.3316597</td>\n",
       "<td>0.1121722</td>\n",
       "<td>0.7276954</td>\n",
       "<td>12.1765206</td>\n",
       "<td>81.9238502</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1503145</td>\n",
       "<td>0.8745582</td>\n",
       "<td>1.6302967</td>\n",
       "<td>0.1594380</td>\n",
       "<td>0.2972143</td>\n",
       "<td>0.0874592</td>\n",
       "<td>0.8151546</td>\n",
       "<td>-12.5441786</td>\n",
       "<td>63.0296674</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1300345</td>\n",
       "<td>0.6769128</td>\n",
       "<td>1.4714054</td>\n",
       "<td>0.1234059</td>\n",
       "<td>0.2682474</td>\n",
       "<td>0.0676887</td>\n",
       "<td>0.8828433</td>\n",
       "<td>-32.3087165</td>\n",
       "<td>47.1405434</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1129563</td>\n",
       "<td>0.4959559</td>\n",
       "<td>1.3320601</td>\n",
       "<td>0.0904162</td>\n",
       "<td>0.2428437</td>\n",
       "<td>0.0495937</td>\n",
       "<td>0.9324370</td>\n",
       "<td>-50.4044061</td>\n",
       "<td>33.2060067</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0978849</td>\n",
       "<td>0.3920433</td>\n",
       "<td>1.2145535</td>\n",
       "<td>0.0714722</td>\n",
       "<td>0.2214215</td>\n",
       "<td>0.0392058</td>\n",
       "<td>0.9716428</td>\n",
       "<td>-60.7956663</td>\n",
       "<td>21.4553489</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0824531</td>\n",
       "<td>0.2194940</td>\n",
       "<td>1.1039951</td>\n",
       "<td>0.0400153</td>\n",
       "<td>0.2012659</td>\n",
       "<td>0.0219486</td>\n",
       "<td>0.9935914</td>\n",
       "<td>-78.0505987</td>\n",
       "<td>10.3995078</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597755</td>\n",
       "<td>0.0640840</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0116830</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0064086</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.5915993</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.58099            4.22909   4.22909            0.770992         0.770992                    0.0423054       0.0423054                  322.909   322.909\n",
       "    2        0.0200069                   0.535031           3.48795   3.85852            0.635878         0.703435                    0.0348915       0.077197                   248.795   285.852\n",
       "    3        0.0300027                   0.500774           3.19729   3.63822            0.582888         0.663273                    0.0319595       0.109156                   219.729   263.822\n",
       "    4        0.0400061                   0.471843           2.94361   3.46454            0.536641         0.631609                    0.0294463       0.138603                   194.361   246.454\n",
       "    5        0.0500019                   0.448387           2.87044   3.34577            0.5233           0.609957                    0.0286923       0.167295                   187.044   234.577\n",
       "    6        0.100004                    0.362304           2.44441   2.89509            0.445632         0.527795                    0.122225        0.28952                    144.441   189.509\n",
       "    7        0.150006                    0.30793            1.98786   2.59268            0.362401         0.472663                    0.0993968       0.388917                   98.7861   159.268\n",
       "    8        0.2                         0.268446           1.71168   2.37246            0.312051         0.432515                    0.0855743       0.474491                   71.1681   137.246\n",
       "    9        0.300004                    0.213201           1.41027   2.05172            0.257101         0.374042                    0.141032        0.615523                   41.0267   105.172\n",
       "    10       0.4                         0.176563           1.12177   1.81924            0.204506         0.33166                     0.112172        0.727695                   12.1765   81.9239\n",
       "    11       0.500004                    0.150314           0.874558  1.6303             0.159438         0.297214                    0.0874592       0.815155                   -12.5442  63.0297\n",
       "    12       0.6                         0.130034           0.676913  1.47141            0.123406         0.268247                    0.0676887       0.882843                   -32.3087  47.1405\n",
       "    13       0.699996                    0.112956           0.495956  1.33206            0.0904162        0.242844                    0.0495937       0.932437                   -50.4044  33.206\n",
       "    14       0.8                         0.0978849          0.392043  1.21455            0.0714722        0.221421                    0.0392058       0.971643                   -60.7957  21.4553\n",
       "    15       0.899996                    0.0824531          0.219494  1.104              0.0400153        0.201266                    0.0219486       0.993591                   -78.0506  10.3995\n",
       "    16       1                           0.0597755          0.064084  1                  0.011683         0.182307                    0.00640865      1                          -93.5916  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.13813682407268132\n",
      "RMSE: 0.37166762580655494\n",
      "LogLoss: 0.4379927029517987\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33020\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 28935.54992780763\n",
      "AIC: 28959.54992780763\n",
      "AUC: 0.7122628231156057\n",
      "Gini: 0.4245256462312115\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18933454821029921: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>18652.0</td>\n",
       "<td>8238.0</td>\n",
       "<td>0.3064</td>\n",
       "<td> (8238.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2386.0</td>\n",
       "<td>3756.0</td>\n",
       "<td>0.3885</td>\n",
       "<td> (2386.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21038.0</td>\n",
       "<td>11994.0</td>\n",
       "<td>0.3216</td>\n",
       "<td> (10624.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      18652  8238   0.3064   (8238.0/26890.0)\n",
       "1      2386   3756   0.3885   (2386.0/6142.0)\n",
       "Total  21038  11994  0.3216   (10624.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1893345</td>\n",
       "<td>0.4142038</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1197407</td>\n",
       "<td>0.5738535</td>\n",
       "<td>330.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3129965</td>\n",
       "<td>0.3817812</td>\n",
       "<td>160.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6010242</td>\n",
       "<td>0.8160572</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0640080</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2092723</td>\n",
       "<td>0.2475688</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1762849</td>\n",
       "<td>0.6525562</td>\n",
       "<td>268.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577331</td>\n",
       "<td>0.6547886</td>\n",
       "<td>286.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.189335     0.414204  256\n",
       "max f2                       0.119741     0.573853  330\n",
       "max f0point5                 0.312996     0.381781  160\n",
       "max accuracy                 0.601024     0.816057  29\n",
       "max precision                0.766658     1         0\n",
       "max recall                   0.064008     1         398\n",
       "max specificity              0.766658     1         0\n",
       "max absolute_mcc             0.209272     0.247569  238\n",
       "max min_per_class_accuracy   0.176285     0.652556  268\n",
       "max mean_per_class_accuracy  0.157733     0.654789  286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5783690</td>\n",
       "<td>3.1358434</td>\n",
       "<td>3.1358434</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.0314230</td>\n",
       "<td>0.0314230</td>\n",
       "<td>213.5843447</td>\n",
       "<td>213.5843447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5293169</td>\n",
       "<td>2.5749465</td>\n",
       "<td>2.8558192</td>\n",
       "<td>0.4787879</td>\n",
       "<td>0.5310136</td>\n",
       "<td>0.0257245</td>\n",
       "<td>0.0571475</td>\n",
       "<td>157.4946469</td>\n",
       "<td>185.5819237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4955180</td>\n",
       "<td>2.4119752</td>\n",
       "<td>2.7080205</td>\n",
       "<td>0.4484848</td>\n",
       "<td>0.5035318</td>\n",
       "<td>0.0240964</td>\n",
       "<td>0.0812439</td>\n",
       "<td>141.1975173</td>\n",
       "<td>170.8020508</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4688250</td>\n",
       "<td>2.4859277</td>\n",
       "<td>2.6524133</td>\n",
       "<td>0.4622356</td>\n",
       "<td>0.4931921</td>\n",
       "<td>0.0249105</td>\n",
       "<td>0.1061543</td>\n",
       "<td>148.5927707</td>\n",
       "<td>165.2413309</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4455599</td>\n",
       "<td>2.2164096</td>\n",
       "<td>2.5653181</td>\n",
       "<td>0.4121212</td>\n",
       "<td>0.4769976</td>\n",
       "<td>0.0221426</td>\n",
       "<td>0.1282970</td>\n",
       "<td>121.6409619</td>\n",
       "<td>156.5318141</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3612868</td>\n",
       "<td>2.0607188</td>\n",
       "<td>2.3130185</td>\n",
       "<td>0.3831719</td>\n",
       "<td>0.4300847</td>\n",
       "<td>0.1030609</td>\n",
       "<td>0.2313579</td>\n",
       "<td>106.0718760</td>\n",
       "<td>131.3018450</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.3068785</td>\n",
       "<td>1.7850835</td>\n",
       "<td>2.1371112</td>\n",
       "<td>0.3319200</td>\n",
       "<td>0.3973764</td>\n",
       "<td>0.0892218</td>\n",
       "<td>0.3205796</td>\n",
       "<td>78.5083530</td>\n",
       "<td>113.7111174</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2675737</td>\n",
       "<td>1.5756523</td>\n",
       "<td>1.9967252</td>\n",
       "<td>0.2929782</td>\n",
       "<td>0.3712729</td>\n",
       "<td>0.0788017</td>\n",
       "<td>0.3993813</td>\n",
       "<td>57.5652259</td>\n",
       "<td>99.6725200</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2132448</td>\n",
       "<td>1.3758567</td>\n",
       "<td>1.7897899</td>\n",
       "<td>0.2558280</td>\n",
       "<td>0.3327952</td>\n",
       "<td>0.1375773</td>\n",
       "<td>0.5369586</td>\n",
       "<td>37.5856668</td>\n",
       "<td>78.9789907</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1764937</td>\n",
       "<td>1.1299935</td>\n",
       "<td>1.6248533</td>\n",
       "<td>0.2101120</td>\n",
       "<td>0.3021267</td>\n",
       "<td>0.1129925</td>\n",
       "<td>0.6499512</td>\n",
       "<td>12.9993524</td>\n",
       "<td>62.4853295</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1497263</td>\n",
       "<td>0.9964784</td>\n",
       "<td>1.4991859</td>\n",
       "<td>0.1852861</td>\n",
       "<td>0.2787600</td>\n",
       "<td>0.0996418</td>\n",
       "<td>0.7495930</td>\n",
       "<td>-0.3521561</td>\n",
       "<td>49.9185933</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1296393</td>\n",
       "<td>0.7587565</td>\n",
       "<td>1.3757872</td>\n",
       "<td>0.1410839</td>\n",
       "<td>0.2558151</td>\n",
       "<td>0.0758711</td>\n",
       "<td>0.8254640</td>\n",
       "<td>-24.1243542</td>\n",
       "<td>37.5787247</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1130667</td>\n",
       "<td>0.6284978</td>\n",
       "<td>1.2690362</td>\n",
       "<td>0.1168635</td>\n",
       "<td>0.2359657</td>\n",
       "<td>0.0628460</td>\n",
       "<td>0.8883100</td>\n",
       "<td>-37.1502161</td>\n",
       "<td>26.9036234</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0982229</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1761501</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2186944</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9408987</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6150117</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0826149</td>\n",
       "<td>0.3989170</td>\n",
       "<td>1.0897938</td>\n",
       "<td>0.0741750</td>\n",
       "<td>0.2026372</td>\n",
       "<td>0.0398893</td>\n",
       "<td>0.9807880</td>\n",
       "<td>-60.1082978</td>\n",
       "<td>8.9793790</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0605773</td>\n",
       "<td>0.1920733</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0357143</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0192120</td>\n",
       "<td>1.0</td>\n",
       "<td>-80.7926687</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain       cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  ---------  -----------------\n",
       "    1        0.0100206                   0.578369           3.13584   3.13584            0.583082         0.583082                    0.031423        0.031423                   213.584    213.584\n",
       "    2        0.0200109                   0.529317           2.57495   2.85582            0.478788         0.531014                    0.0257245       0.0571475                  157.495    185.582\n",
       "    3        0.0300012                   0.495518           2.41198   2.70802            0.448485         0.503532                    0.0240964       0.0812439                  141.198    170.802\n",
       "    4        0.0400218                   0.468825           2.48593   2.65241            0.462236         0.493192                    0.0249105       0.106154                   148.593    165.241\n",
       "    5        0.0500121                   0.44556            2.21641   2.56532            0.412121         0.476998                    0.0221426       0.128297                   121.641    156.532\n",
       "    6        0.100024                    0.361287           2.06072   2.31302            0.383172         0.430085                    0.103061        0.231358                   106.072    131.302\n",
       "    7        0.150006                    0.306878           1.78508   2.13711            0.33192          0.397376                    0.0892218       0.32058                    78.5084    113.711\n",
       "    8        0.200018                    0.267574           1.57565   1.99673            0.292978         0.371273                    0.0788017       0.399381                   57.5652    99.6725\n",
       "    9        0.300012                    0.213245           1.37586   1.78979            0.255828         0.332795                    0.137577        0.536959                   37.5857    78.979\n",
       "    10       0.400006                    0.176494           1.12999   1.62485            0.210112         0.302127                    0.112993        0.649951                   12.9994    62.4853\n",
       "    11       0.5                         0.149726           0.996478  1.49919            0.185286         0.27876                     0.0996418       0.749593                   -0.352156  49.9186\n",
       "    12       0.599994                    0.129639           0.758756  1.37579            0.141084         0.255815                    0.0758711       0.825464                   -24.1244   37.5787\n",
       "    13       0.699988                    0.113067           0.628498  1.26904            0.116863         0.235966                    0.062846        0.88831                    -37.1502   26.9036\n",
       "    14       0.799982                    0.0982229          0.525919  1.17615            0.0977899        0.218694                    0.0525887       0.940899                   -47.4081   17.615\n",
       "    15       0.899976                    0.0826149          0.398917  1.08979            0.074175         0.202637                    0.0398893       0.980788                   -60.1083   8.97938\n",
       "    16       1                           0.0605773          0.192073  1                  0.0357143        0.185941                    0.019212        1                          -80.7927   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.1369765435568584\n",
      "RMSE: 0.3701034227845757\n",
      "LogLoss: 0.4355309009590542\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114069.89827018589\n",
      "AIC: 114093.89827018589\n",
      "AUC: 0.7071203432205223\n",
      "Gini: 0.4142406864410446\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17843972441369682: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>73561.0</td>\n",
       "<td>33520.0</td>\n",
       "<td>0.313</td>\n",
       "<td> (33520.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9215.0</td>\n",
       "<td>14659.0</td>\n",
       "<td>0.386</td>\n",
       "<td> (9215.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>82776.0</td>\n",
       "<td>48179.0</td>\n",
       "<td>0.3263</td>\n",
       "<td> (42735.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      73561  33520  0.313    (33520.0/107081.0)\n",
       "1      9215   14659  0.386    (9215.0/23874.0)\n",
       "Total  82776  48179  0.3263   (42735.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1784397</td>\n",
       "<td>0.4068949</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1136746</td>\n",
       "<td>0.5665672</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2879939</td>\n",
       "<td>0.3708050</td>\n",
       "<td>176.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5663591</td>\n",
       "<td>0.8184567</td>\n",
       "<td>36.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0620870</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1815977</td>\n",
       "<td>0.2412252</td>\n",
       "<td>258.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1671726</td>\n",
       "<td>0.6498819</td>\n",
       "<td>272.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1616542</td>\n",
       "<td>0.6512829</td>\n",
       "<td>278.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.17844      0.406895  261\n",
       "max f2                       0.113675     0.566567  334\n",
       "max f0point5                 0.287994     0.370805  176\n",
       "max accuracy                 0.566359     0.818457  36\n",
       "max precision                0.785924     1         0\n",
       "max recall                   0.062087     1         399\n",
       "max specificity              0.785924     1         0\n",
       "max absolute_mcc             0.181598     0.241225  258\n",
       "max min_per_class_accuracy   0.167173     0.649882  272\n",
       "max mean_per_class_accuracy  0.161654     0.651283  278"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5609815</td>\n",
       "<td>2.9394272</td>\n",
       "<td>2.9394272</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.0294044</td>\n",
       "<td>0.0294044</td>\n",
       "<td>193.9427222</td>\n",
       "<td>193.9427222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5136764</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7719385</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5053435</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0554578</td>\n",
       "<td>160.4449761</td>\n",
       "<td>177.1938491</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4802174</td>\n",
       "<td>2.4346323</td>\n",
       "<td>2.6595603</td>\n",
       "<td>0.4438503</td>\n",
       "<td>0.4848562</td>\n",
       "<td>0.0243361</td>\n",
       "<td>0.0797939</td>\n",
       "<td>143.4632310</td>\n",
       "<td>165.9560331</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4541967</td>\n",
       "<td>2.2987828</td>\n",
       "<td>2.5693487</td>\n",
       "<td>0.4190840</td>\n",
       "<td>0.4684100</td>\n",
       "<td>0.0229957</td>\n",
       "<td>0.1027896</td>\n",
       "<td>129.8782827</td>\n",
       "<td>156.9348739</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4304407</td>\n",
       "<td>2.3592048</td>\n",
       "<td>2.5273392</td>\n",
       "<td>0.4300993</td>\n",
       "<td>0.4607514</td>\n",
       "<td>0.0235821</td>\n",
       "<td>0.1263718</td>\n",
       "<td>135.9204803</td>\n",
       "<td>152.7339208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3489922</td>\n",
       "<td>2.0674422</td>\n",
       "<td>2.2973907</td>\n",
       "<td>0.3769090</td>\n",
       "<td>0.4188302</td>\n",
       "<td>0.1033761</td>\n",
       "<td>0.2297478</td>\n",
       "<td>106.7442216</td>\n",
       "<td>129.7390712</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2957391</td>\n",
       "<td>1.7583311</td>\n",
       "<td>2.1177042</td>\n",
       "<td>0.3205559</td>\n",
       "<td>0.3860721</td>\n",
       "<td>0.0879199</td>\n",
       "<td>0.3176678</td>\n",
       "<td>75.8331123</td>\n",
       "<td>111.7704182</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2569097</td>\n",
       "<td>1.5826559</td>\n",
       "<td>1.9839574</td>\n",
       "<td>0.2885291</td>\n",
       "<td>0.3616891</td>\n",
       "<td>0.0791237</td>\n",
       "<td>0.3967915</td>\n",
       "<td>58.2655941</td>\n",
       "<td>98.3957443</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2032167</td>\n",
       "<td>1.3679632</td>\n",
       "<td>1.7786208</td>\n",
       "<td>0.2493891</td>\n",
       "<td>0.3242548</td>\n",
       "<td>0.1368015</td>\n",
       "<td>0.5335930</td>\n",
       "<td>36.7963184</td>\n",
       "<td>77.8620797</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1679941</td>\n",
       "<td>1.1284673</td>\n",
       "<td>1.6160886</td>\n",
       "<td>0.2057274</td>\n",
       "<td>0.2946241</td>\n",
       "<td>0.1128424</td>\n",
       "<td>0.6464355</td>\n",
       "<td>12.8467313</td>\n",
       "<td>61.6088632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1428590</td>\n",
       "<td>0.9302652</td>\n",
       "<td>1.4789198</td>\n",
       "<td>0.1695938</td>\n",
       "<td>0.2696173</td>\n",
       "<td>0.0930301</td>\n",
       "<td>0.7394655</td>\n",
       "<td>-6.9734773</td>\n",
       "<td>47.8919761</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1238863</td>\n",
       "<td>0.8143060</td>\n",
       "<td>1.3681550</td>\n",
       "<td>0.1484536</td>\n",
       "<td>0.2494241</td>\n",
       "<td>0.0814275</td>\n",
       "<td>0.8208930</td>\n",
       "<td>-18.5693965</td>\n",
       "<td>36.8155036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1081113</td>\n",
       "<td>0.6589009</td>\n",
       "<td>1.2668363</td>\n",
       "<td>0.1201222</td>\n",
       "<td>0.2309530</td>\n",
       "<td>0.0658876</td>\n",
       "<td>0.8867806</td>\n",
       "<td>-34.1099078</td>\n",
       "<td>26.6836336</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0941428</td>\n",
       "<td>0.5185360</td>\n",
       "<td>1.1732952</td>\n",
       "<td>0.0945327</td>\n",
       "<td>0.2138998</td>\n",
       "<td>0.0518556</td>\n",
       "<td>0.9386362</td>\n",
       "<td>-48.1464047</td>\n",
       "<td>17.3295217</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0801775</td>\n",
       "<td>0.3945866</td>\n",
       "<td>1.0867750</td>\n",
       "<td>0.0719359</td>\n",
       "<td>0.1981266</td>\n",
       "<td>0.0394572</td>\n",
       "<td>0.9780933</td>\n",
       "<td>-60.5413434</td>\n",
       "<td>8.6774970</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597666</td>\n",
       "<td>0.2190584</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0399359</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0219067</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.0941597</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.560981           2.93943   2.93943            0.535878         0.535878                    0.0294044       0.0294044                  193.943   193.943\n",
       "    2        0.0200069                   0.513676           2.60445   2.77194            0.474809         0.505344                    0.0260534       0.0554578                  160.445   177.194\n",
       "    3        0.0300027                   0.480217           2.43463   2.65956            0.44385          0.484856                    0.0243361       0.0797939                  143.463   165.956\n",
       "    4        0.0400061                   0.454197           2.29878   2.56935            0.419084         0.46841                     0.0229957       0.10279                    129.878   156.935\n",
       "    5        0.0500019                   0.430441           2.3592    2.52734            0.430099         0.460751                    0.0235821       0.126372                   135.92    152.734\n",
       "    6        0.100004                    0.348992           2.06744   2.29739            0.376909         0.41883                     0.103376        0.229748                   106.744   129.739\n",
       "    7        0.150006                    0.295739           1.75833   2.1177             0.320556         0.386072                    0.0879199       0.317668                   75.8331   111.77\n",
       "    8        0.2                         0.25691            1.58266   1.98396            0.288529         0.361689                    0.0791237       0.396791                   58.2656   98.3957\n",
       "    9        0.300004                    0.203217           1.36796   1.77862            0.249389         0.324255                    0.136802        0.533593                   36.7963   77.8621\n",
       "    10       0.4                         0.167994           1.12847   1.61609            0.205727         0.294624                    0.112842        0.646435                   12.8467   61.6089\n",
       "    11       0.500004                    0.142859           0.930265  1.47892            0.169594         0.269617                    0.0930301       0.739466                   -6.97348  47.892\n",
       "    12       0.6                         0.123886           0.814306  1.36816            0.148454         0.249424                    0.0814275       0.820893                   -18.5694  36.8155\n",
       "    13       0.699996                    0.108111           0.658901  1.26684            0.120122         0.230953                    0.0658876       0.886781                   -34.1099  26.6836\n",
       "    14       0.8                         0.0941428          0.518536  1.1733             0.0945327        0.2139                      0.0518556       0.938636                   -48.1464  17.3295\n",
       "    15       0.899996                    0.0801775          0.394587  1.08677            0.0719359        0.198127                    0.0394572       0.978093                   -60.5413  8.6775\n",
       "    16       1                           0.0597666          0.219058  1                  0.0399359        0.182307                    0.0219067       1                          -78.0942  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OBinomialModel.tnr of >"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20181120_154205\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12685990330985406\n",
      "RMSE: 0.3561739789904002\n",
      "LogLoss: 0.40703958453227496\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 106607.73758484815\n",
      "AIC: 106631.73758484815\n",
      "AUC: 0.7734040182726795\n",
      "Gini: 0.5468080365453589\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21708051286464972: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>83212.0</td>\n",
       "<td>23869.0</td>\n",
       "<td>0.2229</td>\n",
       "<td> (23869.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9376.0</td>\n",
       "<td>14498.0</td>\n",
       "<td>0.3927</td>\n",
       "<td> (9376.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>92588.0</td>\n",
       "<td>38367.0</td>\n",
       "<td>0.2539</td>\n",
       "<td> (33245.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      83212  23869  0.2229   (23869.0/107081.0)\n",
       "1      9376   14498  0.3927   (9376.0/23874.0)\n",
       "Total  92588  38367  0.2539   (33245.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2170805</td>\n",
       "<td>0.4658666</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1410190</td>\n",
       "<td>0.6067433</td>\n",
       "<td>309.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3338429</td>\n",
       "<td>0.4568693</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4390853</td>\n",
       "<td>0.8290100</td>\n",
       "<td>98.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0676438</td>\n",
       "<td>1.0</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2335835</td>\n",
       "<td>0.3269004</td>\n",
       "<td>224.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1855661</td>\n",
       "<td>0.6993958</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1761337</td>\n",
       "<td>0.7006208</td>\n",
       "<td>274.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.217081     0.465867  238\n",
       "max f2                       0.141019     0.606743  309\n",
       "max f0point5                 0.333843     0.456869  155\n",
       "max accuracy                 0.439085     0.82901   98\n",
       "max precision                0.800677     1         0\n",
       "max recall                   0.0676438    1         394\n",
       "max specificity              0.800677     1         0\n",
       "max absolute_mcc             0.233584     0.3269    224\n",
       "max min_per_class_accuracy   0.185566     0.699396  265\n",
       "max mean_per_class_accuracy  0.176134     0.700621  274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5809896</td>\n",
       "<td>4.2290904</td>\n",
       "<td>4.2290904</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.0423054</td>\n",
       "<td>0.0423054</td>\n",
       "<td>322.9090447</td>\n",
       "<td>322.9090447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5350310</td>\n",
       "<td>3.4879528</td>\n",
       "<td>3.8585216</td>\n",
       "<td>0.6358779</td>\n",
       "<td>0.7034351</td>\n",
       "<td>0.0348915</td>\n",
       "<td>0.0771970</td>\n",
       "<td>248.7952815</td>\n",
       "<td>285.8521631</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5007744</td>\n",
       "<td>3.1972882</td>\n",
       "<td>3.6382227</td>\n",
       "<td>0.5828877</td>\n",
       "<td>0.6632731</td>\n",
       "<td>0.0319595</td>\n",
       "<td>0.1091564</td>\n",
       "<td>219.7288214</td>\n",
       "<td>263.8222689</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4718430</td>\n",
       "<td>2.9436144</td>\n",
       "<td>3.4645375</td>\n",
       "<td>0.5366412</td>\n",
       "<td>0.6316091</td>\n",
       "<td>0.0294463</td>\n",
       "<td>0.1386027</td>\n",
       "<td>194.3614440</td>\n",
       "<td>246.4537481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4483865</td>\n",
       "<td>2.8704357</td>\n",
       "<td>3.3457716</td>\n",
       "<td>0.5233002</td>\n",
       "<td>0.6099572</td>\n",
       "<td>0.0286923</td>\n",
       "<td>0.1672950</td>\n",
       "<td>187.0435684</td>\n",
       "<td>234.5771560</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3623038</td>\n",
       "<td>2.4444070</td>\n",
       "<td>2.8950893</td>\n",
       "<td>0.4456323</td>\n",
       "<td>0.5277947</td>\n",
       "<td>0.1222250</td>\n",
       "<td>0.2895200</td>\n",
       "<td>144.4406963</td>\n",
       "<td>189.5089261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3079302</td>\n",
       "<td>1.9878608</td>\n",
       "<td>2.5926798</td>\n",
       "<td>0.3624007</td>\n",
       "<td>0.4726634</td>\n",
       "<td>0.0993968</td>\n",
       "<td>0.3889168</td>\n",
       "<td>98.7860769</td>\n",
       "<td>159.2679764</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2684456</td>\n",
       "<td>1.7116814</td>\n",
       "<td>2.3724554</td>\n",
       "<td>0.3120513</td>\n",
       "<td>0.4325150</td>\n",
       "<td>0.0855743</td>\n",
       "<td>0.4744911</td>\n",
       "<td>71.1681359</td>\n",
       "<td>137.2455391</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2132009</td>\n",
       "<td>1.4102670</td>\n",
       "<td>2.0517178</td>\n",
       "<td>0.2571014</td>\n",
       "<td>0.3740423</td>\n",
       "<td>0.1410321</td>\n",
       "<td>0.6155232</td>\n",
       "<td>41.0267006</td>\n",
       "<td>105.1717765</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1765625</td>\n",
       "<td>1.1217652</td>\n",
       "<td>1.8192385</td>\n",
       "<td>0.2045055</td>\n",
       "<td>0.3316597</td>\n",
       "<td>0.1121722</td>\n",
       "<td>0.7276954</td>\n",
       "<td>12.1765206</td>\n",
       "<td>81.9238502</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1503145</td>\n",
       "<td>0.8745582</td>\n",
       "<td>1.6302967</td>\n",
       "<td>0.1594380</td>\n",
       "<td>0.2972143</td>\n",
       "<td>0.0874592</td>\n",
       "<td>0.8151546</td>\n",
       "<td>-12.5441786</td>\n",
       "<td>63.0296674</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1300345</td>\n",
       "<td>0.6769128</td>\n",
       "<td>1.4714054</td>\n",
       "<td>0.1234059</td>\n",
       "<td>0.2682474</td>\n",
       "<td>0.0676887</td>\n",
       "<td>0.8828433</td>\n",
       "<td>-32.3087165</td>\n",
       "<td>47.1405434</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1129563</td>\n",
       "<td>0.4959559</td>\n",
       "<td>1.3320601</td>\n",
       "<td>0.0904162</td>\n",
       "<td>0.2428437</td>\n",
       "<td>0.0495937</td>\n",
       "<td>0.9324370</td>\n",
       "<td>-50.4044061</td>\n",
       "<td>33.2060067</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0978849</td>\n",
       "<td>0.3920433</td>\n",
       "<td>1.2145535</td>\n",
       "<td>0.0714722</td>\n",
       "<td>0.2214215</td>\n",
       "<td>0.0392058</td>\n",
       "<td>0.9716428</td>\n",
       "<td>-60.7956663</td>\n",
       "<td>21.4553489</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0824531</td>\n",
       "<td>0.2194940</td>\n",
       "<td>1.1039951</td>\n",
       "<td>0.0400153</td>\n",
       "<td>0.2012659</td>\n",
       "<td>0.0219486</td>\n",
       "<td>0.9935914</td>\n",
       "<td>-78.0505987</td>\n",
       "<td>10.3995078</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597755</td>\n",
       "<td>0.0640840</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0116830</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0064086</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.5915993</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.58099            4.22909   4.22909            0.770992         0.770992                    0.0423054       0.0423054                  322.909   322.909\n",
       "    2        0.0200069                   0.535031           3.48795   3.85852            0.635878         0.703435                    0.0348915       0.077197                   248.795   285.852\n",
       "    3        0.0300027                   0.500774           3.19729   3.63822            0.582888         0.663273                    0.0319595       0.109156                   219.729   263.822\n",
       "    4        0.0400061                   0.471843           2.94361   3.46454            0.536641         0.631609                    0.0294463       0.138603                   194.361   246.454\n",
       "    5        0.0500019                   0.448387           2.87044   3.34577            0.5233           0.609957                    0.0286923       0.167295                   187.044   234.577\n",
       "    6        0.100004                    0.362304           2.44441   2.89509            0.445632         0.527795                    0.122225        0.28952                    144.441   189.509\n",
       "    7        0.150006                    0.30793            1.98786   2.59268            0.362401         0.472663                    0.0993968       0.388917                   98.7861   159.268\n",
       "    8        0.2                         0.268446           1.71168   2.37246            0.312051         0.432515                    0.0855743       0.474491                   71.1681   137.246\n",
       "    9        0.300004                    0.213201           1.41027   2.05172            0.257101         0.374042                    0.141032        0.615523                   41.0267   105.172\n",
       "    10       0.4                         0.176563           1.12177   1.81924            0.204506         0.33166                     0.112172        0.727695                   12.1765   81.9239\n",
       "    11       0.500004                    0.150314           0.874558  1.6303             0.159438         0.297214                    0.0874592       0.815155                   -12.5442  63.0297\n",
       "    12       0.6                         0.130034           0.676913  1.47141            0.123406         0.268247                    0.0676887       0.882843                   -32.3087  47.1405\n",
       "    13       0.699996                    0.112956           0.495956  1.33206            0.0904162        0.242844                    0.0495937       0.932437                   -50.4044  33.206\n",
       "    14       0.8                         0.0978849          0.392043  1.21455            0.0714722        0.221421                    0.0392058       0.971643                   -60.7957  21.4553\n",
       "    15       0.899996                    0.0824531          0.219494  1.104              0.0400153        0.201266                    0.0219486       0.993591                   -78.0506  10.3995\n",
       "    16       1                           0.0597755          0.064084  1                  0.011683         0.182307                    0.00640865      1                          -93.5916  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.13813682407268132\n",
      "RMSE: 0.37166762580655494\n",
      "LogLoss: 0.4379927029517987\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33020\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 28935.54992780763\n",
      "AIC: 28959.54992780763\n",
      "AUC: 0.7122628231156057\n",
      "Gini: 0.4245256462312115\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18933454821029921: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>18652.0</td>\n",
       "<td>8238.0</td>\n",
       "<td>0.3064</td>\n",
       "<td> (8238.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2386.0</td>\n",
       "<td>3756.0</td>\n",
       "<td>0.3885</td>\n",
       "<td> (2386.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21038.0</td>\n",
       "<td>11994.0</td>\n",
       "<td>0.3216</td>\n",
       "<td> (10624.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      18652  8238   0.3064   (8238.0/26890.0)\n",
       "1      2386   3756   0.3885   (2386.0/6142.0)\n",
       "Total  21038  11994  0.3216   (10624.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1893345</td>\n",
       "<td>0.4142038</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1197407</td>\n",
       "<td>0.5738535</td>\n",
       "<td>330.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3129965</td>\n",
       "<td>0.3817812</td>\n",
       "<td>160.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6010242</td>\n",
       "<td>0.8160572</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0640080</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2092723</td>\n",
       "<td>0.2475688</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1762849</td>\n",
       "<td>0.6525562</td>\n",
       "<td>268.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577331</td>\n",
       "<td>0.6547886</td>\n",
       "<td>286.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.189335     0.414204  256\n",
       "max f2                       0.119741     0.573853  330\n",
       "max f0point5                 0.312996     0.381781  160\n",
       "max accuracy                 0.601024     0.816057  29\n",
       "max precision                0.766658     1         0\n",
       "max recall                   0.064008     1         398\n",
       "max specificity              0.766658     1         0\n",
       "max absolute_mcc             0.209272     0.247569  238\n",
       "max min_per_class_accuracy   0.176285     0.652556  268\n",
       "max mean_per_class_accuracy  0.157733     0.654789  286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5783690</td>\n",
       "<td>3.1358434</td>\n",
       "<td>3.1358434</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.0314230</td>\n",
       "<td>0.0314230</td>\n",
       "<td>213.5843447</td>\n",
       "<td>213.5843447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5293169</td>\n",
       "<td>2.5749465</td>\n",
       "<td>2.8558192</td>\n",
       "<td>0.4787879</td>\n",
       "<td>0.5310136</td>\n",
       "<td>0.0257245</td>\n",
       "<td>0.0571475</td>\n",
       "<td>157.4946469</td>\n",
       "<td>185.5819237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4955180</td>\n",
       "<td>2.4119752</td>\n",
       "<td>2.7080205</td>\n",
       "<td>0.4484848</td>\n",
       "<td>0.5035318</td>\n",
       "<td>0.0240964</td>\n",
       "<td>0.0812439</td>\n",
       "<td>141.1975173</td>\n",
       "<td>170.8020508</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4688250</td>\n",
       "<td>2.4859277</td>\n",
       "<td>2.6524133</td>\n",
       "<td>0.4622356</td>\n",
       "<td>0.4931921</td>\n",
       "<td>0.0249105</td>\n",
       "<td>0.1061543</td>\n",
       "<td>148.5927707</td>\n",
       "<td>165.2413309</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4455599</td>\n",
       "<td>2.2164096</td>\n",
       "<td>2.5653181</td>\n",
       "<td>0.4121212</td>\n",
       "<td>0.4769976</td>\n",
       "<td>0.0221426</td>\n",
       "<td>0.1282970</td>\n",
       "<td>121.6409619</td>\n",
       "<td>156.5318141</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3612868</td>\n",
       "<td>2.0607188</td>\n",
       "<td>2.3130185</td>\n",
       "<td>0.3831719</td>\n",
       "<td>0.4300847</td>\n",
       "<td>0.1030609</td>\n",
       "<td>0.2313579</td>\n",
       "<td>106.0718760</td>\n",
       "<td>131.3018450</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.3068785</td>\n",
       "<td>1.7850835</td>\n",
       "<td>2.1371112</td>\n",
       "<td>0.3319200</td>\n",
       "<td>0.3973764</td>\n",
       "<td>0.0892218</td>\n",
       "<td>0.3205796</td>\n",
       "<td>78.5083530</td>\n",
       "<td>113.7111174</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2675737</td>\n",
       "<td>1.5756523</td>\n",
       "<td>1.9967252</td>\n",
       "<td>0.2929782</td>\n",
       "<td>0.3712729</td>\n",
       "<td>0.0788017</td>\n",
       "<td>0.3993813</td>\n",
       "<td>57.5652259</td>\n",
       "<td>99.6725200</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2132448</td>\n",
       "<td>1.3758567</td>\n",
       "<td>1.7897899</td>\n",
       "<td>0.2558280</td>\n",
       "<td>0.3327952</td>\n",
       "<td>0.1375773</td>\n",
       "<td>0.5369586</td>\n",
       "<td>37.5856668</td>\n",
       "<td>78.9789907</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1764937</td>\n",
       "<td>1.1299935</td>\n",
       "<td>1.6248533</td>\n",
       "<td>0.2101120</td>\n",
       "<td>0.3021267</td>\n",
       "<td>0.1129925</td>\n",
       "<td>0.6499512</td>\n",
       "<td>12.9993524</td>\n",
       "<td>62.4853295</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1497263</td>\n",
       "<td>0.9964784</td>\n",
       "<td>1.4991859</td>\n",
       "<td>0.1852861</td>\n",
       "<td>0.2787600</td>\n",
       "<td>0.0996418</td>\n",
       "<td>0.7495930</td>\n",
       "<td>-0.3521561</td>\n",
       "<td>49.9185933</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1296393</td>\n",
       "<td>0.7587565</td>\n",
       "<td>1.3757872</td>\n",
       "<td>0.1410839</td>\n",
       "<td>0.2558151</td>\n",
       "<td>0.0758711</td>\n",
       "<td>0.8254640</td>\n",
       "<td>-24.1243542</td>\n",
       "<td>37.5787247</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1130667</td>\n",
       "<td>0.6284978</td>\n",
       "<td>1.2690362</td>\n",
       "<td>0.1168635</td>\n",
       "<td>0.2359657</td>\n",
       "<td>0.0628460</td>\n",
       "<td>0.8883100</td>\n",
       "<td>-37.1502161</td>\n",
       "<td>26.9036234</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0982229</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1761501</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2186944</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9408987</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6150117</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0826149</td>\n",
       "<td>0.3989170</td>\n",
       "<td>1.0897938</td>\n",
       "<td>0.0741750</td>\n",
       "<td>0.2026372</td>\n",
       "<td>0.0398893</td>\n",
       "<td>0.9807880</td>\n",
       "<td>-60.1082978</td>\n",
       "<td>8.9793790</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0605773</td>\n",
       "<td>0.1920733</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0357143</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0192120</td>\n",
       "<td>1.0</td>\n",
       "<td>-80.7926687</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain       cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  ---------  -----------------\n",
       "    1        0.0100206                   0.578369           3.13584   3.13584            0.583082         0.583082                    0.031423        0.031423                   213.584    213.584\n",
       "    2        0.0200109                   0.529317           2.57495   2.85582            0.478788         0.531014                    0.0257245       0.0571475                  157.495    185.582\n",
       "    3        0.0300012                   0.495518           2.41198   2.70802            0.448485         0.503532                    0.0240964       0.0812439                  141.198    170.802\n",
       "    4        0.0400218                   0.468825           2.48593   2.65241            0.462236         0.493192                    0.0249105       0.106154                   148.593    165.241\n",
       "    5        0.0500121                   0.44556            2.21641   2.56532            0.412121         0.476998                    0.0221426       0.128297                   121.641    156.532\n",
       "    6        0.100024                    0.361287           2.06072   2.31302            0.383172         0.430085                    0.103061        0.231358                   106.072    131.302\n",
       "    7        0.150006                    0.306878           1.78508   2.13711            0.33192          0.397376                    0.0892218       0.32058                    78.5084    113.711\n",
       "    8        0.200018                    0.267574           1.57565   1.99673            0.292978         0.371273                    0.0788017       0.399381                   57.5652    99.6725\n",
       "    9        0.300012                    0.213245           1.37586   1.78979            0.255828         0.332795                    0.137577        0.536959                   37.5857    78.979\n",
       "    10       0.400006                    0.176494           1.12999   1.62485            0.210112         0.302127                    0.112993        0.649951                   12.9994    62.4853\n",
       "    11       0.5                         0.149726           0.996478  1.49919            0.185286         0.27876                     0.0996418       0.749593                   -0.352156  49.9186\n",
       "    12       0.599994                    0.129639           0.758756  1.37579            0.141084         0.255815                    0.0758711       0.825464                   -24.1244   37.5787\n",
       "    13       0.699988                    0.113067           0.628498  1.26904            0.116863         0.235966                    0.062846        0.88831                    -37.1502   26.9036\n",
       "    14       0.799982                    0.0982229          0.525919  1.17615            0.0977899        0.218694                    0.0525887       0.940899                   -47.4081   17.615\n",
       "    15       0.899976                    0.0826149          0.398917  1.08979            0.074175         0.202637                    0.0398893       0.980788                   -60.1083   8.97938\n",
       "    16       1                           0.0605773          0.192073  1                  0.0357143        0.185941                    0.019212        1                          -80.7927   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.1369765435568584\n",
      "RMSE: 0.3701034227845757\n",
      "LogLoss: 0.4355309009590542\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114069.89827018589\n",
      "AIC: 114093.89827018589\n",
      "AUC: 0.7071203432205223\n",
      "Gini: 0.4142406864410446\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17843972441369682: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>73561.0</td>\n",
       "<td>33520.0</td>\n",
       "<td>0.313</td>\n",
       "<td> (33520.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9215.0</td>\n",
       "<td>14659.0</td>\n",
       "<td>0.386</td>\n",
       "<td> (9215.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>82776.0</td>\n",
       "<td>48179.0</td>\n",
       "<td>0.3263</td>\n",
       "<td> (42735.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      73561  33520  0.313    (33520.0/107081.0)\n",
       "1      9215   14659  0.386    (9215.0/23874.0)\n",
       "Total  82776  48179  0.3263   (42735.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1784397</td>\n",
       "<td>0.4068949</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1136746</td>\n",
       "<td>0.5665672</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2879939</td>\n",
       "<td>0.3708050</td>\n",
       "<td>176.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5663591</td>\n",
       "<td>0.8184567</td>\n",
       "<td>36.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0620870</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1815977</td>\n",
       "<td>0.2412252</td>\n",
       "<td>258.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1671726</td>\n",
       "<td>0.6498819</td>\n",
       "<td>272.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1616542</td>\n",
       "<td>0.6512829</td>\n",
       "<td>278.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.17844      0.406895  261\n",
       "max f2                       0.113675     0.566567  334\n",
       "max f0point5                 0.287994     0.370805  176\n",
       "max accuracy                 0.566359     0.818457  36\n",
       "max precision                0.785924     1         0\n",
       "max recall                   0.062087     1         399\n",
       "max specificity              0.785924     1         0\n",
       "max absolute_mcc             0.181598     0.241225  258\n",
       "max min_per_class_accuracy   0.167173     0.649882  272\n",
       "max mean_per_class_accuracy  0.161654     0.651283  278"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5609815</td>\n",
       "<td>2.9394272</td>\n",
       "<td>2.9394272</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.0294044</td>\n",
       "<td>0.0294044</td>\n",
       "<td>193.9427222</td>\n",
       "<td>193.9427222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5136764</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7719385</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5053435</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0554578</td>\n",
       "<td>160.4449761</td>\n",
       "<td>177.1938491</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4802174</td>\n",
       "<td>2.4346323</td>\n",
       "<td>2.6595603</td>\n",
       "<td>0.4438503</td>\n",
       "<td>0.4848562</td>\n",
       "<td>0.0243361</td>\n",
       "<td>0.0797939</td>\n",
       "<td>143.4632310</td>\n",
       "<td>165.9560331</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4541967</td>\n",
       "<td>2.2987828</td>\n",
       "<td>2.5693487</td>\n",
       "<td>0.4190840</td>\n",
       "<td>0.4684100</td>\n",
       "<td>0.0229957</td>\n",
       "<td>0.1027896</td>\n",
       "<td>129.8782827</td>\n",
       "<td>156.9348739</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4304407</td>\n",
       "<td>2.3592048</td>\n",
       "<td>2.5273392</td>\n",
       "<td>0.4300993</td>\n",
       "<td>0.4607514</td>\n",
       "<td>0.0235821</td>\n",
       "<td>0.1263718</td>\n",
       "<td>135.9204803</td>\n",
       "<td>152.7339208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3489922</td>\n",
       "<td>2.0674422</td>\n",
       "<td>2.2973907</td>\n",
       "<td>0.3769090</td>\n",
       "<td>0.4188302</td>\n",
       "<td>0.1033761</td>\n",
       "<td>0.2297478</td>\n",
       "<td>106.7442216</td>\n",
       "<td>129.7390712</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2957391</td>\n",
       "<td>1.7583311</td>\n",
       "<td>2.1177042</td>\n",
       "<td>0.3205559</td>\n",
       "<td>0.3860721</td>\n",
       "<td>0.0879199</td>\n",
       "<td>0.3176678</td>\n",
       "<td>75.8331123</td>\n",
       "<td>111.7704182</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2569097</td>\n",
       "<td>1.5826559</td>\n",
       "<td>1.9839574</td>\n",
       "<td>0.2885291</td>\n",
       "<td>0.3616891</td>\n",
       "<td>0.0791237</td>\n",
       "<td>0.3967915</td>\n",
       "<td>58.2655941</td>\n",
       "<td>98.3957443</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2032167</td>\n",
       "<td>1.3679632</td>\n",
       "<td>1.7786208</td>\n",
       "<td>0.2493891</td>\n",
       "<td>0.3242548</td>\n",
       "<td>0.1368015</td>\n",
       "<td>0.5335930</td>\n",
       "<td>36.7963184</td>\n",
       "<td>77.8620797</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1679941</td>\n",
       "<td>1.1284673</td>\n",
       "<td>1.6160886</td>\n",
       "<td>0.2057274</td>\n",
       "<td>0.2946241</td>\n",
       "<td>0.1128424</td>\n",
       "<td>0.6464355</td>\n",
       "<td>12.8467313</td>\n",
       "<td>61.6088632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1428590</td>\n",
       "<td>0.9302652</td>\n",
       "<td>1.4789198</td>\n",
       "<td>0.1695938</td>\n",
       "<td>0.2696173</td>\n",
       "<td>0.0930301</td>\n",
       "<td>0.7394655</td>\n",
       "<td>-6.9734773</td>\n",
       "<td>47.8919761</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1238863</td>\n",
       "<td>0.8143060</td>\n",
       "<td>1.3681550</td>\n",
       "<td>0.1484536</td>\n",
       "<td>0.2494241</td>\n",
       "<td>0.0814275</td>\n",
       "<td>0.8208930</td>\n",
       "<td>-18.5693965</td>\n",
       "<td>36.8155036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1081113</td>\n",
       "<td>0.6589009</td>\n",
       "<td>1.2668363</td>\n",
       "<td>0.1201222</td>\n",
       "<td>0.2309530</td>\n",
       "<td>0.0658876</td>\n",
       "<td>0.8867806</td>\n",
       "<td>-34.1099078</td>\n",
       "<td>26.6836336</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0941428</td>\n",
       "<td>0.5185360</td>\n",
       "<td>1.1732952</td>\n",
       "<td>0.0945327</td>\n",
       "<td>0.2138998</td>\n",
       "<td>0.0518556</td>\n",
       "<td>0.9386362</td>\n",
       "<td>-48.1464047</td>\n",
       "<td>17.3295217</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0801775</td>\n",
       "<td>0.3945866</td>\n",
       "<td>1.0867750</td>\n",
       "<td>0.0719359</td>\n",
       "<td>0.1981266</td>\n",
       "<td>0.0394572</td>\n",
       "<td>0.9780933</td>\n",
       "<td>-60.5413434</td>\n",
       "<td>8.6774970</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597666</td>\n",
       "<td>0.2190584</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0399359</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0219067</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.0941597</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.560981           2.93943   2.93943            0.535878         0.535878                    0.0294044       0.0294044                  193.943   193.943\n",
       "    2        0.0200069                   0.513676           2.60445   2.77194            0.474809         0.505344                    0.0260534       0.0554578                  160.445   177.194\n",
       "    3        0.0300027                   0.480217           2.43463   2.65956            0.44385          0.484856                    0.0243361       0.0797939                  143.463   165.956\n",
       "    4        0.0400061                   0.454197           2.29878   2.56935            0.419084         0.46841                     0.0229957       0.10279                    129.878   156.935\n",
       "    5        0.0500019                   0.430441           2.3592    2.52734            0.430099         0.460751                    0.0235821       0.126372                   135.92    152.734\n",
       "    6        0.100004                    0.348992           2.06744   2.29739            0.376909         0.41883                     0.103376        0.229748                   106.744   129.739\n",
       "    7        0.150006                    0.295739           1.75833   2.1177             0.320556         0.386072                    0.0879199       0.317668                   75.8331   111.77\n",
       "    8        0.2                         0.25691            1.58266   1.98396            0.288529         0.361689                    0.0791237       0.396791                   58.2656   98.3957\n",
       "    9        0.300004                    0.203217           1.36796   1.77862            0.249389         0.324255                    0.136802        0.533593                   36.7963   77.8621\n",
       "    10       0.4                         0.167994           1.12847   1.61609            0.205727         0.294624                    0.112842        0.646435                   12.8467   61.6089\n",
       "    11       0.500004                    0.142859           0.930265  1.47892            0.169594         0.269617                    0.0930301       0.739466                   -6.97348  47.892\n",
       "    12       0.6                         0.123886           0.814306  1.36816            0.148454         0.249424                    0.0814275       0.820893                   -18.5694  36.8155\n",
       "    13       0.699996                    0.108111           0.658901  1.26684            0.120122         0.230953                    0.0658876       0.886781                   -34.1099  26.6836\n",
       "    14       0.8                         0.0941428          0.518536  1.1733             0.0945327        0.2139                      0.0518556       0.938636                   -48.1464  17.3295\n",
       "    15       0.899996                    0.0801775          0.394587  1.08677            0.0719359        0.198127                    0.0394572       0.978093                   -60.5413  8.6775\n",
       "    16       1                           0.0597666          0.219058  1                  0.0399359        0.182307                    0.0219067       1                          -78.0942  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OBinomialModel.tpr of >"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20181120_154205\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.12685990330985406\n",
      "RMSE: 0.3561739789904002\n",
      "LogLoss: 0.40703958453227496\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 106607.73758484815\n",
      "AIC: 106631.73758484815\n",
      "AUC: 0.7734040182726795\n",
      "Gini: 0.5468080365453589\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.21708051286464972: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>83212.0</td>\n",
       "<td>23869.0</td>\n",
       "<td>0.2229</td>\n",
       "<td> (23869.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9376.0</td>\n",
       "<td>14498.0</td>\n",
       "<td>0.3927</td>\n",
       "<td> (9376.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>92588.0</td>\n",
       "<td>38367.0</td>\n",
       "<td>0.2539</td>\n",
       "<td> (33245.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      83212  23869  0.2229   (23869.0/107081.0)\n",
       "1      9376   14498  0.3927   (9376.0/23874.0)\n",
       "Total  92588  38367  0.2539   (33245.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2170805</td>\n",
       "<td>0.4658666</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1410190</td>\n",
       "<td>0.6067433</td>\n",
       "<td>309.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3338429</td>\n",
       "<td>0.4568693</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4390853</td>\n",
       "<td>0.8290100</td>\n",
       "<td>98.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0676438</td>\n",
       "<td>1.0</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006766</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2335835</td>\n",
       "<td>0.3269004</td>\n",
       "<td>224.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1855661</td>\n",
       "<td>0.6993958</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1761337</td>\n",
       "<td>0.7006208</td>\n",
       "<td>274.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.217081     0.465867  238\n",
       "max f2                       0.141019     0.606743  309\n",
       "max f0point5                 0.333843     0.456869  155\n",
       "max accuracy                 0.439085     0.82901   98\n",
       "max precision                0.800677     1         0\n",
       "max recall                   0.0676438    1         394\n",
       "max specificity              0.800677     1         0\n",
       "max absolute_mcc             0.233584     0.3269    224\n",
       "max min_per_class_accuracy   0.185566     0.699396  265\n",
       "max mean_per_class_accuracy  0.176134     0.700621  274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5809896</td>\n",
       "<td>4.2290904</td>\n",
       "<td>4.2290904</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.7709924</td>\n",
       "<td>0.0423054</td>\n",
       "<td>0.0423054</td>\n",
       "<td>322.9090447</td>\n",
       "<td>322.9090447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5350310</td>\n",
       "<td>3.4879528</td>\n",
       "<td>3.8585216</td>\n",
       "<td>0.6358779</td>\n",
       "<td>0.7034351</td>\n",
       "<td>0.0348915</td>\n",
       "<td>0.0771970</td>\n",
       "<td>248.7952815</td>\n",
       "<td>285.8521631</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5007744</td>\n",
       "<td>3.1972882</td>\n",
       "<td>3.6382227</td>\n",
       "<td>0.5828877</td>\n",
       "<td>0.6632731</td>\n",
       "<td>0.0319595</td>\n",
       "<td>0.1091564</td>\n",
       "<td>219.7288214</td>\n",
       "<td>263.8222689</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4718430</td>\n",
       "<td>2.9436144</td>\n",
       "<td>3.4645375</td>\n",
       "<td>0.5366412</td>\n",
       "<td>0.6316091</td>\n",
       "<td>0.0294463</td>\n",
       "<td>0.1386027</td>\n",
       "<td>194.3614440</td>\n",
       "<td>246.4537481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4483865</td>\n",
       "<td>2.8704357</td>\n",
       "<td>3.3457716</td>\n",
       "<td>0.5233002</td>\n",
       "<td>0.6099572</td>\n",
       "<td>0.0286923</td>\n",
       "<td>0.1672950</td>\n",
       "<td>187.0435684</td>\n",
       "<td>234.5771560</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3623038</td>\n",
       "<td>2.4444070</td>\n",
       "<td>2.8950893</td>\n",
       "<td>0.4456323</td>\n",
       "<td>0.5277947</td>\n",
       "<td>0.1222250</td>\n",
       "<td>0.2895200</td>\n",
       "<td>144.4406963</td>\n",
       "<td>189.5089261</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3079302</td>\n",
       "<td>1.9878608</td>\n",
       "<td>2.5926798</td>\n",
       "<td>0.3624007</td>\n",
       "<td>0.4726634</td>\n",
       "<td>0.0993968</td>\n",
       "<td>0.3889168</td>\n",
       "<td>98.7860769</td>\n",
       "<td>159.2679764</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2684456</td>\n",
       "<td>1.7116814</td>\n",
       "<td>2.3724554</td>\n",
       "<td>0.3120513</td>\n",
       "<td>0.4325150</td>\n",
       "<td>0.0855743</td>\n",
       "<td>0.4744911</td>\n",
       "<td>71.1681359</td>\n",
       "<td>137.2455391</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2132009</td>\n",
       "<td>1.4102670</td>\n",
       "<td>2.0517178</td>\n",
       "<td>0.2571014</td>\n",
       "<td>0.3740423</td>\n",
       "<td>0.1410321</td>\n",
       "<td>0.6155232</td>\n",
       "<td>41.0267006</td>\n",
       "<td>105.1717765</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1765625</td>\n",
       "<td>1.1217652</td>\n",
       "<td>1.8192385</td>\n",
       "<td>0.2045055</td>\n",
       "<td>0.3316597</td>\n",
       "<td>0.1121722</td>\n",
       "<td>0.7276954</td>\n",
       "<td>12.1765206</td>\n",
       "<td>81.9238502</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1503145</td>\n",
       "<td>0.8745582</td>\n",
       "<td>1.6302967</td>\n",
       "<td>0.1594380</td>\n",
       "<td>0.2972143</td>\n",
       "<td>0.0874592</td>\n",
       "<td>0.8151546</td>\n",
       "<td>-12.5441786</td>\n",
       "<td>63.0296674</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1300345</td>\n",
       "<td>0.6769128</td>\n",
       "<td>1.4714054</td>\n",
       "<td>0.1234059</td>\n",
       "<td>0.2682474</td>\n",
       "<td>0.0676887</td>\n",
       "<td>0.8828433</td>\n",
       "<td>-32.3087165</td>\n",
       "<td>47.1405434</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1129563</td>\n",
       "<td>0.4959559</td>\n",
       "<td>1.3320601</td>\n",
       "<td>0.0904162</td>\n",
       "<td>0.2428437</td>\n",
       "<td>0.0495937</td>\n",
       "<td>0.9324370</td>\n",
       "<td>-50.4044061</td>\n",
       "<td>33.2060067</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0978849</td>\n",
       "<td>0.3920433</td>\n",
       "<td>1.2145535</td>\n",
       "<td>0.0714722</td>\n",
       "<td>0.2214215</td>\n",
       "<td>0.0392058</td>\n",
       "<td>0.9716428</td>\n",
       "<td>-60.7956663</td>\n",
       "<td>21.4553489</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0824531</td>\n",
       "<td>0.2194940</td>\n",
       "<td>1.1039951</td>\n",
       "<td>0.0400153</td>\n",
       "<td>0.2012659</td>\n",
       "<td>0.0219486</td>\n",
       "<td>0.9935914</td>\n",
       "<td>-78.0505987</td>\n",
       "<td>10.3995078</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597755</td>\n",
       "<td>0.0640840</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0116830</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0064086</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.5915993</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.58099            4.22909   4.22909            0.770992         0.770992                    0.0423054       0.0423054                  322.909   322.909\n",
       "    2        0.0200069                   0.535031           3.48795   3.85852            0.635878         0.703435                    0.0348915       0.077197                   248.795   285.852\n",
       "    3        0.0300027                   0.500774           3.19729   3.63822            0.582888         0.663273                    0.0319595       0.109156                   219.729   263.822\n",
       "    4        0.0400061                   0.471843           2.94361   3.46454            0.536641         0.631609                    0.0294463       0.138603                   194.361   246.454\n",
       "    5        0.0500019                   0.448387           2.87044   3.34577            0.5233           0.609957                    0.0286923       0.167295                   187.044   234.577\n",
       "    6        0.100004                    0.362304           2.44441   2.89509            0.445632         0.527795                    0.122225        0.28952                    144.441   189.509\n",
       "    7        0.150006                    0.30793            1.98786   2.59268            0.362401         0.472663                    0.0993968       0.388917                   98.7861   159.268\n",
       "    8        0.2                         0.268446           1.71168   2.37246            0.312051         0.432515                    0.0855743       0.474491                   71.1681   137.246\n",
       "    9        0.300004                    0.213201           1.41027   2.05172            0.257101         0.374042                    0.141032        0.615523                   41.0267   105.172\n",
       "    10       0.4                         0.176563           1.12177   1.81924            0.204506         0.33166                     0.112172        0.727695                   12.1765   81.9239\n",
       "    11       0.500004                    0.150314           0.874558  1.6303             0.159438         0.297214                    0.0874592       0.815155                   -12.5442  63.0297\n",
       "    12       0.6                         0.130034           0.676913  1.47141            0.123406         0.268247                    0.0676887       0.882843                   -32.3087  47.1405\n",
       "    13       0.699996                    0.112956           0.495956  1.33206            0.0904162        0.242844                    0.0495937       0.932437                   -50.4044  33.206\n",
       "    14       0.8                         0.0978849          0.392043  1.21455            0.0714722        0.221421                    0.0392058       0.971643                   -60.7957  21.4553\n",
       "    15       0.899996                    0.0824531          0.219494  1.104              0.0400153        0.201266                    0.0219486       0.993591                   -78.0506  10.3995\n",
       "    16       1                           0.0597755          0.064084  1                  0.011683         0.182307                    0.00640865      1                          -93.5916  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.13813682407268132\n",
      "RMSE: 0.37166762580655494\n",
      "LogLoss: 0.4379927029517987\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33020\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 28935.54992780763\n",
      "AIC: 28959.54992780763\n",
      "AUC: 0.7122628231156057\n",
      "Gini: 0.4245256462312115\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18933454821029921: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>18652.0</td>\n",
       "<td>8238.0</td>\n",
       "<td>0.3064</td>\n",
       "<td> (8238.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2386.0</td>\n",
       "<td>3756.0</td>\n",
       "<td>0.3885</td>\n",
       "<td> (2386.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21038.0</td>\n",
       "<td>11994.0</td>\n",
       "<td>0.3216</td>\n",
       "<td> (10624.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      18652  8238   0.3064   (8238.0/26890.0)\n",
       "1      2386   3756   0.3885   (2386.0/6142.0)\n",
       "Total  21038  11994  0.3216   (10624.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1893345</td>\n",
       "<td>0.4142038</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1197407</td>\n",
       "<td>0.5738535</td>\n",
       "<td>330.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3129965</td>\n",
       "<td>0.3817812</td>\n",
       "<td>160.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6010242</td>\n",
       "<td>0.8160572</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0640080</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7666583</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2092723</td>\n",
       "<td>0.2475688</td>\n",
       "<td>238.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1762849</td>\n",
       "<td>0.6525562</td>\n",
       "<td>268.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577331</td>\n",
       "<td>0.6547886</td>\n",
       "<td>286.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.189335     0.414204  256\n",
       "max f2                       0.119741     0.573853  330\n",
       "max f0point5                 0.312996     0.381781  160\n",
       "max accuracy                 0.601024     0.816057  29\n",
       "max precision                0.766658     1         0\n",
       "max recall                   0.064008     1         398\n",
       "max specificity              0.766658     1         0\n",
       "max absolute_mcc             0.209272     0.247569  238\n",
       "max min_per_class_accuracy   0.176285     0.652556  268\n",
       "max mean_per_class_accuracy  0.157733     0.654789  286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5783690</td>\n",
       "<td>3.1358434</td>\n",
       "<td>3.1358434</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.5830816</td>\n",
       "<td>0.0314230</td>\n",
       "<td>0.0314230</td>\n",
       "<td>213.5843447</td>\n",
       "<td>213.5843447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5293169</td>\n",
       "<td>2.5749465</td>\n",
       "<td>2.8558192</td>\n",
       "<td>0.4787879</td>\n",
       "<td>0.5310136</td>\n",
       "<td>0.0257245</td>\n",
       "<td>0.0571475</td>\n",
       "<td>157.4946469</td>\n",
       "<td>185.5819237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4955180</td>\n",
       "<td>2.4119752</td>\n",
       "<td>2.7080205</td>\n",
       "<td>0.4484848</td>\n",
       "<td>0.5035318</td>\n",
       "<td>0.0240964</td>\n",
       "<td>0.0812439</td>\n",
       "<td>141.1975173</td>\n",
       "<td>170.8020508</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4688250</td>\n",
       "<td>2.4859277</td>\n",
       "<td>2.6524133</td>\n",
       "<td>0.4622356</td>\n",
       "<td>0.4931921</td>\n",
       "<td>0.0249105</td>\n",
       "<td>0.1061543</td>\n",
       "<td>148.5927707</td>\n",
       "<td>165.2413309</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4455599</td>\n",
       "<td>2.2164096</td>\n",
       "<td>2.5653181</td>\n",
       "<td>0.4121212</td>\n",
       "<td>0.4769976</td>\n",
       "<td>0.0221426</td>\n",
       "<td>0.1282970</td>\n",
       "<td>121.6409619</td>\n",
       "<td>156.5318141</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3612868</td>\n",
       "<td>2.0607188</td>\n",
       "<td>2.3130185</td>\n",
       "<td>0.3831719</td>\n",
       "<td>0.4300847</td>\n",
       "<td>0.1030609</td>\n",
       "<td>0.2313579</td>\n",
       "<td>106.0718760</td>\n",
       "<td>131.3018450</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.3068785</td>\n",
       "<td>1.7850835</td>\n",
       "<td>2.1371112</td>\n",
       "<td>0.3319200</td>\n",
       "<td>0.3973764</td>\n",
       "<td>0.0892218</td>\n",
       "<td>0.3205796</td>\n",
       "<td>78.5083530</td>\n",
       "<td>113.7111174</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2675737</td>\n",
       "<td>1.5756523</td>\n",
       "<td>1.9967252</td>\n",
       "<td>0.2929782</td>\n",
       "<td>0.3712729</td>\n",
       "<td>0.0788017</td>\n",
       "<td>0.3993813</td>\n",
       "<td>57.5652259</td>\n",
       "<td>99.6725200</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2132448</td>\n",
       "<td>1.3758567</td>\n",
       "<td>1.7897899</td>\n",
       "<td>0.2558280</td>\n",
       "<td>0.3327952</td>\n",
       "<td>0.1375773</td>\n",
       "<td>0.5369586</td>\n",
       "<td>37.5856668</td>\n",
       "<td>78.9789907</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1764937</td>\n",
       "<td>1.1299935</td>\n",
       "<td>1.6248533</td>\n",
       "<td>0.2101120</td>\n",
       "<td>0.3021267</td>\n",
       "<td>0.1129925</td>\n",
       "<td>0.6499512</td>\n",
       "<td>12.9993524</td>\n",
       "<td>62.4853295</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1497263</td>\n",
       "<td>0.9964784</td>\n",
       "<td>1.4991859</td>\n",
       "<td>0.1852861</td>\n",
       "<td>0.2787600</td>\n",
       "<td>0.0996418</td>\n",
       "<td>0.7495930</td>\n",
       "<td>-0.3521561</td>\n",
       "<td>49.9185933</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1296393</td>\n",
       "<td>0.7587565</td>\n",
       "<td>1.3757872</td>\n",
       "<td>0.1410839</td>\n",
       "<td>0.2558151</td>\n",
       "<td>0.0758711</td>\n",
       "<td>0.8254640</td>\n",
       "<td>-24.1243542</td>\n",
       "<td>37.5787247</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1130667</td>\n",
       "<td>0.6284978</td>\n",
       "<td>1.2690362</td>\n",
       "<td>0.1168635</td>\n",
       "<td>0.2359657</td>\n",
       "<td>0.0628460</td>\n",
       "<td>0.8883100</td>\n",
       "<td>-37.1502161</td>\n",
       "<td>26.9036234</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0982229</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1761501</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2186944</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9408987</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6150117</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0826149</td>\n",
       "<td>0.3989170</td>\n",
       "<td>1.0897938</td>\n",
       "<td>0.0741750</td>\n",
       "<td>0.2026372</td>\n",
       "<td>0.0398893</td>\n",
       "<td>0.9807880</td>\n",
       "<td>-60.1082978</td>\n",
       "<td>8.9793790</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0605773</td>\n",
       "<td>0.1920733</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0357143</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0192120</td>\n",
       "<td>1.0</td>\n",
       "<td>-80.7926687</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain       cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  ---------  -----------------\n",
       "    1        0.0100206                   0.578369           3.13584   3.13584            0.583082         0.583082                    0.031423        0.031423                   213.584    213.584\n",
       "    2        0.0200109                   0.529317           2.57495   2.85582            0.478788         0.531014                    0.0257245       0.0571475                  157.495    185.582\n",
       "    3        0.0300012                   0.495518           2.41198   2.70802            0.448485         0.503532                    0.0240964       0.0812439                  141.198    170.802\n",
       "    4        0.0400218                   0.468825           2.48593   2.65241            0.462236         0.493192                    0.0249105       0.106154                   148.593    165.241\n",
       "    5        0.0500121                   0.44556            2.21641   2.56532            0.412121         0.476998                    0.0221426       0.128297                   121.641    156.532\n",
       "    6        0.100024                    0.361287           2.06072   2.31302            0.383172         0.430085                    0.103061        0.231358                   106.072    131.302\n",
       "    7        0.150006                    0.306878           1.78508   2.13711            0.33192          0.397376                    0.0892218       0.32058                    78.5084    113.711\n",
       "    8        0.200018                    0.267574           1.57565   1.99673            0.292978         0.371273                    0.0788017       0.399381                   57.5652    99.6725\n",
       "    9        0.300012                    0.213245           1.37586   1.78979            0.255828         0.332795                    0.137577        0.536959                   37.5857    78.979\n",
       "    10       0.400006                    0.176494           1.12999   1.62485            0.210112         0.302127                    0.112993        0.649951                   12.9994    62.4853\n",
       "    11       0.5                         0.149726           0.996478  1.49919            0.185286         0.27876                     0.0996418       0.749593                   -0.352156  49.9186\n",
       "    12       0.599994                    0.129639           0.758756  1.37579            0.141084         0.255815                    0.0758711       0.825464                   -24.1244   37.5787\n",
       "    13       0.699988                    0.113067           0.628498  1.26904            0.116863         0.235966                    0.062846        0.88831                    -37.1502   26.9036\n",
       "    14       0.799982                    0.0982229          0.525919  1.17615            0.0977899        0.218694                    0.0525887       0.940899                   -47.4081   17.615\n",
       "    15       0.899976                    0.0826149          0.398917  1.08979            0.074175         0.202637                    0.0398893       0.980788                   -60.1083   8.97938\n",
       "    16       1                           0.0605773          0.192073  1                  0.0357143        0.185941                    0.019212        1                          -80.7927   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.1369765435568584\n",
      "RMSE: 0.3701034227845757\n",
      "LogLoss: 0.4355309009590542\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130943\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114069.89827018589\n",
      "AIC: 114093.89827018589\n",
      "AUC: 0.7071203432205223\n",
      "Gini: 0.4142406864410446\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17843972441369682: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>73561.0</td>\n",
       "<td>33520.0</td>\n",
       "<td>0.313</td>\n",
       "<td> (33520.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9215.0</td>\n",
       "<td>14659.0</td>\n",
       "<td>0.386</td>\n",
       "<td> (9215.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>82776.0</td>\n",
       "<td>48179.0</td>\n",
       "<td>0.3263</td>\n",
       "<td> (42735.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      73561  33520  0.313    (33520.0/107081.0)\n",
       "1      9215   14659  0.386    (9215.0/23874.0)\n",
       "Total  82776  48179  0.3263   (42735.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1784397</td>\n",
       "<td>0.4068949</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1136746</td>\n",
       "<td>0.5665672</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2879939</td>\n",
       "<td>0.3708050</td>\n",
       "<td>176.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5663591</td>\n",
       "<td>0.8184567</td>\n",
       "<td>36.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0620870</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.7859236</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1815977</td>\n",
       "<td>0.2412252</td>\n",
       "<td>258.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1671726</td>\n",
       "<td>0.6498819</td>\n",
       "<td>272.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1616542</td>\n",
       "<td>0.6512829</td>\n",
       "<td>278.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.17844      0.406895  261\n",
       "max f2                       0.113675     0.566567  334\n",
       "max f0point5                 0.287994     0.370805  176\n",
       "max accuracy                 0.566359     0.818457  36\n",
       "max precision                0.785924     1         0\n",
       "max recall                   0.062087     1         399\n",
       "max specificity              0.785924     1         0\n",
       "max absolute_mcc             0.181598     0.241225  258\n",
       "max min_per_class_accuracy   0.167173     0.649882  272\n",
       "max mean_per_class_accuracy  0.161654     0.651283  278"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5609815</td>\n",
       "<td>2.9394272</td>\n",
       "<td>2.9394272</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.5358779</td>\n",
       "<td>0.0294044</td>\n",
       "<td>0.0294044</td>\n",
       "<td>193.9427222</td>\n",
       "<td>193.9427222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5136764</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7719385</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5053435</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0554578</td>\n",
       "<td>160.4449761</td>\n",
       "<td>177.1938491</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4802174</td>\n",
       "<td>2.4346323</td>\n",
       "<td>2.6595603</td>\n",
       "<td>0.4438503</td>\n",
       "<td>0.4848562</td>\n",
       "<td>0.0243361</td>\n",
       "<td>0.0797939</td>\n",
       "<td>143.4632310</td>\n",
       "<td>165.9560331</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4541967</td>\n",
       "<td>2.2987828</td>\n",
       "<td>2.5693487</td>\n",
       "<td>0.4190840</td>\n",
       "<td>0.4684100</td>\n",
       "<td>0.0229957</td>\n",
       "<td>0.1027896</td>\n",
       "<td>129.8782827</td>\n",
       "<td>156.9348739</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4304407</td>\n",
       "<td>2.3592048</td>\n",
       "<td>2.5273392</td>\n",
       "<td>0.4300993</td>\n",
       "<td>0.4607514</td>\n",
       "<td>0.0235821</td>\n",
       "<td>0.1263718</td>\n",
       "<td>135.9204803</td>\n",
       "<td>152.7339208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3489922</td>\n",
       "<td>2.0674422</td>\n",
       "<td>2.2973907</td>\n",
       "<td>0.3769090</td>\n",
       "<td>0.4188302</td>\n",
       "<td>0.1033761</td>\n",
       "<td>0.2297478</td>\n",
       "<td>106.7442216</td>\n",
       "<td>129.7390712</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2957391</td>\n",
       "<td>1.7583311</td>\n",
       "<td>2.1177042</td>\n",
       "<td>0.3205559</td>\n",
       "<td>0.3860721</td>\n",
       "<td>0.0879199</td>\n",
       "<td>0.3176678</td>\n",
       "<td>75.8331123</td>\n",
       "<td>111.7704182</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2569097</td>\n",
       "<td>1.5826559</td>\n",
       "<td>1.9839574</td>\n",
       "<td>0.2885291</td>\n",
       "<td>0.3616891</td>\n",
       "<td>0.0791237</td>\n",
       "<td>0.3967915</td>\n",
       "<td>58.2655941</td>\n",
       "<td>98.3957443</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2032167</td>\n",
       "<td>1.3679632</td>\n",
       "<td>1.7786208</td>\n",
       "<td>0.2493891</td>\n",
       "<td>0.3242548</td>\n",
       "<td>0.1368015</td>\n",
       "<td>0.5335930</td>\n",
       "<td>36.7963184</td>\n",
       "<td>77.8620797</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1679941</td>\n",
       "<td>1.1284673</td>\n",
       "<td>1.6160886</td>\n",
       "<td>0.2057274</td>\n",
       "<td>0.2946241</td>\n",
       "<td>0.1128424</td>\n",
       "<td>0.6464355</td>\n",
       "<td>12.8467313</td>\n",
       "<td>61.6088632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1428590</td>\n",
       "<td>0.9302652</td>\n",
       "<td>1.4789198</td>\n",
       "<td>0.1695938</td>\n",
       "<td>0.2696173</td>\n",
       "<td>0.0930301</td>\n",
       "<td>0.7394655</td>\n",
       "<td>-6.9734773</td>\n",
       "<td>47.8919761</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1238863</td>\n",
       "<td>0.8143060</td>\n",
       "<td>1.3681550</td>\n",
       "<td>0.1484536</td>\n",
       "<td>0.2494241</td>\n",
       "<td>0.0814275</td>\n",
       "<td>0.8208930</td>\n",
       "<td>-18.5693965</td>\n",
       "<td>36.8155036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1081113</td>\n",
       "<td>0.6589009</td>\n",
       "<td>1.2668363</td>\n",
       "<td>0.1201222</td>\n",
       "<td>0.2309530</td>\n",
       "<td>0.0658876</td>\n",
       "<td>0.8867806</td>\n",
       "<td>-34.1099078</td>\n",
       "<td>26.6836336</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0941428</td>\n",
       "<td>0.5185360</td>\n",
       "<td>1.1732952</td>\n",
       "<td>0.0945327</td>\n",
       "<td>0.2138998</td>\n",
       "<td>0.0518556</td>\n",
       "<td>0.9386362</td>\n",
       "<td>-48.1464047</td>\n",
       "<td>17.3295217</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0801775</td>\n",
       "<td>0.3945866</td>\n",
       "<td>1.0867750</td>\n",
       "<td>0.0719359</td>\n",
       "<td>0.1981266</td>\n",
       "<td>0.0394572</td>\n",
       "<td>0.9780933</td>\n",
       "<td>-60.5413434</td>\n",
       "<td>8.6774970</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0597666</td>\n",
       "<td>0.2190584</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0399359</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0219067</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.0941597</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.560981           2.93943   2.93943            0.535878         0.535878                    0.0294044       0.0294044                  193.943   193.943\n",
       "    2        0.0200069                   0.513676           2.60445   2.77194            0.474809         0.505344                    0.0260534       0.0554578                  160.445   177.194\n",
       "    3        0.0300027                   0.480217           2.43463   2.65956            0.44385          0.484856                    0.0243361       0.0797939                  143.463   165.956\n",
       "    4        0.0400061                   0.454197           2.29878   2.56935            0.419084         0.46841                     0.0229957       0.10279                    129.878   156.935\n",
       "    5        0.0500019                   0.430441           2.3592    2.52734            0.430099         0.460751                    0.0235821       0.126372                   135.92    152.734\n",
       "    6        0.100004                    0.348992           2.06744   2.29739            0.376909         0.41883                     0.103376        0.229748                   106.744   129.739\n",
       "    7        0.150006                    0.295739           1.75833   2.1177             0.320556         0.386072                    0.0879199       0.317668                   75.8331   111.77\n",
       "    8        0.2                         0.25691            1.58266   1.98396            0.288529         0.361689                    0.0791237       0.396791                   58.2656   98.3957\n",
       "    9        0.300004                    0.203217           1.36796   1.77862            0.249389         0.324255                    0.136802        0.533593                   36.7963   77.8621\n",
       "    10       0.4                         0.167994           1.12847   1.61609            0.205727         0.294624                    0.112842        0.646435                   12.8467   61.6089\n",
       "    11       0.500004                    0.142859           0.930265  1.47892            0.169594         0.269617                    0.0930301       0.739466                   -6.97348  47.892\n",
       "    12       0.6                         0.123886           0.814306  1.36816            0.148454         0.249424                    0.0814275       0.820893                   -18.5694  36.8155\n",
       "    13       0.699996                    0.108111           0.658901  1.26684            0.120122         0.230953                    0.0658876       0.886781                   -34.1099  26.6836\n",
       "    14       0.8                         0.0941428          0.518536  1.1733             0.0945327        0.2139                      0.0518556       0.938636                   -48.1464  17.3295\n",
       "    15       0.899996                    0.0801775          0.394587  1.08677            0.0719359        0.198127                    0.0394572       0.978093                   -60.5413  8.6775\n",
       "    16       1                           0.0597666          0.219058  1                  0.0399359        0.182307                    0.0219067       1                          -78.0942  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ModelBase.weights of >"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Sets for Binary Classifier \n",
    "\n",
    "#### Some Kaggle Binary classification competitions  \n",
    "\n",
    "The idea here is to get a range of datasets to test our H2O binary classification models as well as to understand which approaches work best for binary classification.   The hope is to get a single model or set of models that perform well in these competitions as well as logic and tests to dynamically choose the best models and their parameters.  \n",
    "\n",
    "[Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction)    \n",
    "\n",
    "[Facebook Recruiting IV: Human or Robot?](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot)    \n",
    "\n",
    "[DonorsChoose.org Application Screening Predict whether teachers' project proposals are accepted](https://www.kaggle.com/c/donorschoose-application-screening)    \n",
    "\n",
    "[Statoil/C-CORE Iceberg Classifier Challenge Ship or iceberg, can you decide from space?](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)    \n",
    "\n",
    "[WSDM - KKBox's Churn Prediction Challenge Can you predict when subscribers will churn?](https://www.kaggle.com/c/kkbox-churn-prediction-challenge)    \n",
    "\n",
    "[Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year.](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)    \n",
    "\n",
    "[Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year.](https://www.kaggle.com/c/dato-native)    \n",
    "\n",
    "[Data Science Bowl 2017 Can you improve lung cancer detection?](https://www.kaggle.com/c/data-science-bowl-2017)    \n",
    "\n",
    "[Random Acts of Pizza Predicting altruism through free pizza](https://www.kaggle.com/c/random-acts-of-pizza)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update:  October 3, 2018"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
