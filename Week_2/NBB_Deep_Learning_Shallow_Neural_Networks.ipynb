{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 01 What is \"Deep Learning?\" \n",
    "- 02 Why Deep Learning?    \n",
    "- 03 The Perceptron (Neural units)  \n",
    "- 04 Shallow Neural Network  \n",
    "- 05 Activation functions  \n",
    "- 06 Loss functions   \n",
    "- 07 Cross entropy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"Deep Learning?\"\n",
    "\n",
    "[Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is the stacking of artificial neural networks (ANNs) to create stacked neural networks, [deep belief networks](https://en.wikipedia.org/wiki/Deep_belief_network), [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) and deep generative models. A deep neural network (DNN) is an ANN with multiple hidden layers between the input and output layers.\n",
    "\n",
    "An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.\n",
    "\n",
    "_ Deep learning is basically the deep stacking of artificial neurons to learn complex models of data. _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Learning?\n",
    "\n",
    "- It works\n",
    "\n",
    "Deep learning and neural networks are increasingly important concepts as demonstrated through their performance on difficult problems in computer vision medical diagnosis, natural language processing and many other domains. \n",
    "\n",
    "- Learns feature selection   \n",
    "\n",
    "Deep learning algorithms are unique in that they try to learn latent features from data, as opposed to traditional machine learning where features selection is typically handcrafted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data\n",
    "\n",
    "The [MNIST database](http://yann.lecun.com/exdb/mnist/) of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4v8BOX6Pco9cUTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset is structured as a 3-dimensional array of each image, and that images width and image height (28×28 pixels per image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "## 60K 28×28 sized training images\n",
    "print (X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIAlKMoAAFiCogwAgCUoygAAWOL/ATR3+KAZRPmrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot 4 more images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[55], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[555], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[5555], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[55555], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![But what *is* a Neural Network? ](http://nikbearbrown.com/YouTube/MachineLearning/IMG/But_what_is_a_Neural_Network.png)   \n",
    "\n",
    "[But what *is* a Neural Network?](https://youtu.be/aircAruvnKk)   \n",
    "\n",
    "\n",
    "## The Perceptron (Neural units)\n",
    "\n",
    "The [perceptron]()https://en.wikipedia.org/wiki/Perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide\n",
    "whether an input, represented by a vector of numbers, belongs to some\n",
    "specific class or not). It is a type of linear classifier, i.e. a\n",
    "classification algorithm that makes its predictions based on a linear\n",
    "predictor function combining a set of weights with the feature\n",
    "vector. \n",
    "\n",
    "The perceptron algorithm dates back to the late 1950s, and is the basis of [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network).\n",
    "\n",
    "Definition\n",
    "----------\n",
    "\n",
    "In the modern sense, the perceptron is an algorithm for learning a\n",
    "binary classifier: a function that maps its input (a real-valued\n",
    "[vector]) to an output value $f(x)$ (a single [binary] value):\n",
    "\n",
    "$$f(x) = \\begin{cases}1 & \\text{if }\\ w \\cdot x + b > 0\\\\0 & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "where is a vector of real-valued weights, $w \\cdot x$ is the [dot\n",
    "product] $\\sum_{i=1}^m w_i x_i$, where m is the number of inputs to the\n",
    "perceptron and is the *bias*. The bias shifts the decision boundary away\n",
    "from the origin and does not depend on any input value.\n",
    "\n",
    "The value of $f(x)$ (0 or 1) is used to classify as either a positive or\n",
    "a negative instance, in the case of a binary classification problem. If\n",
    "is negative, then the weighted combination of inputs must produce a\n",
    "positive value greater than $|b|$ in order to push the classifier neuron\n",
    "over the 0 threshold. Spatially, the bias alters the position (though\n",
    "not the orientation) of the decision boundary. The perceptron learning\n",
    "algorithm does not terminate if the learning set is not linearly\n",
    "separable. If the vectors are not linearly separable learning will\n",
    "never reach a point where all vectors are classified properly. The most\n",
    "famous example of the perceptron's inability to solve problems with\n",
    "linearly nonseparable vectors is the Boolean exclusive-or problem. The\n",
    "solution spaces of decision boundaries for all binary functions and\n",
    "learning behaviors are studied in the reference.\n",
    "\n",
    "In the context of neural networks, a perceptron is an _artificial\n",
    "neuron_ using the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function) as the activation function.\n",
    "The perceptron algorithm is also termed the **single-layer perceptron**,\n",
    "to distinguish it from a multilayer perceptron, which is a misnomer\n",
    "for a more complicated neural network. As a linear classifier, the\n",
    "single-layer perceptron is the simplest [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network).\n",
    "\n",
    "Learning algorithm\n",
    "------------------\n",
    "\n",
    "Below is an example of a learning algorithm for a (single-layer)\n",
    "perceptron. For [multilayer perceptrons], where a hidden layer exists,\n",
    "more sophisticated algorithms such as [backpropagation] must be used.\n",
    "Alternatively, methods such as the [delta rule] can be used if the\n",
    "function is non-linear and differentiable, although the one below will\n",
    "work as well.\n",
    "\n",
    "When multiple perceptrons are combined in an artificial neural network,\n",
    "each output neuron operates independently of all the others; thus,\n",
    "learning each output can be considered in isolation. ![A diagram showing a perceptron updating its linear boundary as more\n",
    "training examples are added.](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron_example.svg.png)\n",
    "\n",
    "### Definitions\n",
    "\n",
    "We first define some variables:\n",
    "\n",
    "-   $y = f(\\mathbf{z})$ denotes the *output* from the perceptron for an\n",
    "    input vector $\\mathbf{z}$.\n",
    "-   $D = \\{(\\mathbf{x}_1,d_1),\\dots,(\\mathbf{x}_s,d_s)\\}$ is the\n",
    "    *training set* of $s$ samples, where:\n",
    "    -   $\\mathbf{x}_j$ is the $n$-dimensional input vector.\n",
    "    -   $d_j$ is the desired output value of the perceptron for that\n",
    "        input.\n",
    "\n",
    "We show the values of the features as follows:\n",
    "\n",
    "-   $x_{j,i}$ is the value of the $i$th feature of the $j$th training\n",
    "    *input vector*.\n",
    "-   $x_{j,0} = 1$.\n",
    "\n",
    "To represent the weights:\n",
    "\n",
    "-   $w_i$ is the $i$th value in the *weight vector*, to be multiplied by\n",
    "    the value of the $i$th input feature.\n",
    "-   Because $x_{j,0} = 1$, the $w_0$ is effectively a bias that we use\n",
    "    instead of the bias constant $b$.\n",
    "\n",
    "To show the time-dependence of $\\mathbf{w}$, we use:\n",
    "\n",
    "-   $w_i(t)$ is the weight $i$ at time $t$.\n",
    "\n",
    "Unlike other linear classification algorithms such as [logistic\n",
    "regression], there is no need for a *learning rate* in the perceptron\n",
    "algorithm. This is because multiplying the update by any constant simply\n",
    "rescales the weights but never changes the sign of the prediction.\n",
    "\n",
    "![The appropriate weights are applied to the inputs, and the resulting\n",
    "weighted sum passed to a function that produces the output o.](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Perceptron.svg.png)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Initialize the weights and the threshold. Weights may\n",
    "be initialized to 0 or to a small random value. In the example below, we\n",
    "use 0.  \n",
    "2. For each example in our training set , perform the following\n",
    "steps over the input $\\mathbf{x}_j$ and desired output $d_j$   \n",
    "\n",
    "Calculate the actual output:   \n",
    "\n",
    "a. $$\\begin{align}\n",
    "y_j(t) &= f[\\mathbf{w}(t)\\cdot\\mathbf{x}_j] \\\\\n",
    "&= f[w_0(t)x_{j,0} + w_1(t)x_{j,1} + w_2(t)x_{j,2} + \\dotsb + w_n(t)x_{j,n}]\n",
    "\\end{align}$$\n",
    "\n",
    "b. Update the weights:  \n",
    "\n",
    "$$w_i(t+1) = w_i(t) + (d_j - y_j(t)) x_{j,i}$$, for all features\n",
    "$0 \\leq i \\leq n$.   \n",
    "\n",
    "**offline learning**\n",
    "\n",
    "For offline learning, the step 2 may be repeated until the iteration error  $\\frac{1}{s} \\sum_{j=1}^s |d_j - y_j(t)|$ is less than a user-specified error threshold $\\gamma$, or a predetermined number of iterations have been completed. The algorithm updates the weights after steps 2a and 2b. These weights are immediately applied to a pair in the training set, and subsequently updated, rather than waiting until all pairs in the training set have undergone these steps. \n",
    "\n",
    "Multiclass perceptron\n",
    "---------------------\n",
    "\n",
    "Like most other techniques for training linear classifiers, the\n",
    "perceptron generalizes naturally to multiclass classification. Here,\n",
    "the input $x$ and the output $y$ are drawn from arbitrary sets. A\n",
    "feature representation function $f(x,y)$ maps each possible input/output\n",
    "pair to a finite-dimensional real-valued feature vector. As before, the\n",
    "feature vector is multiplied by a weight vector $w$, but now the\n",
    "resulting score is used to choose among many possible outputs:\n",
    "\n",
    "$$\\hat y = \\operatorname{argmax}_y f(x,y) \\cdot w.$$ ≈ Learning again\n",
    "iterates over the examples, predicting an output for each, leaving the\n",
    "weights unchanged when the predicted output matches the target, and\n",
    "changing them when it does not. The update becomes:\n",
    "\n",
    "$$w_{t+1} = w_t + f(x, y) - f(x,\\hat y).$$\n",
    "\n",
    "This multiclass feedback formulation reduces to the original perceptron\n",
    "when $x$ is a real-valued vector, $y$ is chosen from $\\{0,1\\}$, and\n",
    "$f(x,y) = y x$.\n",
    "\n",
    "For certain problems, input/output representations and features can be\n",
    "chosen so that $\\mathrm{argmax}_y f(x,y) \\cdot w$ can be found\n",
    "efficiently even though $y$ is chosen from a very large or even infinite\n",
    "set.\n",
    "\n",
    "In recent years, perceptron training has become popular in the field of\n",
    "[natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) for such tasks as [part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "and [syntactic parsing](https://en.wikipedia.org/wiki/Parsing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neural network\n",
    "\n",
    "[Artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) (**ANNs**) or **[connectionist] systems**\n",
    "are computing systems inspired by the biological neural networks that\n",
    "constitute animal brains. Such systems learn (progressively improve\n",
    "performance) to do tasks by considering examples, generally without\n",
    "task-specific programming. For example, in image recognition, they might\n",
    "learn to identify images that contain cats by analyzing example images\n",
    "that have been manually labeled as “cat” or “no cat” and using the\n",
    "analytic results to identify cats in other images. They have found most\n",
    "use in applications difficult to express in a traditional computer\n",
    "algorithm using rule-based programming.\n",
    "\n",
    "An ANN is based on a collection of connected units called artificial\n",
    "neurons, (analogous to axons in a biological brain). Each\n",
    "connection synapse) between neurons can transmit a signal to another\n",
    "neuron. The receiving (postsynaptic) neuron can process the signal(s)\n",
    "and then signal downstream neurons connected to it. Neurons may have\n",
    "state, generally represented by [real numbers], typically between 0 and 1.\n",
    "\n",
    "Neurons and synapses may also have a weight that varies as learning\n",
    "proceeds, which can increase or decrease the strength of the signal that\n",
    "it sends downstream. Further, they may have a threshold such that only\n",
    "if the aggregate signal is below (or above) that level is the downstream\n",
    "signal sent.\n",
    "\n",
    "Typically, neurons are organized in layers. Different layers may perform\n",
    "different kinds of transformations on their inputs. Signals travel from\n",
    "the first (input), to the last (output) layer, possibly after traversing\n",
    "the layers multiple times.\n",
    "\n",
    "The original goal of the neural network approach was to solve problems\n",
    "in the same way that a human brain would. Over time, attention focused\n",
    "on matching specific mental abilities, leading to deviations from\n",
    "biology such as backpropagation, or passing information in the reverse\n",
    "direction and adjusting the network to reflect that information.\n",
    "\n",
    "Neural networks have been used on a variety of tasks, including\n",
    "computer vision, speech recognition, machine translation, social\n",
    "network filtering, playing board and video games, medical diagnosis and\n",
    "in many other domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "\n",
    "**Backpropagation** is a method used in artificial neural networks to\n",
    "calculate a gradient that is needed in the calculation of the\n",
    "weights to be used in the network. It is commonly used to train\n",
    "deep neural networks, a term referring to neural networks with\n",
    "more than one hidden layer.\n",
    "\n",
    "Backpropagation is a special case of an older and more general technique\n",
    "called automatic differentiation. In the context of learning,\n",
    "backpropagation is commonly used by the gradient descent optimization\n",
    "algorithm to adjust the weight of neurons by calculating the gradient\n",
    "of the loss function. This technique is also sometimes called\n",
    "**backward propagation of errors**, because the error is calculated at\n",
    "the output and distributed back through the network layers.\n",
    "\n",
    "\n",
    "![What is backpropagation really doing?](http://nikbearbrown.com/YouTube/MachineLearning/IMG/What_is_backpropagation_really_doing.png)\n",
    "\n",
    "[What is backpropagation really doing?](https://youtu.be/Ilg3gGewQ5U)   \n",
    "\n",
    "![Gradient descent, how neural networks learn?](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Gradient_descent_how_neural_networks_learn.png)\n",
    "\n",
    "[Gradient descent, how neural networks learn?](https://youtu.be/IHZwWFHWa-w)  \n",
    "\n",
    "![Backpropagation calculus](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Backpropagation_calculus.png)\n",
    "\n",
    "[Backpropagation calculus](https://youtu.be/tIeHLnjs5U8)   \n",
    "\n",
    "\n",
    "**Loss function**\n",
    "\n",
    "\n",
    "Sometimes referred to as the **cost function** or **error function**\n",
    "(not to be confused with the Gauss error function), the loss function\n",
    "is a function that maps values of one or more variables onto a real\n",
    "number intuitively representing some \\\"cost\\\" associated with those\n",
    "values. For backpropagation, the loss function calculates the difference\n",
    "between the network output and its expected output, after a case\n",
    "propagates through the network.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Two assumptions must be made about the form of the error function.^1\n",
    "The first is that it can be written as an average\n",
    "$E=\\frac{1}{n}\\sum_xE_x$ over error functions $E_x$, for $n$ individual\n",
    "training examples, $x$. The reason for this assumption is that the\n",
    "backpropagation algorithm calculates the gradient of the error function\n",
    "for a single training example, which needs to be generalized to the\n",
    "overall error function. The second assumption is that it can be written\n",
    "as a function of the outputs from the neural network.\n",
    "\n",
    "### Example loss function\n",
    "\n",
    "Let $y,y'$ be vectors in $\\mathbb{R}^n$.\n",
    "\n",
    "Select an error function $E(y,y')$ measuring the difference between two\n",
    "outputs. The standard choice is the square of the Euclidean distance\n",
    "between the vectors $y$ and $y'$:\n",
    "\n",
    "$E(y,y') = \\tfrac{1}{2} \\lVert y-y'\\rVert^2$\n",
    "\n",
    "Note that the factor of $\\tfrac{1}{2}$ conveniently cancels the exponent\n",
    "when the error function is subsequently differentiated.\n",
    "\n",
    "The error function over $n$ training examples can simply be written as\n",
    "an average of losses over individual\n",
    "examples$$E=\\frac{1}{2n}\\sum_x\\lVert (y(x)-y'(x)) \\rVert^2$$\n",
    "\n",
    "and therefore, the partial derivative with respect to the\n",
    "outputs$$\\frac{\\partial E}{\\partial y'} = y'-y$$\n",
    "\n",
    "\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Let $N$ be a neural network with $e$ connections, $m$ inputs, and $n$\n",
    "outputs.\n",
    "\n",
    "Below, $x_1,x_2,\\dots$ will denote vectors in $\\mathbb{R}^m$,\n",
    "$y_1,y_2,\\dots$ vectors in $\\mathbb{R}^n$, and $w_0, w_1, w_2, \\ldots$\n",
    "vectors in $\\mathbb{R}^e$. These are called *inputs*, *outputs* and\n",
    "*weights* respectively.\n",
    "\n",
    "The neural network corresponds to a function $y = f_N(w, x)$ which,\n",
    "given a weight $w$, maps an input $x$ to an output $y$.\n",
    "\n",
    "The optimization takes as input a sequence of *training examples*\n",
    "$(x_1,y_1), \\dots, (x_p, y_p)$ and produces a sequence of weights\n",
    "$w_0, w_1, \\dots, w_p$ starting from some initial weight $w_0$, usually\n",
    "chosen at random.\n",
    "\n",
    "These weights are computed in turn: first compute $w_i$ using only\n",
    "$(x_i, y_i, w_{i-1})$ for $i = 1, \\dots, p$. The output of the algorithm\n",
    "is then $w_p$, giving us a new function $x \\mapsto f_N(w_p, x)$. The\n",
    "computation is the same in each step, hence only the case $i = 1$ is\n",
    "described.\n",
    "\n",
    "Calculating $w_1$ from $(x_1, y_1, w_0)$ is done by considering a\n",
    "variable weight $w$ and applying gradient descent to the function\n",
    "$w\\mapsto E(f_N(w, x_1), y_1)$ to find a local minimum, starting at\n",
    "$w = w_0$.\n",
    "\n",
    "This makes $w_1$ the minimizing weight found by gradient descent.\n",
    "\n",
    "\n",
    "**Algorithm in code**\n",
    "\n",
    "\n",
    "To implement the algorithm above, explicit formulas are required for the\n",
    "gradient of the function $w \\mapsto E(f_N(w, x), y)$ where the function\n",
    "is $E(y,y')= |y-y'|^2$.\n",
    "\n",
    "The learning algorithm can be divided into two phases: propagation and\n",
    "weight update.\n",
    "\n",
    "### Phase 1: propagation\n",
    "\n",
    "Each propagation involves the following steps:\n",
    "\n",
    "1.  Propagation forward through the network to generate the output\n",
    "    value(s)\n",
    "2.  Calculation of the cost (error term)\n",
    "3.  Propagation of the output activations back through the network using\n",
    "    the training pattern target in order to generate the deltas (the\n",
    "    difference between the targeted and actual output values) of all\n",
    "    output and hidden neurons.\n",
    "\n",
    "### Phase 2: weight update\n",
    "\n",
    "For each weight, the following steps must be followed:\n",
    "\n",
    "1.  The weight\\'s output delta and input activation are multiplied to\n",
    "    find the gradient of the weight.\n",
    "2.  A ratio (percentage) of the weight\\'s gradient is subtracted from\n",
    "    the weight.\n",
    "\n",
    "This ratio (percentage) influences the speed and quality of learning; it\n",
    "is called the *learning rate*. The greater the ratio, the faster the\n",
    "neuron trains, but the lower the ratio, the more accurate the training\n",
    "is. The sign of the gradient of a weight indicates whether the error\n",
    "varies directly with, or inversely to, the weight. Therefore, the weight\n",
    "must be updated in the opposite direction, \\\"descending\\\" the gradient.\n",
    "\n",
    "Learning is repeated (on new batches) until the network performs\n",
    "adequately.\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "The following is pseudocode for a stochastic gradient descent\n",
    "algorithm for training a three-layer network (only one hidden layer):\n",
    "\n",
    "```python\n",
    "  initialize network weights (often small random values)\\\n",
    "  **do**\\\n",
    "     **forEach** training example named ex\\\n",
    "        prediction = _neural-net-output_(network, ex)  *// forward pass*\\\n",
    "        actual = _teacher-output_(ex)\\\n",
    "        compute error (prediction - actual) at the output units\\\n",
    "          *// backward pass*\\\n",
    "           *// backward pass continued*\\\n",
    "        update network weights *// input layer not modified by error estimate*\\\n",
    "  **until** all examples classified correctly or another stopping criterion satisfied\\\n",
    "  **return** the network\n",
    "```\n",
    "\n",
    "The lines labeled \\\"backward pass\\\" can be implemented using the\n",
    "backpropagation algorithm, which calculates the gradient of the error of\n",
    "the network regarding the network\\'s modifiable weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many input neurons?\n",
    "\n",
    "What is the input layer for the MNIST data?  Each image is $28x28 = 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 10K 28×28 sized test images\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multi-layer perceptron model we must reduce the images down into a a single input layer as a vector of pixels. In this case the 28×28 sized images will be 784 pixel input values (28x28=784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values are gray scale between 0 and 255. Scaling of input values when using neural network models is a good idea. Neural network models propagate values and the rate of propagation can be effected by their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the data between 0 and 1\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network\n",
    "\n",
    "\n",
    "A shallow neural network has few layers (just one dense layer in this case). Dense means every neuron connected to every other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.72549021,  0.62352943,\n",
       "        0.59215689,  0.23529412,  0.14117648,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.87058824,  0.99607843,  0.99607843,  0.99607843,  0.99607843,\n",
       "        0.94509804,  0.7764706 ,  0.7764706 ,  0.7764706 ,  0.7764706 ,\n",
       "        0.7764706 ,  0.7764706 ,  0.7764706 ,  0.7764706 ,  0.66666669,\n",
       "        0.20392157,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.26274511,  0.44705883,\n",
       "        0.28235295,  0.44705883,  0.63921571,  0.89019608,  0.99607843,\n",
       "        0.88235295,  0.99607843,  0.99607843,  0.99607843,  0.98039216,\n",
       "        0.89803922,  0.99607843,  0.99607843,  0.54901963,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.06666667,  0.25882354,  0.05490196,  0.26274511,\n",
       "        0.26274511,  0.26274511,  0.23137255,  0.08235294,  0.9254902 ,\n",
       "        0.99607843,  0.41568628,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32549021,  0.99215686,  0.81960785,  0.07058824,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.08627451,  0.9137255 ,\n",
       "        1.        ,  0.32549021,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.50588238,  0.99607843,  0.93333334,  0.17254902,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.23137255,  0.97647059,\n",
       "        0.99607843,  0.24313726,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.52156866,  0.99607843,  0.73333335,  0.01960784,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.03529412,  0.80392158,\n",
       "        0.97254902,  0.22745098,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.49411765,  0.99607843,  0.71372551,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.29411766,  0.98431373,\n",
       "        0.94117647,  0.22352941,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.07450981,  0.86666667,  0.99607843,  0.65098041,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01176471,  0.79607844,  0.99607843,\n",
       "        0.85882354,  0.13725491,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.99607843,  0.99607843,  0.3019608 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.12156863,  0.87843138,  0.99607843,\n",
       "        0.4509804 ,  0.00392157,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.52156866,  0.99607843,  0.99607843,  0.20392157,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.23921569,  0.94901961,  0.99607843,\n",
       "        0.99607843,  0.20392157,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.47450981,  0.99607843,  0.99607843,  0.85882354,  0.15686275,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.47450981,  0.99607843,\n",
       "        0.81176472,  0.07058824,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. As such, it is good practice to use a one hot encoding of the class values, transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "We can easily do this using the built-in np_utils.to_categorical() helper function in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_A(n=55,i=784,o=10):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 784, output 10\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='sigmoid', input_shape=(i,)))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=shallow_net_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 55)                43175     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                560       \n",
      "=================================================================\n",
      "Total params: 43,735\n",
      "Trainable params: 43,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0943 - acc: 0.0993 - val_loss: 0.0932 - val_acc: 0.1032\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0927 - acc: 0.0984 - val_loss: 0.0922 - val_acc: 0.0985\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0920 - acc: 0.0984 - val_loss: 0.0916 - val_acc: 0.1011\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0914 - acc: 0.1155 - val_loss: 0.0911 - val_acc: 0.1264\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0909 - acc: 0.1447 - val_loss: 0.0907 - val_acc: 0.1545\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0905 - acc: 0.1690 - val_loss: 0.0903 - val_acc: 0.1787\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0901 - acc: 0.1906 - val_loss: 0.0899 - val_acc: 0.2001\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0898 - acc: 0.2121 - val_loss: 0.0896 - val_acc: 0.2239\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0894 - acc: 0.2330 - val_loss: 0.0892 - val_acc: 0.2448\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0890 - acc: 0.2534 - val_loss: 0.0888 - val_acc: 0.2611\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0887 - acc: 0.2707 - val_loss: 0.0885 - val_acc: 0.2768\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0884 - acc: 0.2850 - val_loss: 0.0881 - val_acc: 0.2915\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0880 - acc: 0.2965 - val_loss: 0.0878 - val_acc: 0.3022\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0877 - acc: 0.3064 - val_loss: 0.0875 - val_acc: 0.3119\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0873 - acc: 0.3137 - val_loss: 0.0871 - val_acc: 0.3204\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0870 - acc: 0.3232 - val_loss: 0.0868 - val_acc: 0.3300\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0867 - acc: 0.3309 - val_loss: 0.0864 - val_acc: 0.3370\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0863 - acc: 0.3379 - val_loss: 0.0861 - val_acc: 0.3431\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0860 - acc: 0.3438 - val_loss: 0.0857 - val_acc: 0.3479\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0856 - acc: 0.3500 - val_loss: 0.0854 - val_acc: 0.3552\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0853 - acc: 0.3551 - val_loss: 0.0850 - val_acc: 0.3597\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0849 - acc: 0.3593 - val_loss: 0.0847 - val_acc: 0.3642\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0846 - acc: 0.3639 - val_loss: 0.0843 - val_acc: 0.3699\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0842 - acc: 0.3689 - val_loss: 0.0839 - val_acc: 0.3746\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0839 - acc: 0.3733 - val_loss: 0.0836 - val_acc: 0.3790\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0835 - acc: 0.3801 - val_loss: 0.0832 - val_acc: 0.3843\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0831 - acc: 0.3853 - val_loss: 0.0828 - val_acc: 0.3897\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0827 - acc: 0.3910 - val_loss: 0.0824 - val_acc: 0.3968\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0823 - acc: 0.3987 - val_loss: 0.0820 - val_acc: 0.4045\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0819 - acc: 0.4060 - val_loss: 0.0816 - val_acc: 0.4113\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0815 - acc: 0.4127 - val_loss: 0.0811 - val_acc: 0.4189\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0811 - acc: 0.4193 - val_loss: 0.0807 - val_acc: 0.4249\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0807 - acc: 0.4261 - val_loss: 0.0803 - val_acc: 0.4330\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0803 - acc: 0.4326 - val_loss: 0.0799 - val_acc: 0.4392\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0798 - acc: 0.4388 - val_loss: 0.0794 - val_acc: 0.4450\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0794 - acc: 0.4454 - val_loss: 0.0790 - val_acc: 0.4509\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0789 - acc: 0.4510 - val_loss: 0.0785 - val_acc: 0.4580\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0785 - acc: 0.4570 - val_loss: 0.0781 - val_acc: 0.4633\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0780 - acc: 0.4619 - val_loss: 0.0776 - val_acc: 0.4680\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0776 - acc: 0.4671 - val_loss: 0.0771 - val_acc: 0.4735\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0771 - acc: 0.4727 - val_loss: 0.0767 - val_acc: 0.4795\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0767 - acc: 0.4785 - val_loss: 0.0762 - val_acc: 0.4847\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0762 - acc: 0.4833 - val_loss: 0.0757 - val_acc: 0.4888\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0758 - acc: 0.4880 - val_loss: 0.0752 - val_acc: 0.4934\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0753 - acc: 0.4932 - val_loss: 0.0748 - val_acc: 0.4980\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0748 - acc: 0.4972 - val_loss: 0.0743 - val_acc: 0.5021\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0744 - acc: 0.5029 - val_loss: 0.0738 - val_acc: 0.5059\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0739 - acc: 0.5077 - val_loss: 0.0733 - val_acc: 0.5106\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0734 - acc: 0.5124 - val_loss: 0.0729 - val_acc: 0.5144\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0730 - acc: 0.5175 - val_loss: 0.0724 - val_acc: 0.5197\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0725 - acc: 0.5213 - val_loss: 0.0719 - val_acc: 0.5245\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0720 - acc: 0.5255 - val_loss: 0.0714 - val_acc: 0.5291\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0716 - acc: 0.5306 - val_loss: 0.0710 - val_acc: 0.5340\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0711 - acc: 0.5351 - val_loss: 0.0705 - val_acc: 0.5387\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0706 - acc: 0.5397 - val_loss: 0.0700 - val_acc: 0.5434\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0702 - acc: 0.5443 - val_loss: 0.0696 - val_acc: 0.5477\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0697 - acc: 0.5483 - val_loss: 0.0691 - val_acc: 0.5536\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0692 - acc: 0.5538 - val_loss: 0.0686 - val_acc: 0.5575\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0688 - acc: 0.5587 - val_loss: 0.0681 - val_acc: 0.5627\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0683 - acc: 0.5640 - val_loss: 0.0677 - val_acc: 0.5675\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0679 - acc: 0.5689 - val_loss: 0.0672 - val_acc: 0.5714\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0674 - acc: 0.5742 - val_loss: 0.0668 - val_acc: 0.5772\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0670 - acc: 0.5795 - val_loss: 0.0663 - val_acc: 0.5818\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s - loss: 0.0665 - acc: 0.5849 - val_loss: 0.0658 - val_acc: 0.5864\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0661 - acc: 0.5898 - val_loss: 0.0654 - val_acc: 0.5922\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0656 - acc: 0.5940 - val_loss: 0.0649 - val_acc: 0.5957\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0652 - acc: 0.5991 - val_loss: 0.0645 - val_acc: 0.6008\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0647 - acc: 0.6039 - val_loss: 0.0640 - val_acc: 0.6062\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0643 - acc: 0.6085 - val_loss: 0.0636 - val_acc: 0.6107\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0638 - acc: 0.6137 - val_loss: 0.0631 - val_acc: 0.6152\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0634 - acc: 0.6177 - val_loss: 0.0627 - val_acc: 0.6200\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0629 - acc: 0.6223 - val_loss: 0.0622 - val_acc: 0.6244\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0625 - acc: 0.6260 - val_loss: 0.0618 - val_acc: 0.6276\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0621 - acc: 0.6306 - val_loss: 0.0613 - val_acc: 0.6324\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0616 - acc: 0.6351 - val_loss: 0.0609 - val_acc: 0.6367\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0612 - acc: 0.6390 - val_loss: 0.0605 - val_acc: 0.6422\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0608 - acc: 0.6435 - val_loss: 0.0600 - val_acc: 0.6458\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0603 - acc: 0.6468 - val_loss: 0.0596 - val_acc: 0.6505\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0599 - acc: 0.6506 - val_loss: 0.0592 - val_acc: 0.6534\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0595 - acc: 0.6537 - val_loss: 0.0587 - val_acc: 0.6576\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0591 - acc: 0.6569 - val_loss: 0.0583 - val_acc: 0.6624\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0587 - acc: 0.6606 - val_loss: 0.0579 - val_acc: 0.6652\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0582 - acc: 0.6639 - val_loss: 0.0575 - val_acc: 0.6691\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0578 - acc: 0.6669 - val_loss: 0.0571 - val_acc: 0.6724\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0574 - acc: 0.6707 - val_loss: 0.0566 - val_acc: 0.6768\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0570 - acc: 0.6740 - val_loss: 0.0562 - val_acc: 0.6804\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0566 - acc: 0.6764 - val_loss: 0.0558 - val_acc: 0.6840\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0562 - acc: 0.6801 - val_loss: 0.0554 - val_acc: 0.6877\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0558 - acc: 0.6832 - val_loss: 0.0550 - val_acc: 0.6912\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0554 - acc: 0.6853 - val_loss: 0.0546 - val_acc: 0.6941\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0550 - acc: 0.6883 - val_loss: 0.0542 - val_acc: 0.6977\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0546 - acc: 0.6904 - val_loss: 0.0538 - val_acc: 0.7018\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0542 - acc: 0.6930 - val_loss: 0.0534 - val_acc: 0.7040\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0539 - acc: 0.6957 - val_loss: 0.0531 - val_acc: 0.7073\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0535 - acc: 0.6986 - val_loss: 0.0527 - val_acc: 0.7105\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0531 - acc: 0.7017 - val_loss: 0.0523 - val_acc: 0.7124\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0527 - acc: 0.7038 - val_loss: 0.0519 - val_acc: 0.7155\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0524 - acc: 0.7056 - val_loss: 0.0516 - val_acc: 0.7179\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0520 - acc: 0.7085 - val_loss: 0.0512 - val_acc: 0.7199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1226252b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8608/10000 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.051187293976545332, 0.71989999999999998]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 68% accuracy after 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0516 - acc: 0.7109 - val_loss: 0.0508 - val_acc: 0.7233\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0513 - acc: 0.7132 - val_loss: 0.0505 - val_acc: 0.7261\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0509 - acc: 0.7160 - val_loss: 0.0501 - val_acc: 0.7283\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0506 - acc: 0.7184 - val_loss: 0.0498 - val_acc: 0.7298\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0502 - acc: 0.7204 - val_loss: 0.0494 - val_acc: 0.7319\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0499 - acc: 0.7231 - val_loss: 0.0491 - val_acc: 0.7344\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0495 - acc: 0.7254 - val_loss: 0.0487 - val_acc: 0.7378\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0492 - acc: 0.7279 - val_loss: 0.0484 - val_acc: 0.7405\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0489 - acc: 0.7300 - val_loss: 0.0480 - val_acc: 0.7425\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0485 - acc: 0.7323 - val_loss: 0.0477 - val_acc: 0.7451\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0482 - acc: 0.7345 - val_loss: 0.0474 - val_acc: 0.7463\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0479 - acc: 0.7366 - val_loss: 0.0471 - val_acc: 0.7486\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0476 - acc: 0.7384 - val_loss: 0.0467 - val_acc: 0.7508\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0472 - acc: 0.7409 - val_loss: 0.0464 - val_acc: 0.7532\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0469 - acc: 0.7432 - val_loss: 0.0461 - val_acc: 0.7562\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0466 - acc: 0.7453 - val_loss: 0.0458 - val_acc: 0.7585\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0463 - acc: 0.7474 - val_loss: 0.0455 - val_acc: 0.7606\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0460 - acc: 0.7494 - val_loss: 0.0452 - val_acc: 0.7631\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0457 - acc: 0.7514 - val_loss: 0.0449 - val_acc: 0.7658\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0454 - acc: 0.7537 - val_loss: 0.0446 - val_acc: 0.7679\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0451 - acc: 0.7563 - val_loss: 0.0443 - val_acc: 0.7707\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0448 - acc: 0.7580 - val_loss: 0.0440 - val_acc: 0.7725\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0446 - acc: 0.7601 - val_loss: 0.0437 - val_acc: 0.7754\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0443 - acc: 0.7623 - val_loss: 0.0434 - val_acc: 0.7774\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0440 - acc: 0.7643 - val_loss: 0.0431 - val_acc: 0.7803\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0437 - acc: 0.7671 - val_loss: 0.0429 - val_acc: 0.7818\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0434 - acc: 0.7688 - val_loss: 0.0426 - val_acc: 0.7835\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0432 - acc: 0.7716 - val_loss: 0.0423 - val_acc: 0.7852\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0429 - acc: 0.7739 - val_loss: 0.0420 - val_acc: 0.7872\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0426 - acc: 0.7758 - val_loss: 0.0418 - val_acc: 0.7900\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0424 - acc: 0.7776 - val_loss: 0.0415 - val_acc: 0.7920\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0421 - acc: 0.7802 - val_loss: 0.0412 - val_acc: 0.7942\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0419 - acc: 0.7823 - val_loss: 0.0410 - val_acc: 0.7957\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0416 - acc: 0.7845 - val_loss: 0.0407 - val_acc: 0.7980\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0413 - acc: 0.7866 - val_loss: 0.0405 - val_acc: 0.7995\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0411 - acc: 0.7880 - val_loss: 0.0402 - val_acc: 0.8014\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0409 - acc: 0.7900 - val_loss: 0.0400 - val_acc: 0.8042\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0406 - acc: 0.7918 - val_loss: 0.0397 - val_acc: 0.8059\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0404 - acc: 0.7934 - val_loss: 0.0395 - val_acc: 0.8083\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0401 - acc: 0.7952 - val_loss: 0.0392 - val_acc: 0.8097\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0399 - acc: 0.7970 - val_loss: 0.0390 - val_acc: 0.8113\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0397 - acc: 0.7983 - val_loss: 0.0388 - val_acc: 0.8129\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0394 - acc: 0.8000 - val_loss: 0.0385 - val_acc: 0.8147\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0392 - acc: 0.8018 - val_loss: 0.0383 - val_acc: 0.8162\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0390 - acc: 0.8034 - val_loss: 0.0381 - val_acc: 0.8176\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0387 - acc: 0.8049 - val_loss: 0.0379 - val_acc: 0.8192\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0385 - acc: 0.8064 - val_loss: 0.0376 - val_acc: 0.8209\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0383 - acc: 0.8080 - val_loss: 0.0374 - val_acc: 0.8226\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0381 - acc: 0.8096 - val_loss: 0.0372 - val_acc: 0.8241\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0379 - acc: 0.8112 - val_loss: 0.0370 - val_acc: 0.8249\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0377 - acc: 0.8126 - val_loss: 0.0368 - val_acc: 0.8266\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0375 - acc: 0.8137 - val_loss: 0.0365 - val_acc: 0.8274\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0372 - acc: 0.8151 - val_loss: 0.0363 - val_acc: 0.8282\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0370 - acc: 0.8162 - val_loss: 0.0361 - val_acc: 0.8293\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0368 - acc: 0.8176 - val_loss: 0.0359 - val_acc: 0.8300\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0366 - acc: 0.8185 - val_loss: 0.0357 - val_acc: 0.8309\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0364 - acc: 0.8199 - val_loss: 0.0355 - val_acc: 0.8321\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0362 - acc: 0.8209 - val_loss: 0.0353 - val_acc: 0.8331\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0361 - acc: 0.8224 - val_loss: 0.0351 - val_acc: 0.8340\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0359 - acc: 0.8234 - val_loss: 0.0349 - val_acc: 0.8351\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0357 - acc: 0.8246 - val_loss: 0.0347 - val_acc: 0.8360\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0355 - acc: 0.8258 - val_loss: 0.0346 - val_acc: 0.8374\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0353 - acc: 0.8269 - val_loss: 0.0344 - val_acc: 0.8379\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s - loss: 0.0351 - acc: 0.8283 - val_loss: 0.0342 - val_acc: 0.8391\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0349 - acc: 0.8294 - val_loss: 0.0340 - val_acc: 0.8396\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0348 - acc: 0.8302 - val_loss: 0.0338 - val_acc: 0.8402\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0346 - acc: 0.8309 - val_loss: 0.0336 - val_acc: 0.8414\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0344 - acc: 0.8320 - val_loss: 0.0335 - val_acc: 0.8428\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0342 - acc: 0.8328 - val_loss: 0.0333 - val_acc: 0.8435\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0341 - acc: 0.8337 - val_loss: 0.0331 - val_acc: 0.8442\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0339 - acc: 0.8346 - val_loss: 0.0329 - val_acc: 0.8450\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0337 - acc: 0.8357 - val_loss: 0.0328 - val_acc: 0.8457\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0335 - acc: 0.8362 - val_loss: 0.0326 - val_acc: 0.8477\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0334 - acc: 0.8370 - val_loss: 0.0324 - val_acc: 0.8486\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0332 - acc: 0.8378 - val_loss: 0.0323 - val_acc: 0.8490\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0331 - acc: 0.8385 - val_loss: 0.0321 - val_acc: 0.8498\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0329 - acc: 0.8388 - val_loss: 0.0320 - val_acc: 0.8503\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0327 - acc: 0.8398 - val_loss: 0.0318 - val_acc: 0.8505\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0326 - acc: 0.8404 - val_loss: 0.0316 - val_acc: 0.8506\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0324 - acc: 0.8413 - val_loss: 0.0315 - val_acc: 0.8509\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0323 - acc: 0.8416 - val_loss: 0.0313 - val_acc: 0.8516\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0321 - acc: 0.8425 - val_loss: 0.0312 - val_acc: 0.8522\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0320 - acc: 0.8433 - val_loss: 0.0310 - val_acc: 0.8524\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0318 - acc: 0.8439 - val_loss: 0.0309 - val_acc: 0.8528\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0317 - acc: 0.8444 - val_loss: 0.0307 - val_acc: 0.8534\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0315 - acc: 0.8453 - val_loss: 0.0306 - val_acc: 0.8540\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0314 - acc: 0.8457 - val_loss: 0.0305 - val_acc: 0.8547\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0313 - acc: 0.8465 - val_loss: 0.0303 - val_acc: 0.8556\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0311 - acc: 0.8471 - val_loss: 0.0302 - val_acc: 0.8562\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0310 - acc: 0.8475 - val_loss: 0.0300 - val_acc: 0.8568\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0309 - acc: 0.8481 - val_loss: 0.0299 - val_acc: 0.8572\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0307 - acc: 0.8484 - val_loss: 0.0298 - val_acc: 0.8579\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0306 - acc: 0.8487 - val_loss: 0.0296 - val_acc: 0.8581\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0305 - acc: 0.8492 - val_loss: 0.0295 - val_acc: 0.8585\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0303 - acc: 0.8498 - val_loss: 0.0294 - val_acc: 0.8594\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0302 - acc: 0.8503 - val_loss: 0.0292 - val_acc: 0.8599\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0301 - acc: 0.8511 - val_loss: 0.0291 - val_acc: 0.8607\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0299 - acc: 0.8512 - val_loss: 0.0290 - val_acc: 0.8612\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0298 - acc: 0.8519 - val_loss: 0.0289 - val_acc: 0.8616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12dda54a8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9024/10000 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028857796752452852, 0.86160000000000003]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 86% accuracy after another 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0297 - acc: 0.8519 - val_loss: 0.0287 - val_acc: 0.8619\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0296 - acc: 0.8528 - val_loss: 0.0286 - val_acc: 0.8625\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0295 - acc: 0.8531 - val_loss: 0.0285 - val_acc: 0.8628\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0293 - acc: 0.8538 - val_loss: 0.0284 - val_acc: 0.8636\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0292 - acc: 0.8543 - val_loss: 0.0283 - val_acc: 0.8640\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0291 - acc: 0.8548 - val_loss: 0.0281 - val_acc: 0.8646\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0290 - acc: 0.8550 - val_loss: 0.0280 - val_acc: 0.8651\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0289 - acc: 0.8557 - val_loss: 0.0279 - val_acc: 0.8655\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0288 - acc: 0.8562 - val_loss: 0.0278 - val_acc: 0.8656\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0287 - acc: 0.8565 - val_loss: 0.0277 - val_acc: 0.8660\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0285 - acc: 0.8568 - val_loss: 0.0276 - val_acc: 0.8663\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0284 - acc: 0.8572 - val_loss: 0.0275 - val_acc: 0.8665\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0283 - acc: 0.8577 - val_loss: 0.0274 - val_acc: 0.8672\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0282 - acc: 0.8580 - val_loss: 0.0273 - val_acc: 0.8673\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0281 - acc: 0.8583 - val_loss: 0.0271 - val_acc: 0.8676\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0280 - acc: 0.8588 - val_loss: 0.0270 - val_acc: 0.8679\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0279 - acc: 0.8593 - val_loss: 0.0269 - val_acc: 0.8681\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0278 - acc: 0.8596 - val_loss: 0.0268 - val_acc: 0.8687\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0277 - acc: 0.8599 - val_loss: 0.0267 - val_acc: 0.8691\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0276 - acc: 0.8602 - val_loss: 0.0266 - val_acc: 0.8695\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0275 - acc: 0.8606 - val_loss: 0.0265 - val_acc: 0.8697\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0274 - acc: 0.8610 - val_loss: 0.0264 - val_acc: 0.8701\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0273 - acc: 0.8612 - val_loss: 0.0263 - val_acc: 0.8702\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0272 - acc: 0.8615 - val_loss: 0.0263 - val_acc: 0.8709\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0271 - acc: 0.8618 - val_loss: 0.0262 - val_acc: 0.8709\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0270 - acc: 0.8621 - val_loss: 0.0261 - val_acc: 0.8713\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0270 - acc: 0.8623 - val_loss: 0.0260 - val_acc: 0.8716\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0269 - acc: 0.8625 - val_loss: 0.0259 - val_acc: 0.8721\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0268 - acc: 0.8628 - val_loss: 0.0258 - val_acc: 0.8723\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0267 - acc: 0.8630 - val_loss: 0.0257 - val_acc: 0.8727\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0266 - acc: 0.8633 - val_loss: 0.0256 - val_acc: 0.8726\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0265 - acc: 0.8636 - val_loss: 0.0255 - val_acc: 0.8727\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0264 - acc: 0.8638 - val_loss: 0.0254 - val_acc: 0.8728\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0263 - acc: 0.8640 - val_loss: 0.0254 - val_acc: 0.8733\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0262 - acc: 0.8643 - val_loss: 0.0253 - val_acc: 0.8734\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0262 - acc: 0.8645 - val_loss: 0.0252 - val_acc: 0.8736\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0261 - acc: 0.8649 - val_loss: 0.0251 - val_acc: 0.8739\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0260 - acc: 0.8653 - val_loss: 0.0250 - val_acc: 0.8745\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0259 - acc: 0.8655 - val_loss: 0.0249 - val_acc: 0.8749\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0258 - acc: 0.8660 - val_loss: 0.0249 - val_acc: 0.8750\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0258 - acc: 0.8661 - val_loss: 0.0248 - val_acc: 0.8751\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0257 - acc: 0.8664 - val_loss: 0.0247 - val_acc: 0.8754\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0256 - acc: 0.8667 - val_loss: 0.0246 - val_acc: 0.8758\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0255 - acc: 0.8671 - val_loss: 0.0246 - val_acc: 0.8760\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0255 - acc: 0.8674 - val_loss: 0.0245 - val_acc: 0.8759\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0254 - acc: 0.8676 - val_loss: 0.0244 - val_acc: 0.8764\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0253 - acc: 0.8678 - val_loss: 0.0243 - val_acc: 0.8765\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0252 - acc: 0.8680 - val_loss: 0.0243 - val_acc: 0.8766\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0252 - acc: 0.8684 - val_loss: 0.0242 - val_acc: 0.8770\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0251 - acc: 0.8687 - val_loss: 0.0241 - val_acc: 0.8773\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0250 - acc: 0.8689 - val_loss: 0.0240 - val_acc: 0.8772\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0249 - acc: 0.8693 - val_loss: 0.0240 - val_acc: 0.8776\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0249 - acc: 0.8696 - val_loss: 0.0239 - val_acc: 0.8778\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0248 - acc: 0.8696 - val_loss: 0.0238 - val_acc: 0.8780\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0247 - acc: 0.8701 - val_loss: 0.0238 - val_acc: 0.8782\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0247 - acc: 0.8702 - val_loss: 0.0237 - val_acc: 0.8783\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0246 - acc: 0.8705 - val_loss: 0.0236 - val_acc: 0.8784\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0245 - acc: 0.8707 - val_loss: 0.0236 - val_acc: 0.8786\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0245 - acc: 0.8710 - val_loss: 0.0235 - val_acc: 0.8788\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0244 - acc: 0.8713 - val_loss: 0.0234 - val_acc: 0.8787\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0243 - acc: 0.8716 - val_loss: 0.0234 - val_acc: 0.8788\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0243 - acc: 0.8716 - val_loss: 0.0233 - val_acc: 0.8790\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0242 - acc: 0.8720 - val_loss: 0.0232 - val_acc: 0.8794\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s - loss: 0.0241 - acc: 0.8722 - val_loss: 0.0232 - val_acc: 0.8800\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0241 - acc: 0.8724 - val_loss: 0.0231 - val_acc: 0.8802\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0240 - acc: 0.8726 - val_loss: 0.0230 - val_acc: 0.8803\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0240 - acc: 0.8728 - val_loss: 0.0230 - val_acc: 0.8803\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0239 - acc: 0.8730 - val_loss: 0.0229 - val_acc: 0.8802\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0238 - acc: 0.8733 - val_loss: 0.0229 - val_acc: 0.8803\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0238 - acc: 0.8735 - val_loss: 0.0228 - val_acc: 0.8807\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0237 - acc: 0.8738 - val_loss: 0.0227 - val_acc: 0.8808\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0237 - acc: 0.8739 - val_loss: 0.0227 - val_acc: 0.8809\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0236 - acc: 0.8742 - val_loss: 0.0226 - val_acc: 0.8810\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0235 - acc: 0.8743 - val_loss: 0.0226 - val_acc: 0.8815\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0235 - acc: 0.8745 - val_loss: 0.0225 - val_acc: 0.8815\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0234 - acc: 0.8747 - val_loss: 0.0225 - val_acc: 0.8818\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0234 - acc: 0.8751 - val_loss: 0.0224 - val_acc: 0.8819\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0233 - acc: 0.8752 - val_loss: 0.0223 - val_acc: 0.8822\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0233 - acc: 0.8754 - val_loss: 0.0223 - val_acc: 0.8826\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0232 - acc: 0.8755 - val_loss: 0.0222 - val_acc: 0.8828\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0231 - acc: 0.8758 - val_loss: 0.0222 - val_acc: 0.8830\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0231 - acc: 0.8759 - val_loss: 0.0221 - val_acc: 0.8834\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0230 - acc: 0.8762 - val_loss: 0.0221 - val_acc: 0.8833\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0230 - acc: 0.8764 - val_loss: 0.0220 - val_acc: 0.8837\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0229 - acc: 0.8765 - val_loss: 0.0220 - val_acc: 0.8837\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0229 - acc: 0.8769 - val_loss: 0.0219 - val_acc: 0.8837\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0228 - acc: 0.8771 - val_loss: 0.0219 - val_acc: 0.8838\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0228 - acc: 0.8772 - val_loss: 0.0218 - val_acc: 0.8844\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0227 - acc: 0.8773 - val_loss: 0.0218 - val_acc: 0.8844\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0227 - acc: 0.8775 - val_loss: 0.0217 - val_acc: 0.8848\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0226 - acc: 0.8776 - val_loss: 0.0217 - val_acc: 0.8854\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0226 - acc: 0.8778 - val_loss: 0.0216 - val_acc: 0.8855\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0225 - acc: 0.8780 - val_loss: 0.0216 - val_acc: 0.8858\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0225 - acc: 0.8780 - val_loss: 0.0215 - val_acc: 0.8860\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0224 - acc: 0.8780 - val_loss: 0.0215 - val_acc: 0.8862\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0224 - acc: 0.8783 - val_loss: 0.0214 - val_acc: 0.8865\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0223 - acc: 0.8784 - val_loss: 0.0214 - val_acc: 0.8869\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0223 - acc: 0.8785 - val_loss: 0.0213 - val_acc: 0.8869\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0223 - acc: 0.8786 - val_loss: 0.0213 - val_acc: 0.8868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ddb3ef0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9696/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.021296201345324516, 0.88680000000000003]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 88% accuracy after another 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "In computational networks, the [activation function](https://en.wikipedia.org/wiki/Activation_function) of a node\n",
    "defines the output of that node given an input or set of inputs. A\n",
    "standard computer chip circuit can be seen as a digital network of\n",
    "activation functions that can be “ON” (1) or “OFF” (0), depending on\n",
    "input. This is similar to the behavior of the linear perceptron in\n",
    "neural networks. However, only *nonlinear* activation functions allow\n",
    "such networks to compute nontrivial problems using only a small number\n",
    "of nodes. In artificial neural networks this function is also called\n",
    "the **transfer function**.\n",
    "\n",
    "Functions\n",
    "---------\n",
    "\n",
    "In biologically inspired neural networks, the activation function is\n",
    "usually an abstraction representing the rate of action potential\n",
    "firing in the cell. In its simplest form, this function is binary—that\n",
    "is, either the neuron is firing or not. The function looks like\n",
    "$\\phi(v_i)=U(v_i)$, where $U$ is the Heaviside step function. In this\n",
    "case many neurons must be used in computation beyond linear separation\n",
    "of categories.\n",
    "\n",
    "A line of positive slope may be used to reflect the increase in firing\n",
    "rate that occurs as input current increases. Such a function would be of\n",
    "the form $\\phi(v_i)=\\mu v_i$, where $\\mu$ is the slope. This activation\n",
    "function is linear, and therefore has the same problems as the binary\n",
    "function. In addition, networks constructed using this model have\n",
    "unstable convergence because neuron inputs along favored paths tend to\n",
    "increase without bound, as this function is not normalizable.\n",
    "\n",
    "All problems mentioned above can be handled by using a normalizable\n",
    "sigmoid activation function. One realistic model stays at zero until\n",
    "input current is received, at which point the firing frequency increases\n",
    "quickly at first, but gradually approaches an asymptote at 100% firing\n",
    "rate. Mathematically, this looks like $\\phi(v_i)=U(v_i)\\tanh(v_i)$,\n",
    "where the hyperbolic tangent function can be replaced by any sigmoid\n",
    "function. This behavior is realistically reflected in the neuron, as\n",
    "neurons cannot physically fire faster than a certain rate. This model\n",
    "runs into problems, however, in computational networks as it is not\n",
    "differentiable, a requirement to calculate backpropagation.\n",
    "\n",
    "The final model, then, that is used in multilayer perceptrons is a\n",
    "sigmoidal activation function in the form of a hyperbolic tangent. Two\n",
    "forms of this function are commonly used: $\\phi(v_i)=\\tanh(v_i)$ whose\n",
    "range is normalized from -1 to 1, and $\\phi(v_i) = (1+\\exp(-v_i))^{-1}$\n",
    "is vertically translated to normalize from 0 to 1. The latter model is\n",
    "often considered more biologically realistic, but it runs into\n",
    "theoretical and experimental difficulties with certain types.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of activation functions\n",
    "\n",
    "Some desirable properties in an activation function include:\n",
    "\n",
    "-   Nonlinear – When the activation function is non-linear, then a\n",
    "    two-layer neural network can be proven to be a universal function\n",
    "    approximator. The identity activation function does not satisfy\n",
    "    this property. When multiple layers use the identity activation\n",
    "    function, the entire network is equivalent to a single-layer model.   \n",
    "-   Continuously differentiable – This property is necessary for\n",
    "    enabling gradient-based optimization methods. The binary step\n",
    "    activation function is not differentiable at 0, and it\n",
    "    differentiates to 0 for all other values, so gradient-based methods\n",
    "    can make no progress with it.   \n",
    "-   Range – When the range of the activation function is finite,\n",
    "    gradient-based training methods tend to be more stable, because\n",
    "    pattern presentations significantly affect only limited weights.\n",
    "    When the range is infinite, training is generally more efficient\n",
    "    because pattern presentations significantly affect most of the\n",
    "    weights. In the latter case, smaller learning rates are typically\n",
    "    necessary.   \n",
    "-   Monotonic – When the activation function is monotonic, the error\n",
    "    surface associated with a single-layer model is guaranteed to be\n",
    "    convex.   \n",
    "-   Smooth Functions with a Monotonic derivative – These have been shown\n",
    "    to generalize better in some cases. The argument for these\n",
    "    properties suggests that such activation functions are more\n",
    "    consistent with Occam's razor.   \n",
    "-   Approximates identity near the origin – When activation functions\n",
    "    have this property, the neural network will learn efficiently when\n",
    "    its weights are initialized with small random values. When the\n",
    "    activation function does not approximate identity near the origin,\n",
    "    special care must be used when initializing the weights.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified linear unit (ReLU) transfer function\n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "\n",
    "Activation identity \n",
    "\n",
    "$f(x)=x$   \n",
    "$f'(x)=1$   \n",
    "$(-\\infty,\\infty)$  \n",
    "$C^\\infty$   \n",
    "\n",
    "![Identity](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_identity.svg.png )\n",
    "\n",
    "\n",
    "Logistic (a.k.a. Soft step)\n",
    "\n",
    "$f(x)=\\frac{1}{1+e^{-x}}$   \n",
    "$f'(x)=f(x)(1-f(x))$   \n",
    "$(0,1)$  \n",
    "$C^\\infty$  \n",
    "\n",
    "![Logistic](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_logistic.svg.png)  \n",
    "\n",
    "TanH\n",
    "\n",
    "$f(x)=\\tanh(x)=\\frac{2}{1+e^{-2x}}-1$   \n",
    "$f'(x)=1-f(x)^2$    \n",
    "$(-1,1)$    \n",
    "$C^\\infty$    \n",
    "\n",
    "![TanH](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_tanh.svg.png)  \n",
    "\n",
    "Rectified linear unit (ReLU)\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "    0 & \\text{for } x < 0\\\\\n",
    "    x & \\text{for } x \\ge 0\\end{cases}$   \n",
    "    \n",
    "$f'(x) = \\begin{cases}\n",
    "    0 & \\text{for } x < 0\\\\\n",
    "    1 & \\text{for } x \\ge 0\\end{cases}$    \n",
    "\n",
    "$[0,\\infty)$   \n",
    "$C^0$  \n",
    "    \n",
    "![Rectified linear unit (ReLU)](http://nikbearbrown.com/YouTube/MachineLearning/IMG/240px-Activation_rectified_linear.svg.png)\n",
    "\n",
    "\n",
    "The Rectified linear unit (ReLU) seem to work well empirically.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_B(n=55,i=784,o=10):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 784, output 10\n",
    "    # Using relu\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 55)                43175     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                560       \n",
      "=================================================================\n",
      "Total params: 43,735\n",
      "Trainable params: 43,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn2=shallow_net_B()\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0912 - acc: 0.0756 - val_loss: 0.0902 - val_acc: 0.1054\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0892 - acc: 0.1552 - val_loss: 0.0883 - val_acc: 0.2029\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0873 - acc: 0.2435 - val_loss: 0.0863 - val_acc: 0.2915\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0852 - acc: 0.3212 - val_loss: 0.0841 - val_acc: 0.3640\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0829 - acc: 0.3821 - val_loss: 0.0816 - val_acc: 0.4106\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0803 - acc: 0.4162 - val_loss: 0.0788 - val_acc: 0.4353\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0775 - acc: 0.4381 - val_loss: 0.0758 - val_acc: 0.4511\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0744 - acc: 0.4594 - val_loss: 0.0726 - val_acc: 0.4724\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0712 - acc: 0.4845 - val_loss: 0.0693 - val_acc: 0.5041\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0680 - acc: 0.5242 - val_loss: 0.0661 - val_acc: 0.5474\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0649 - acc: 0.5734 - val_loss: 0.0630 - val_acc: 0.6030\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0618 - acc: 0.6247 - val_loss: 0.0599 - val_acc: 0.6471\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0587 - acc: 0.6640 - val_loss: 0.0568 - val_acc: 0.6821\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0557 - acc: 0.6906 - val_loss: 0.0538 - val_acc: 0.7076\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0528 - acc: 0.7085 - val_loss: 0.0509 - val_acc: 0.7248\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0501 - acc: 0.7229 - val_loss: 0.0483 - val_acc: 0.7400\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0477 - acc: 0.7377 - val_loss: 0.0458 - val_acc: 0.7556\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0454 - acc: 0.7532 - val_loss: 0.0436 - val_acc: 0.7726\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0433 - acc: 0.7701 - val_loss: 0.0414 - val_acc: 0.7882\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0413 - acc: 0.7853 - val_loss: 0.0395 - val_acc: 0.8010\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0395 - acc: 0.7971 - val_loss: 0.0377 - val_acc: 0.8104\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0378 - acc: 0.8064 - val_loss: 0.0361 - val_acc: 0.8188\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0363 - acc: 0.8137 - val_loss: 0.0346 - val_acc: 0.8254\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0350 - acc: 0.8196 - val_loss: 0.0333 - val_acc: 0.8324\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0337 - acc: 0.8244 - val_loss: 0.0321 - val_acc: 0.8362\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0326 - acc: 0.8294 - val_loss: 0.0311 - val_acc: 0.8407\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0316 - acc: 0.8339 - val_loss: 0.0301 - val_acc: 0.8450\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0307 - acc: 0.8371 - val_loss: 0.0292 - val_acc: 0.8479\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0299 - acc: 0.8400 - val_loss: 0.0284 - val_acc: 0.8509\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0291 - acc: 0.8433 - val_loss: 0.0276 - val_acc: 0.8539\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0284 - acc: 0.8466 - val_loss: 0.0269 - val_acc: 0.8558\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0277 - acc: 0.8489 - val_loss: 0.0263 - val_acc: 0.8587\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0271 - acc: 0.8512 - val_loss: 0.0257 - val_acc: 0.8613\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0266 - acc: 0.8538 - val_loss: 0.0252 - val_acc: 0.8628\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0261 - acc: 0.8562 - val_loss: 0.0247 - val_acc: 0.8640\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0256 - acc: 0.8582 - val_loss: 0.0242 - val_acc: 0.8662\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0252 - acc: 0.8601 - val_loss: 0.0238 - val_acc: 0.8674\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0247 - acc: 0.8618 - val_loss: 0.0234 - val_acc: 0.8689\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0244 - acc: 0.8637 - val_loss: 0.0230 - val_acc: 0.8704\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0240 - acc: 0.8654 - val_loss: 0.0227 - val_acc: 0.8712\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0236 - acc: 0.8668 - val_loss: 0.0223 - val_acc: 0.8730\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0233 - acc: 0.8681 - val_loss: 0.0220 - val_acc: 0.8744\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0230 - acc: 0.8693 - val_loss: 0.0217 - val_acc: 0.8749\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0227 - acc: 0.8702 - val_loss: 0.0215 - val_acc: 0.8762\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0224 - acc: 0.8714 - val_loss: 0.0212 - val_acc: 0.8770\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0222 - acc: 0.8726 - val_loss: 0.0209 - val_acc: 0.8788\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0219 - acc: 0.8739 - val_loss: 0.0207 - val_acc: 0.8795\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0217 - acc: 0.8751 - val_loss: 0.0205 - val_acc: 0.8811\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0215 - acc: 0.8760 - val_loss: 0.0203 - val_acc: 0.8820\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0213 - acc: 0.8771 - val_loss: 0.0200 - val_acc: 0.8829\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0211 - acc: 0.8778 - val_loss: 0.0198 - val_acc: 0.8844\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0209 - acc: 0.8789 - val_loss: 0.0197 - val_acc: 0.8853\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0207 - acc: 0.8797 - val_loss: 0.0195 - val_acc: 0.8862\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0205 - acc: 0.8806 - val_loss: 0.0193 - val_acc: 0.8870\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0203 - acc: 0.8816 - val_loss: 0.0191 - val_acc: 0.8884\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0201 - acc: 0.8824 - val_loss: 0.0190 - val_acc: 0.8889\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0200 - acc: 0.8832 - val_loss: 0.0188 - val_acc: 0.8892\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0198 - acc: 0.8837 - val_loss: 0.0187 - val_acc: 0.8896\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0197 - acc: 0.8847 - val_loss: 0.0185 - val_acc: 0.8906\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0195 - acc: 0.8855 - val_loss: 0.0184 - val_acc: 0.8915\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0194 - acc: 0.8859 - val_loss: 0.0183 - val_acc: 0.8923\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0193 - acc: 0.8867 - val_loss: 0.0181 - val_acc: 0.8932\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0191 - acc: 0.8873 - val_loss: 0.0180 - val_acc: 0.8936\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s - loss: 0.0190 - acc: 0.8878 - val_loss: 0.0179 - val_acc: 0.8945\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0189 - acc: 0.8885 - val_loss: 0.0178 - val_acc: 0.8950\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0188 - acc: 0.8888 - val_loss: 0.0177 - val_acc: 0.8956\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0186 - acc: 0.8894 - val_loss: 0.0176 - val_acc: 0.8961\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0185 - acc: 0.8898 - val_loss: 0.0174 - val_acc: 0.8963\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0184 - acc: 0.8905 - val_loss: 0.0173 - val_acc: 0.8971\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0183 - acc: 0.8908 - val_loss: 0.0172 - val_acc: 0.8978\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0182 - acc: 0.8913 - val_loss: 0.0171 - val_acc: 0.8982\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0181 - acc: 0.8917 - val_loss: 0.0171 - val_acc: 0.8988\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0180 - acc: 0.8923 - val_loss: 0.0170 - val_acc: 0.8990\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0179 - acc: 0.8925 - val_loss: 0.0169 - val_acc: 0.8996\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0178 - acc: 0.8929 - val_loss: 0.0168 - val_acc: 0.9003\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0177 - acc: 0.8935 - val_loss: 0.0167 - val_acc: 0.9006\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0176 - acc: 0.8937 - val_loss: 0.0166 - val_acc: 0.9010\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0176 - acc: 0.8942 - val_loss: 0.0165 - val_acc: 0.9015\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0175 - acc: 0.8948 - val_loss: 0.0165 - val_acc: 0.9020\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0174 - acc: 0.8950 - val_loss: 0.0164 - val_acc: 0.9023\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0173 - acc: 0.8955 - val_loss: 0.0163 - val_acc: 0.9024\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0172 - acc: 0.8959 - val_loss: 0.0162 - val_acc: 0.9028\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0172 - acc: 0.8962 - val_loss: 0.0162 - val_acc: 0.9028\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0171 - acc: 0.8967 - val_loss: 0.0161 - val_acc: 0.9027\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0170 - acc: 0.8970 - val_loss: 0.0160 - val_acc: 0.9030\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0169 - acc: 0.8975 - val_loss: 0.0160 - val_acc: 0.9031\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0169 - acc: 0.8981 - val_loss: 0.0159 - val_acc: 0.9036\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0168 - acc: 0.8980 - val_loss: 0.0158 - val_acc: 0.9040\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0167 - acc: 0.8985 - val_loss: 0.0158 - val_acc: 0.9040\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0167 - acc: 0.8987 - val_loss: 0.0157 - val_acc: 0.9039\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0166 - acc: 0.8990 - val_loss: 0.0156 - val_acc: 0.9042\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0165 - acc: 0.8993 - val_loss: 0.0156 - val_acc: 0.9045\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0165 - acc: 0.8997 - val_loss: 0.0155 - val_acc: 0.9051\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0164 - acc: 0.9000 - val_loss: 0.0155 - val_acc: 0.9048\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0164 - acc: 0.9004 - val_loss: 0.0154 - val_acc: 0.9052\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0163 - acc: 0.9008 - val_loss: 0.0154 - val_acc: 0.9054\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0162 - acc: 0.9011 - val_loss: 0.0153 - val_acc: 0.9053\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0162 - acc: 0.9012 - val_loss: 0.0152 - val_acc: 0.9056\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0161 - acc: 0.9016 - val_loss: 0.0152 - val_acc: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e124b00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015197866915352642, 0.90559999999999996]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% accuracy after first 99 epochs with Relu\n",
    "nn2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0161 - acc: 0.9020 - val_loss: 0.0151 - val_acc: 0.9056\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0160 - acc: 0.9022 - val_loss: 0.0151 - val_acc: 0.9058\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0160 - acc: 0.9024 - val_loss: 0.0150 - val_acc: 0.9064\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0159 - acc: 0.9026 - val_loss: 0.0150 - val_acc: 0.9062\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0159 - acc: 0.9029 - val_loss: 0.0150 - val_acc: 0.9065\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0158 - acc: 0.9032 - val_loss: 0.0149 - val_acc: 0.9070\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0158 - acc: 0.9034 - val_loss: 0.0149 - val_acc: 0.9075\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0157 - acc: 0.9037 - val_loss: 0.0148 - val_acc: 0.9077\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0157 - acc: 0.9040 - val_loss: 0.0148 - val_acc: 0.9079\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0156 - acc: 0.9041 - val_loss: 0.0147 - val_acc: 0.9083\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0156 - acc: 0.9043 - val_loss: 0.0147 - val_acc: 0.9089\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0155 - acc: 0.9048 - val_loss: 0.0146 - val_acc: 0.9089\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0155 - acc: 0.9048 - val_loss: 0.0146 - val_acc: 0.9093\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0154 - acc: 0.9052 - val_loss: 0.0146 - val_acc: 0.9097\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0154 - acc: 0.9055 - val_loss: 0.0145 - val_acc: 0.9099\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0153 - acc: 0.9056 - val_loss: 0.0145 - val_acc: 0.9106\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0153 - acc: 0.9059 - val_loss: 0.0144 - val_acc: 0.9106\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0152 - acc: 0.9062 - val_loss: 0.0144 - val_acc: 0.9112\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0152 - acc: 0.9062 - val_loss: 0.0144 - val_acc: 0.9111\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0152 - acc: 0.9064 - val_loss: 0.0143 - val_acc: 0.9111\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0151 - acc: 0.9067 - val_loss: 0.0143 - val_acc: 0.9118\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0151 - acc: 0.9070 - val_loss: 0.0142 - val_acc: 0.9121\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0150 - acc: 0.9071 - val_loss: 0.0142 - val_acc: 0.9124\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0150 - acc: 0.9071 - val_loss: 0.0142 - val_acc: 0.9125\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0150 - acc: 0.9075 - val_loss: 0.0141 - val_acc: 0.9126\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0149 - acc: 0.9078 - val_loss: 0.0141 - val_acc: 0.9127\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0149 - acc: 0.9080 - val_loss: 0.0141 - val_acc: 0.9126\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0148 - acc: 0.9081 - val_loss: 0.0140 - val_acc: 0.9131\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0148 - acc: 0.9083 - val_loss: 0.0140 - val_acc: 0.9128\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0148 - acc: 0.9085 - val_loss: 0.0140 - val_acc: 0.9130\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0147 - acc: 0.9088 - val_loss: 0.0139 - val_acc: 0.9132\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0147 - acc: 0.9089 - val_loss: 0.0139 - val_acc: 0.9136\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0147 - acc: 0.9091 - val_loss: 0.0139 - val_acc: 0.9139\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0146 - acc: 0.9094 - val_loss: 0.0138 - val_acc: 0.9143\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0146 - acc: 0.9098 - val_loss: 0.0138 - val_acc: 0.9141\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0146 - acc: 0.9097 - val_loss: 0.0138 - val_acc: 0.9145\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0145 - acc: 0.9101 - val_loss: 0.0137 - val_acc: 0.9142\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0145 - acc: 0.9102 - val_loss: 0.0137 - val_acc: 0.9147\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0145 - acc: 0.9102 - val_loss: 0.0137 - val_acc: 0.9146\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9104 - val_loss: 0.0137 - val_acc: 0.9147\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9106 - val_loss: 0.0136 - val_acc: 0.9148\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0144 - acc: 0.9110 - val_loss: 0.0136 - val_acc: 0.9149\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0143 - acc: 0.9111 - val_loss: 0.0136 - val_acc: 0.9149\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0143 - acc: 0.9113 - val_loss: 0.0135 - val_acc: 0.9148\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0143 - acc: 0.9115 - val_loss: 0.0135 - val_acc: 0.9151\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9115 - val_loss: 0.0135 - val_acc: 0.9155\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9117 - val_loss: 0.0135 - val_acc: 0.9156\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0142 - acc: 0.9120 - val_loss: 0.0134 - val_acc: 0.9162\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9122 - val_loss: 0.0134 - val_acc: 0.9160\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9123 - val_loss: 0.0134 - val_acc: 0.9163\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9122 - val_loss: 0.0134 - val_acc: 0.9165\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0141 - acc: 0.9126 - val_loss: 0.0133 - val_acc: 0.9166\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9126 - val_loss: 0.0133 - val_acc: 0.9166\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9126 - val_loss: 0.0133 - val_acc: 0.9171\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0140 - acc: 0.9130 - val_loss: 0.0133 - val_acc: 0.9170\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9132 - val_loss: 0.0132 - val_acc: 0.9177\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9134 - val_loss: 0.0132 - val_acc: 0.9176\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9133 - val_loss: 0.0132 - val_acc: 0.9179\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0139 - acc: 0.9137 - val_loss: 0.0132 - val_acc: 0.9177\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9138 - val_loss: 0.0131 - val_acc: 0.9178\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9138 - val_loss: 0.0131 - val_acc: 0.9181\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0138 - acc: 0.9140 - val_loss: 0.0131 - val_acc: 0.9183\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9142 - val_loss: 0.0131 - val_acc: 0.9186\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9142 - val_loss: 0.0130 - val_acc: 0.9185\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9142 - val_loss: 0.0130 - val_acc: 0.9189\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0137 - acc: 0.9146 - val_loss: 0.0130 - val_acc: 0.9190\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9147 - val_loss: 0.0130 - val_acc: 0.9193\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9150 - val_loss: 0.0129 - val_acc: 0.9192\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9151 - val_loss: 0.0129 - val_acc: 0.9191\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0136 - acc: 0.9151 - val_loss: 0.0129 - val_acc: 0.9192\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9155 - val_loss: 0.0129 - val_acc: 0.9192\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9156 - val_loss: 0.0129 - val_acc: 0.9191\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9156 - val_loss: 0.0128 - val_acc: 0.9193\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0135 - acc: 0.9159 - val_loss: 0.0128 - val_acc: 0.9192\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9159 - val_loss: 0.0128 - val_acc: 0.9191\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9162 - val_loss: 0.0128 - val_acc: 0.9196\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9161 - val_loss: 0.0128 - val_acc: 0.9199\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0134 - acc: 0.9163 - val_loss: 0.0127 - val_acc: 0.9199\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9165 - val_loss: 0.0127 - val_acc: 0.9201\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9166 - val_loss: 0.0127 - val_acc: 0.9202\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9167 - val_loss: 0.0127 - val_acc: 0.9201\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9169 - val_loss: 0.0127 - val_acc: 0.9205\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0133 - acc: 0.9169 - val_loss: 0.0126 - val_acc: 0.9205\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9171 - val_loss: 0.0126 - val_acc: 0.9205\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9172 - val_loss: 0.0126 - val_acc: 0.9206\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9174 - val_loss: 0.0126 - val_acc: 0.9208\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0132 - acc: 0.9176 - val_loss: 0.0126 - val_acc: 0.9209\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9177 - val_loss: 0.0125 - val_acc: 0.9208\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9179 - val_loss: 0.0125 - val_acc: 0.9210\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9181 - val_loss: 0.0125 - val_acc: 0.9211\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9182 - val_loss: 0.0125 - val_acc: 0.9211\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0131 - acc: 0.9184 - val_loss: 0.0125 - val_acc: 0.9211\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0130 - acc: 0.9182 - val_loss: 0.0124 - val_acc: 0.9214\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0130 - acc: 0.9185 - val_loss: 0.0124 - val_acc: 0.9218\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0130 - acc: 0.9186 - val_loss: 0.0124 - val_acc: 0.9217\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0130 - acc: 0.9187 - val_loss: 0.0124 - val_acc: 0.9219\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0130 - acc: 0.9189 - val_loss: 0.0124 - val_acc: 0.9221\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0129 - acc: 0.9191 - val_loss: 0.0124 - val_acc: 0.9218\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.0129 - acc: 0.9192 - val_loss: 0.0123 - val_acc: 0.9220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122654668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7552/10000 [=====================>........] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.012338483134144916, 0.92200000000000004]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 92% accuracy after another 99 epochs with Relu\n",
    "# Seems to be a plateau\n",
    "nn2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss or cost functions\n",
    "\n",
    "Loss function\n",
    "-------------\n",
    "\n",
    "Sometimes referred to as the **cost function** or **error function**\n",
    "(not to be confused with the Gauss error function), the loss function\n",
    "is a function that maps values of one or more variables onto a real\n",
    "number intuitively representing some “cost” associated with those\n",
    "values. For backpropagation, the loss function calculates the difference\n",
    "between the network output and its expected output, after a case\n",
    "propagates through the network.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "Two assumptions must be made about the form of the error function.\n",
    "\n",
    "The first is that it can be written as an average\n",
    "$E=\\frac{1}{n}\\sum_xE_x$ over error functions $E_x$, for individual\n",
    "training examples, $x$. The reason for this assumption is that the\n",
    "backpropagation algorithm calculates the gradient of the error function\n",
    "for a single training example, which needs to be generalized to the\n",
    "overall error function. The second assumption is that it can be written\n",
    "as a function of the outputs from the neural network.\n",
    "\n",
    "### Example loss function\n",
    "\n",
    "Let $y,y'$ be vectors in $\\mathbb{R}^n$.\n",
    "\n",
    "Select an error function $E(y,y')$ measuring the difference between two\n",
    "outputs.\n",
    "\n",
    "The standard choice is $E(y,y') = \\tfrac{1}{2} \\lVert y-y'\\rVert^2$,\n",
    "\n",
    "the square of the Euclidean distance between the vectors $y$ and $y'$.\n",
    "\n",
    "The factor of $\\tfrac{1}{2}$ conveniently cancels the exponent when the\n",
    "error function is subsequently differentiated.\n",
    "\n",
    "The error function over $n$ training examples can be written as an\n",
    "average$$E=\\frac{1}{2n}\\sum_x\\lVert (y(x)-y'(x)) \\rVert^2$$And the\n",
    "partial derivative with respect to the\n",
    "outputs$$\\frac{\\partial E}{\\partial y'} = y'-y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy\n",
    "\n",
    "In information theory, the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between two probability\n",
    "distributions $p$ and $q$ over the same underlying set of events\n",
    "measures the average number of bits needed to identify an event drawn\n",
    "from the set, if a coding scheme is used that is optimized for an\n",
    "“unnatural” probability distribution $q$, rather than the “true”\n",
    "distribution $p$.\n",
    "\n",
    "The cross entropy for the distributions $p$ and $q$ over a given set is\n",
    "defined as follows:\n",
    "\n",
    "$$H(p, q) = \\operatorname{E}_p[-\\log q] = H(p) + D_{\\mathrm{KL}}(p \\| q),\\!$$\n",
    "\n",
    "where $H(p)$ is the entropy of $p$, and $D_{\\mathrm{KL}}(p \\| q)$ is\n",
    "the [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) of $q$ from $p$ (also known as the\n",
    "*relative entropy* of *p* with respect to *q* — note the reversal of\n",
    "emphasis).\n",
    "\n",
    "For discrete $p$ and $q$ this means\n",
    "\n",
    "$$H(p, q) = -\\sum_x p(x)\\, \\log q(x). \\!$$\n",
    "\n",
    "The situation for continuous distributions is analogous. We have to\n",
    "assume that $p$ and $q$ are [absolutely continuous] with respect to some\n",
    "reference [measure] $r$ (usually $r$ is a Lebesgue measure on a\n",
    "Borel [σ-algebra]). Let $P$ and $Q$ be probability density functions\n",
    "of $p$ and $q$ with respect to $r$. Then\n",
    "\n",
    "$$-\\int_X P(x)\\, \\log Q(x)\\, dr(x) = \\operatorname{E}_p[-\\log Q]. \\!$$\n",
    "\n",
    "NB: The notation $H(p,q)$ is also used for a different concept, the\n",
    "joint entropy of $p$ and $q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_C(n=55,i=784,o=10):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 784, output 10\n",
    "    # Using relu and \n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(10, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 55)                43175     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                560       \n",
      "=================================================================\n",
      "Total params: 43,735\n",
      "Trainable params: 43,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn3=shallow_net_C()\n",
    "nn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 1.2643 - acc: 0.6854 - val_loss: 0.6699 - val_acc: 0.8520\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.5697 - acc: 0.8599 - val_loss: 0.4638 - val_acc: 0.8839\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.4490 - acc: 0.8814 - val_loss: 0.3969 - val_acc: 0.8955\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3977 - acc: 0.8917 - val_loss: 0.3615 - val_acc: 0.9024\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3674 - acc: 0.8981 - val_loss: 0.3388 - val_acc: 0.9072\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3467 - acc: 0.9027 - val_loss: 0.3231 - val_acc: 0.9101\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3310 - acc: 0.9069 - val_loss: 0.3107 - val_acc: 0.9134\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3183 - acc: 0.9105 - val_loss: 0.3005 - val_acc: 0.9167\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.3076 - acc: 0.9130 - val_loss: 0.2924 - val_acc: 0.9186\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2982 - acc: 0.9162 - val_loss: 0.2848 - val_acc: 0.9202\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2901 - acc: 0.9188 - val_loss: 0.2792 - val_acc: 0.9206\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2827 - acc: 0.9203 - val_loss: 0.2719 - val_acc: 0.9229\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2759 - acc: 0.9220 - val_loss: 0.2664 - val_acc: 0.9238\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2698 - acc: 0.9241 - val_loss: 0.2612 - val_acc: 0.9253\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2640 - acc: 0.9260 - val_loss: 0.2567 - val_acc: 0.9264\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2586 - acc: 0.9267 - val_loss: 0.2519 - val_acc: 0.9285\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2536 - acc: 0.9284 - val_loss: 0.2478 - val_acc: 0.9280\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2487 - acc: 0.9300 - val_loss: 0.2427 - val_acc: 0.9311\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2441 - acc: 0.9315 - val_loss: 0.2392 - val_acc: 0.9334\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2395 - acc: 0.9329 - val_loss: 0.2352 - val_acc: 0.9348\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2355 - acc: 0.9338 - val_loss: 0.2314 - val_acc: 0.9351\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2313 - acc: 0.9348 - val_loss: 0.2278 - val_acc: 0.9350\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2271 - acc: 0.9366 - val_loss: 0.2257 - val_acc: 0.9364\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2233 - acc: 0.9375 - val_loss: 0.2213 - val_acc: 0.9365\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2196 - acc: 0.9386 - val_loss: 0.2176 - val_acc: 0.9382\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2161 - acc: 0.9393 - val_loss: 0.2141 - val_acc: 0.9388\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2124 - acc: 0.9403 - val_loss: 0.2108 - val_acc: 0.9396\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2092 - acc: 0.9412 - val_loss: 0.2085 - val_acc: 0.9403\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2059 - acc: 0.9423 - val_loss: 0.2053 - val_acc: 0.9405\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.2029 - acc: 0.9432 - val_loss: 0.2025 - val_acc: 0.9413\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1997 - acc: 0.9438 - val_loss: 0.1996 - val_acc: 0.9425\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1967 - acc: 0.9449 - val_loss: 0.1968 - val_acc: 0.9435\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1938 - acc: 0.9456 - val_loss: 0.1949 - val_acc: 0.9446\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1910 - acc: 0.9468 - val_loss: 0.1924 - val_acc: 0.9453\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1883 - acc: 0.9474 - val_loss: 0.1897 - val_acc: 0.9456\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1856 - acc: 0.9481 - val_loss: 0.1874 - val_acc: 0.9465\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1831 - acc: 0.9490 - val_loss: 0.1845 - val_acc: 0.9474\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1806 - acc: 0.9500 - val_loss: 0.1830 - val_acc: 0.9477\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1781 - acc: 0.9500 - val_loss: 0.1809 - val_acc: 0.9487\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1757 - acc: 0.9508 - val_loss: 0.1781 - val_acc: 0.9488\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1734 - acc: 0.9513 - val_loss: 0.1762 - val_acc: 0.9494\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1711 - acc: 0.9518 - val_loss: 0.1748 - val_acc: 0.9498\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1689 - acc: 0.9525 - val_loss: 0.1730 - val_acc: 0.9501\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1667 - acc: 0.9538 - val_loss: 0.1715 - val_acc: 0.9512\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1646 - acc: 0.9537 - val_loss: 0.1686 - val_acc: 0.9509\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1625 - acc: 0.9539 - val_loss: 0.1669 - val_acc: 0.9518\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1606 - acc: 0.9549 - val_loss: 0.1658 - val_acc: 0.9521\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1586 - acc: 0.9550 - val_loss: 0.1630 - val_acc: 0.9532\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 0s - loss: 0.1567 - acc: 0.9557 - val_loss: 0.1618 - val_acc: 0.9533\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1548 - acc: 0.9564 - val_loss: 0.1606 - val_acc: 0.9536\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1530 - acc: 0.9566 - val_loss: 0.1584 - val_acc: 0.9548\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1512 - acc: 0.9570 - val_loss: 0.1575 - val_acc: 0.9550\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1494 - acc: 0.9576 - val_loss: 0.1559 - val_acc: 0.9555\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1477 - acc: 0.9582 - val_loss: 0.1538 - val_acc: 0.9563\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1460 - acc: 0.9586 - val_loss: 0.1536 - val_acc: 0.9556\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1444 - acc: 0.9589 - val_loss: 0.1513 - val_acc: 0.9570\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1428 - acc: 0.9592 - val_loss: 0.1496 - val_acc: 0.9573\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1412 - acc: 0.9598 - val_loss: 0.1487 - val_acc: 0.9579\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1397 - acc: 0.9605 - val_loss: 0.1471 - val_acc: 0.9577\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1381 - acc: 0.9609 - val_loss: 0.1462 - val_acc: 0.9595\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1367 - acc: 0.9613 - val_loss: 0.1446 - val_acc: 0.9587\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1352 - acc: 0.9618 - val_loss: 0.1438 - val_acc: 0.9601\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1337 - acc: 0.9621 - val_loss: 0.1430 - val_acc: 0.9606\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s - loss: 0.1325 - acc: 0.9627 - val_loss: 0.1418 - val_acc: 0.9609\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1312 - acc: 0.9628 - val_loss: 0.1400 - val_acc: 0.9601\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1298 - acc: 0.9633 - val_loss: 0.1395 - val_acc: 0.9614\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1285 - acc: 0.9637 - val_loss: 0.1380 - val_acc: 0.9610\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1273 - acc: 0.9644 - val_loss: 0.1369 - val_acc: 0.9624\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1261 - acc: 0.9642 - val_loss: 0.1360 - val_acc: 0.9621\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1249 - acc: 0.9647 - val_loss: 0.1346 - val_acc: 0.9624\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1238 - acc: 0.9650 - val_loss: 0.1340 - val_acc: 0.9634\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1227 - acc: 0.9655 - val_loss: 0.1328 - val_acc: 0.9633\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1215 - acc: 0.9657 - val_loss: 0.1319 - val_acc: 0.9631\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1204 - acc: 0.9657 - val_loss: 0.1313 - val_acc: 0.9643\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1194 - acc: 0.9664 - val_loss: 0.1303 - val_acc: 0.9646\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1183 - acc: 0.9668 - val_loss: 0.1297 - val_acc: 0.9643\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1173 - acc: 0.9673 - val_loss: 0.1286 - val_acc: 0.9648\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1162 - acc: 0.9673 - val_loss: 0.1283 - val_acc: 0.9645\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1154 - acc: 0.9674 - val_loss: 0.1270 - val_acc: 0.9649\n",
      "Epoch 80/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1143 - acc: 0.9679 - val_loss: 0.1263 - val_acc: 0.9653\n",
      "Epoch 81/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1134 - acc: 0.9681 - val_loss: 0.1255 - val_acc: 0.9654\n",
      "Epoch 82/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1124 - acc: 0.9686 - val_loss: 0.1247 - val_acc: 0.9651\n",
      "Epoch 83/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1114 - acc: 0.9686 - val_loss: 0.1249 - val_acc: 0.9653\n",
      "Epoch 84/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1107 - acc: 0.9689 - val_loss: 0.1236 - val_acc: 0.9658\n",
      "Epoch 85/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1097 - acc: 0.9695 - val_loss: 0.1231 - val_acc: 0.9658\n",
      "Epoch 86/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1089 - acc: 0.9694 - val_loss: 0.1219 - val_acc: 0.9669\n",
      "Epoch 87/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1080 - acc: 0.9702 - val_loss: 0.1220 - val_acc: 0.9661\n",
      "Epoch 88/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1072 - acc: 0.9702 - val_loss: 0.1208 - val_acc: 0.9672\n",
      "Epoch 89/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1063 - acc: 0.9704 - val_loss: 0.1203 - val_acc: 0.9671\n",
      "Epoch 90/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1056 - acc: 0.9704 - val_loss: 0.1193 - val_acc: 0.9678\n",
      "Epoch 91/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1047 - acc: 0.9708 - val_loss: 0.1192 - val_acc: 0.9673\n",
      "Epoch 92/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1039 - acc: 0.9712 - val_loss: 0.1182 - val_acc: 0.9669\n",
      "Epoch 93/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1032 - acc: 0.9711 - val_loss: 0.1179 - val_acc: 0.9673\n",
      "Epoch 94/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1024 - acc: 0.9715 - val_loss: 0.1172 - val_acc: 0.9674\n",
      "Epoch 95/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1017 - acc: 0.9714 - val_loss: 0.1169 - val_acc: 0.9678\n",
      "Epoch 96/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1010 - acc: 0.9719 - val_loss: 0.1160 - val_acc: 0.9676\n",
      "Epoch 97/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.1003 - acc: 0.9720 - val_loss: 0.1155 - val_acc: 0.9676\n",
      "Epoch 98/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0995 - acc: 0.9724 - val_loss: 0.1160 - val_acc: 0.9675\n",
      "Epoch 99/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0989 - acc: 0.9725 - val_loss: 0.1150 - val_acc: 0.9679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e5bdeb8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8800/10000 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11497668759040534, 0.96789999999999998]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 96% accuracy after first 99 epochs with Relu and Cross-entropy\n",
    "nn3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0981 - acc: 0.9723 - val_loss: 0.1141 - val_acc: 0.9674\n",
      "Epoch 2/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0976 - acc: 0.9727 - val_loss: 0.1138 - val_acc: 0.9679\n",
      "Epoch 3/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0968 - acc: 0.9731 - val_loss: 0.1129 - val_acc: 0.9679\n",
      "Epoch 4/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0962 - acc: 0.9731 - val_loss: 0.1129 - val_acc: 0.9676\n",
      "Epoch 5/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0955 - acc: 0.9735 - val_loss: 0.1120 - val_acc: 0.9677\n",
      "Epoch 6/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0949 - acc: 0.9735 - val_loss: 0.1119 - val_acc: 0.9679\n",
      "Epoch 7/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0943 - acc: 0.9738 - val_loss: 0.1108 - val_acc: 0.9681\n",
      "Epoch 8/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0936 - acc: 0.9741 - val_loss: 0.1108 - val_acc: 0.9673\n",
      "Epoch 9/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0931 - acc: 0.9744 - val_loss: 0.1102 - val_acc: 0.9679\n",
      "Epoch 10/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0924 - acc: 0.9742 - val_loss: 0.1102 - val_acc: 0.9678\n",
      "Epoch 11/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0919 - acc: 0.9744 - val_loss: 0.1097 - val_acc: 0.9680\n",
      "Epoch 12/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0913 - acc: 0.9745 - val_loss: 0.1090 - val_acc: 0.9679\n",
      "Epoch 13/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0907 - acc: 0.9748 - val_loss: 0.1089 - val_acc: 0.9683\n",
      "Epoch 14/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0902 - acc: 0.9750 - val_loss: 0.1085 - val_acc: 0.9685\n",
      "Epoch 15/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0895 - acc: 0.9754 - val_loss: 0.1082 - val_acc: 0.9684\n",
      "Epoch 16/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0890 - acc: 0.9755 - val_loss: 0.1072 - val_acc: 0.9688\n",
      "Epoch 17/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0885 - acc: 0.9757 - val_loss: 0.1070 - val_acc: 0.9690\n",
      "Epoch 18/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0879 - acc: 0.9756 - val_loss: 0.1065 - val_acc: 0.9692\n",
      "Epoch 19/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0874 - acc: 0.9758 - val_loss: 0.1064 - val_acc: 0.9693\n",
      "Epoch 20/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0869 - acc: 0.9758 - val_loss: 0.1058 - val_acc: 0.9689\n",
      "Epoch 21/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0863 - acc: 0.9761 - val_loss: 0.1054 - val_acc: 0.9692\n",
      "Epoch 22/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0858 - acc: 0.9764 - val_loss: 0.1052 - val_acc: 0.9690\n",
      "Epoch 23/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0854 - acc: 0.9763 - val_loss: 0.1045 - val_acc: 0.9693\n",
      "Epoch 24/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0848 - acc: 0.9765 - val_loss: 0.1044 - val_acc: 0.9695\n",
      "Epoch 25/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0844 - acc: 0.9767 - val_loss: 0.1042 - val_acc: 0.9691\n",
      "Epoch 26/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0839 - acc: 0.9767 - val_loss: 0.1045 - val_acc: 0.9687\n",
      "Epoch 27/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0834 - acc: 0.9767 - val_loss: 0.1035 - val_acc: 0.9696\n",
      "Epoch 28/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0829 - acc: 0.9772 - val_loss: 0.1035 - val_acc: 0.9700\n",
      "Epoch 29/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0824 - acc: 0.9771 - val_loss: 0.1026 - val_acc: 0.9698\n",
      "Epoch 30/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0819 - acc: 0.9773 - val_loss: 0.1025 - val_acc: 0.9698\n",
      "Epoch 31/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0815 - acc: 0.9774 - val_loss: 0.1021 - val_acc: 0.9697\n",
      "Epoch 32/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0810 - acc: 0.9777 - val_loss: 0.1017 - val_acc: 0.9699\n",
      "Epoch 33/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0806 - acc: 0.9776 - val_loss: 0.1016 - val_acc: 0.9703\n",
      "Epoch 34/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0801 - acc: 0.9781 - val_loss: 0.1011 - val_acc: 0.9703\n",
      "Epoch 35/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0797 - acc: 0.9779 - val_loss: 0.1011 - val_acc: 0.9704\n",
      "Epoch 36/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0793 - acc: 0.9783 - val_loss: 0.1006 - val_acc: 0.9701\n",
      "Epoch 37/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0789 - acc: 0.9782 - val_loss: 0.1001 - val_acc: 0.9706\n",
      "Epoch 38/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0785 - acc: 0.9783 - val_loss: 0.1003 - val_acc: 0.9707\n",
      "Epoch 39/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0779 - acc: 0.9785 - val_loss: 0.1005 - val_acc: 0.9699\n",
      "Epoch 40/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0776 - acc: 0.9784 - val_loss: 0.1003 - val_acc: 0.9702\n",
      "Epoch 41/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0772 - acc: 0.9785 - val_loss: 0.0990 - val_acc: 0.9709\n",
      "Epoch 42/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0768 - acc: 0.9790 - val_loss: 0.0989 - val_acc: 0.9707\n",
      "Epoch 43/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0764 - acc: 0.9789 - val_loss: 0.0986 - val_acc: 0.9706\n",
      "Epoch 44/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0759 - acc: 0.9790 - val_loss: 0.0985 - val_acc: 0.9710\n",
      "Epoch 45/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0755 - acc: 0.9793 - val_loss: 0.0982 - val_acc: 0.9709\n",
      "Epoch 46/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0752 - acc: 0.9793 - val_loss: 0.0978 - val_acc: 0.9707\n",
      "Epoch 47/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0748 - acc: 0.9794 - val_loss: 0.0979 - val_acc: 0.9709\n",
      "Epoch 48/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0744 - acc: 0.9794 - val_loss: 0.0976 - val_acc: 0.9711\n",
      "Epoch 49/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0740 - acc: 0.9797 - val_loss: 0.0977 - val_acc: 0.9704\n",
      "Epoch 50/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0736 - acc: 0.9798 - val_loss: 0.0966 - val_acc: 0.9719\n",
      "Epoch 51/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0733 - acc: 0.9798 - val_loss: 0.0966 - val_acc: 0.9710\n",
      "Epoch 52/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0729 - acc: 0.9800 - val_loss: 0.0966 - val_acc: 0.9709\n",
      "Epoch 53/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0726 - acc: 0.9801 - val_loss: 0.0964 - val_acc: 0.9712\n",
      "Epoch 54/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0722 - acc: 0.9803 - val_loss: 0.0962 - val_acc: 0.9712\n",
      "Epoch 55/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0718 - acc: 0.9803 - val_loss: 0.0956 - val_acc: 0.9717\n",
      "Epoch 56/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0715 - acc: 0.9806 - val_loss: 0.0958 - val_acc: 0.9714\n",
      "Epoch 57/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0711 - acc: 0.9804 - val_loss: 0.0954 - val_acc: 0.9719\n",
      "Epoch 58/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0707 - acc: 0.9807 - val_loss: 0.0955 - val_acc: 0.9719\n",
      "Epoch 59/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0704 - acc: 0.9808 - val_loss: 0.0948 - val_acc: 0.9718\n",
      "Epoch 60/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0700 - acc: 0.9807 - val_loss: 0.0946 - val_acc: 0.9715\n",
      "Epoch 61/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0698 - acc: 0.9811 - val_loss: 0.0943 - val_acc: 0.9715\n",
      "Epoch 62/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0694 - acc: 0.9811 - val_loss: 0.0941 - val_acc: 0.9721\n",
      "Epoch 63/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0690 - acc: 0.9814 - val_loss: 0.0942 - val_acc: 0.9721\n",
      "Epoch 64/99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 1s - loss: 0.0687 - acc: 0.9815 - val_loss: 0.0936 - val_acc: 0.9725\n",
      "Epoch 65/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0684 - acc: 0.9814 - val_loss: 0.0942 - val_acc: 0.9724\n",
      "Epoch 66/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0681 - acc: 0.9817 - val_loss: 0.0935 - val_acc: 0.9719\n",
      "Epoch 67/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0678 - acc: 0.9818 - val_loss: 0.0932 - val_acc: 0.9724\n",
      "Epoch 68/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0674 - acc: 0.9817 - val_loss: 0.0931 - val_acc: 0.9725\n",
      "Epoch 69/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0671 - acc: 0.9817 - val_loss: 0.0933 - val_acc: 0.9718\n",
      "Epoch 70/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0668 - acc: 0.9822 - val_loss: 0.0931 - val_acc: 0.9727\n",
      "Epoch 71/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0665 - acc: 0.9821 - val_loss: 0.0924 - val_acc: 0.9724\n",
      "Epoch 72/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0662 - acc: 0.9823 - val_loss: 0.0924 - val_acc: 0.9726\n",
      "Epoch 73/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0659 - acc: 0.9821 - val_loss: 0.0922 - val_acc: 0.9726\n",
      "Epoch 74/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0656 - acc: 0.9823 - val_loss: 0.0919 - val_acc: 0.9725\n",
      "Epoch 75/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0654 - acc: 0.9824 - val_loss: 0.0920 - val_acc: 0.9727\n",
      "Epoch 76/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0650 - acc: 0.9825 - val_loss: 0.0917 - val_acc: 0.9727\n",
      "Epoch 77/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0647 - acc: 0.9825 - val_loss: 0.0915 - val_acc: 0.9727\n",
      "Epoch 78/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0644 - acc: 0.9828 - val_loss: 0.0917 - val_acc: 0.9725\n",
      "Epoch 79/99\n",
      "60000/60000 [==============================] - 1s - loss: 0.0641 - acc: 0.9827 - val_loss: 0.0911 - val_acc: 0.9728\n",
      "Epoch 80/99\n",
      " 5888/60000 [=>............................] - ETA: 0s - loss: 0.0604 - acc: 0.9845"
     ]
    }
   ],
   "source": [
    "nn3.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 97% accuracy after another 99 epochs with Relu and Cross-entropy\n",
    "nn3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "With a fairly simple shallow net we've done fairly well classifying (97% accuracy after another 99 epochs with Relu and Cross-entropy) on the [MNIST](http://yann.lecun.com/exdb/mnist/)  handwritten digit classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update September 22, 2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
